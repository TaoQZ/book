{"./":{"url":"./","title":"前言","keywords":"","body":" jinrishici.load(function(result) { var sentence = document.querySelector(\"#poem_sentence\") var info = document.querySelector(\"#poem_info\") var trans = document.querySelector(\"#poem_trans\") sentence.innerHTML = result.data.content info.innerHTML = '【' + result.data.origin.dynasty + '】' + result.data.origin.author + '《' + result.data.origin.title + '》' }); Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"git/Git.html":{"url":"git/Git.html","title":"git安装及简介","keywords":"","body":"Git是什么及为什么使用安装及配置基本概念介绍常用命令冲突TortoiseGit:Idea配置git可能出现的问题1. 解决git clone 克隆慢2. 解决github.com 无法访问连接超时3.删除所有文件Git 是什么及为什么使用 ​ git是一个强大的版本控制工具 ​ 例如在企业团队开发中,需要协同开发,每个人负责不同的模块,但又有可能依赖其他的模块 ​ 使用git可以解决这些问题,同时还可以对版本进行严格的控制与管理 安装及配置 ​ 官网:https://git-scm.com/download ​ 一路点击下一步,取消最后的两个选项,点finish ​ 安装完成后 配置用户名和邮箱 git config --global user.name \"用户名\" git config --global user.email \"邮箱地址\" 配置后的文件在 C:\\用户\\{用户名}\\.gitconfig文件中 ​ 生成ssh 秘钥与 github相关联 1. ssh-keygen -t rsa -C \"邮箱地址\" 默认生成到当前用户 C:\\用户\\{当前用户}\\.ssh\\id_rsa.pub 2. 进入github官网,登陆后 3. Settings --> SSH and GPG keys --> New SSH key 4. 将文件中的秘钥全部复制到 key 中 基本概念介绍 master : 默认开发分支 origin : 默认远程版本库 Index/Stage : 暂存区(工作区和版本库之间) add后的区域 workspace : 工作区 init后的区域 repository : 本地仓库 commit后的区域 remote : 远程仓库 github head : 指针 可以通过指针切换版本 常用命令 git init : 初始化仓库 把这个目录变成Git可以管理的仓库 git add 文件名: 添加文件 直接写符号.将(全部文件) 把文件添加到本地仓库的暂存区 git commit -m \"提交 注释\" 提交到当前分支 git remote add origin 远程仓库地址 ：远程仓库与本地仓库添加关联 有关联后,就不用再写这行 git pull origin master : 在推之前先拉 更新代码 git push -u origin master 将代码推到远程仓库 git log 查看日志 git status 查看当前仓库状态 git rm -r --cached filename : 删除多与文件,只删除git上的文件不会删除本地的文件,删除之后commit提交即可 冲突 ​ 对同一行或者同一块区域的代码进行修改后,导致本地仓库与远程仓库无法合并,因为软件也不知道采用哪一段代码,需要人工解决 这是出现冲突后在推时出现的情况 冲突:不同的人修改了相同位置的代码 ======= 其他人的代码 >>>>>>>版本号(具体的版本号可以在命令行中查询) 手动解决冲突后提交或合并后提交即可 TortoiseGit: ​ 使用git bash 需要很多重复的操作克隆 提交 推送,可以使用视图化工具简化操作 ​ 官网:https://tortoisegit.org/download/ ​ ​ 点击finish后,会进入配置 ​ 填入用户名和邮箱 ​ 安装完成后可能会需要重启 重启完成后点击鼠标右键会出现 ​ TortoiseGit --> settings 选择下方选项进行配置 安装完成后,在任意位置鼠标右键,可以完成快速克隆 提交... Idea配置git ​ Settings --> Version Control ​ 登录 ​ 如果选择上方的Enter token 登录 ​ 需要登录github官网 创建token ​ ​ 可能出现的问题 1. 解决git clone 克隆慢 ​ 开启代理(本人使用的ssr 默认端口是1080): git config --global http.proxy socks5://127.0.0.1:1080 ​ 关闭代理: git config --global http.proxy \"\" ​ 如果代理没有关闭可能也会出现 fatal: unable to access 'https://github.com/xxx/xxx.git/': Failed to connect to 127.0.0.1 port 1080: Connection refused ​ 可以使用命令关闭也可以在 C:\\Users\\{用户名}\\\\.gitconfig 中删除proxy代理 2. 解决github.com 无法访问连接超时 ​ 先在命令行ping官网 github.com ​ 如果ping不通 ​ 在 C:\\Windows\\System32\\drivers\\etc\\hosts 末尾添加 192.30.255.112 github.com git 185.31.16.184 github.global.ssl.fastly.net ​ 3.删除所有文件 git rm * -r Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"git/Idea中使用Git.html":{"url":"git/Idea中使用Git.html","title":"Idea使用Git","keywords":"","body":"配置使用1. 创建项目2. 添加文件3. 提交并推送4. 冲突配置 ​ 在git一章中有写在Idea中如何配置git 使用 1. 创建项目 ​ idea中创建项目 相当于 init 将当前项目变为工作空间 ​ github中创建新的仓库 2. 添加文件 ​ 相当于 add (文件) ​ 3. 提交并推送 ​ 在idea菜单栏VSC选项 相当于 commit ​ 选择需要提交的文件,在Commit Message 中填写信息,选择commit and push ​ 选择commit和push后,在第一次push时会遇到下图,需要将当前工作空间与远程仓库进行关联 ​ 填写完成后便可以进行推送 ​ 4. 冲突 ​ 不同的人在对相同行数的代码进行修改时,一方已经提交,另一方在拉取或者推送时会遇到冲突的问题 案例: ​ 第一次提交的代码 ​ 假设有人修改了代码,并上传成功 ​ 同时修改idea中的代码 ​ 进行commit 提交后进行pull 拉取 ​ 会出现冲突提示 ,点击中间的合并按钮 ​ 进行合并 ​ 合并后重新提交 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"vue/Vue.html":{"url":"vue/Vue.html","title":"Vue基础","keywords":"","body":"VueVue是什么使用Vue使用cdn官网下载入门案例方法生命周期指令计算属性axios路由router-link & router-view配置路由:style scopeexport default路由传参路由跳转嵌套路由Vuex什么是Vuex,为什么要有Vuex在项目中使用vuex访问vuex中的数据及方法mutations传参问题注意事项组件之间传值父传子子传父非父子组件之间传值导航守卫全局的单个路由独享的组件级别的Vue Vue是什么 ​ Vue是一套用于构建用户界面的渐进式框架(可以循序渐进的进行学习),是一个MVVM的框架,功能强大,重要特点双向绑定 ​ MVVM: ​ M:model,数据/模型层 ​ V:view 视图层 ​ VM:核心层,负责连接M和V,这一层已经由Vue实现好了 ​ 双向绑定: ​ 更新view页面的数据同步到data ​ 更新data中的数据同步渲染到页面 使用Vue ​ 以下案例的github地址:https://github.com/TaoQZ/Vue_example.git 使用cdn 官网下载 https://cn.vuejs.org/v2/guide/installation.html 入门案例 Title // 创建vue实例 // el 获取指定id的容器 // data 数据 // methods 方法 var vm = new Vue({ el:'#app', data:{ msg:'测试' }, methods:{ } }) 方法 var vm = new Vue({ methods:{ fun1:function () { console.log('方法1') }, fun2(){ console.log('方法2') }, // 箭头函数 // 1. 没有参数,或多个参数使用 () (a,b) // 2. 一个参数可以省略() 直接写形参 a // 3. 返回值 只有一行代码包括返回值 可以省略{} // 4. 多行代码需要 {} // 5. 注意 使用该箭头函数定义方法时,方法中this的指向不在再是Vue实例 fun3: () => console.log('方法3') } }); vm.fun1(); vm.fun2(); vm.fun3(); 生命周期 var vm = new Vue({ el:'#app', data:{ msg:'+' }, methods:{ show(){ console.log('我是show方法') } }, // 初始化前 数据和方法均为未初始化 beforeCreate(){ // undefined console.log(this.msg) // this.show is not a function this.show() console.log('=======================================') }, // 初始化完成 数据和方法已初始化完成 created(){ // 全部正常打印 console.log(this.msg) this.show() console.log('=======================================') }, // 双向绑定(挂载)前,表示模板已经编译完成,但是未将模板渲染到页面中 beforeMount(){ // 未渲染状态,只是原始字符串 console.log(document.getElementById('app').innerText) console.log('=======================================') }, // 双向绑定(挂载)后,将数据渲染到页面 mounted(){ // + console.log(document.getElementById('app').innerText) console.log('=======================================') }, // 更新钩子函数需要改变数据才会触发 // 更新前 data中的数据已经发生变化 未渲染到页面(页面还没同步) beforeUpdate(){ // + console.log(\"更新前\"+this.msg) // - console.log(\"更新前\"+document.getElementById('app').innerText) }, // 更新后 updated(){ // - console.log(\"更新后\"+this.msg) // - console.log(\"更新后\"+document.getElementById('app').innerText) }, // 销毁Vue实例 beforeDestroy(){ console.log('销毁前'+this) }, destroyed(){ console.log('销毁后'+this) } }) // 销毁Vue实例 vm.$destroy(); 指令 插值表达式 ​ 格式:{{}} ​ 可以直接获取Vue实例中定义的数据或函数 ​ 支持有返回值的函数或表达式 ​ 注意:该方式有缺点,在网速较慢时会出现{{}} 闪烁问题 2 var vm = new Vue({ el:'#app', data:{ msg:'zz' }, methods:{ show(){ return 10 } } }) v-text v-html ​ 为了解决插值闪烁问题提供了解决办法 ​ v-text : 将数据原样输出 ​ v-html : 可以将html的字符串渲染到页面 ​ var vm = new Vue({ el:'#app', data:{ msg:'tao' } }) v-model 加 表单元素 ​ 插值表达式,v-text,v-html : 只是数据的单向绑定 ​ v-model可以使视图和数据进行双向绑定,互相影响 ​ Title hobby: 网球 游泳 跑步 sex: 男 女 单复选框： 下拉列表 请选择 // 注意: 在data中拿到的值是标签的value值 var vm = new Vue({ el:'#app', data:{ num:1, hobby:[], checked: true, sex:'male', items:['河北','天津','北京'], province:'' } }) v-on ​ 例子 涉及事件 ​ click keydown mouseover mouseout ​ 事件处理的指令 var vm = new Vue({ el:'#app', methods:{ show(e){ console.log(e.keyCode) }, inDiv(){ console.log(\"鼠标移入\") }, outDiv(){ console.log(\"鼠标移出\") }, buttonClick(){ console.log('点击事件') }, isEnter(e){ console.log(e.keyCode) } } }) ​ 事件冒泡+vue解决办法_按键修饰符 ​ 事件发生时,触发内层的事件会同时触发外层的事件 ​ 常用按键修饰符 ​ .enter // 表示键盘的enter键 .tab .delete (捕获 \"删除\" 和 \"退格\" 键) .esc .space .up .down .left .right .ctrl .alt .shift .meta 百度 var vm = new Vue({ el:'#app', methods:{ divClick(){ console.log('div') }, buttonClick(e){ // js中冒泡的解决方案 // 阻止事件的传播行为 // e.stopPropagation(); console.log('button') }, jump:()=>{ console.log('点击a标签后没有跳转网页') }, isEnter(e){ console.log(e.keyCode) } } }) v-for --------- var vm = new Vue({ el:'#app', data:{ users:[ {name:\"aa\",age:\"23\",sex:\"男\"}, {name:\"bb\",age:\"21\",sex:\"男\"}, {name:\"cc\",age:\"20\",sex:\"男\"} ] } }) v-if v-else-if v-else ​ v-else 必须紧跟在 v-if 或 v-else-if 后 ​ v-else-if 必须紧跟在v-if 或者 v-else-if 后 ​ A B C 索引 姓名 年龄 性别 20\"> {{index}} {{user.name}} {{user.age}} {{user.sex}} var vm = new Vue({ el:'#app', data:{ char:'A', users:[ {name:\"aa\",age:\"23\",sex:\"男\"}, {name:\"bb\",age:\"21\",sex:\"男\"}, {name:\"cc\",age:\"20\",sex:\"男\"} ] } }) ​ v-show与v-if flag flag var vm = new Vue({ el:'#app', data:{ flag:true }, methods:{ change(){ this.flag = !this.flag } } }) v-bind ​ 将html标签的属性与data中的数据进行绑定,可以简写为 :需要绑定的属性 Title .red{ background-color: red; } .black{ background-color: black; } .green{ background-color: green; } 请选择 var vm = new Vue({ el:'#app', data:{ myclass:'', colors:[ {name:'红',clas:'red'}, {name:'黑',clas:'black'}, {name:'绿',clas:'green'} ] } }) ​ 计算属性 ​ 解决在插值表达式中书写过长或需要长期维护使用的表达式比较麻烦 var vm = new Vue({ el:'#app', data:{ msg:'' }, methods:{ method_num(){ return 1+1; } }, computed:{ computed_num(){ return 2+1; } } }) axios ​ 用于发送http请求 ​ cdn 在使用vue-cli脚手架创建的项目中使用 下载依赖 npm install axios 在项目中引入 全局引入需要在main.js中导入 局部引入需要在export default 上导入: // 导入 axios import axios from 'axios' // 设置请求路径的前缀 axios.defaults.baseURL='http://localhost:8080' // 第一个axios可以当做全局的访问前缀 Vue.prototype.axios(相当于起别名) = axios // 配置拦截器 axios.interceptors.request.use(request=>{ //获取token let token = sessionStorage.getItem(\"token\"); if(token){ request.headers.authorization = token } return request } , error=>{}) Vue.prototype.axios=axios 示例: 全局 全局需要使用this. 局部不需要 export default { name: \"list\", data(){ return{ users:[] } }, methods:{ findAll(){ this.axios.get('/user') .then(res => this.users = res.data) }, created() { this.findAll(); } } ​ 2.局部 // 局部引入axios 不需要this // import axios from 'axios' // axios.defaults.baseURL='http://localhost:8080' export default { name: \"list\", data(){ return{ users:[] } }, methods:{ findAll(){ axios.get('/user') .then(res => this.users = res.data) } }, created() { this.findAll(); } } ​ git传递对象参数 this.axios.get('url',{ params : 对象 }) ​ delete传递参数 数组 ​ 需要配合qs使用 安装 npm install qs main.js 中配置 import qs from 'qs' Vue.prototype.qs = qs this.axios.delete(\"/book\", { params: { books: arr }, paramsSerializer: params => { // false 用来控制格式 return qs.stringify(params, { indices: false }) } }).then(() => { this.getPageInfo(this.num, this.size); }) 路由 router-link & router-view : : 配置路由: ​ 使用vue-cli创建项目后就已做好的: ​ 在router/index.js文件中 导入 vue-router并使用 // 导入 导入时名字可以随便起,但在use名字时必须相同 import Vue from 'vue' import VueRouter from 'vue-router' // 导入组件 全局引入 import Home from '../views/Home' // 使用 Vue.use(VueRouter) // 创建路由 数组 const routes = [ // path:访问路径 // name:路由名称 // component:要跳转到的组件 需要导入 { // 写路径时记得写 / path: '/', name: 'home', component: Home, // 再使用时引入的方式,可以提高首页显示速度 // component: () => import('../views/Home.vue') } ] ​ main.js import Vue from 'vue' import App from './App.vue' import router from './router' import store from './store' Vue.config.productionTip = false; new Vue({ router, // 相当于 router:router store, render: h => h(App) }).$mount('#app'); style scope export default // 就是vue的实例 是es6的语法 和之前写的差别是,data只能按照这种格式(和写时组件一样) export default { name: \"list\", methods:{ to(){ alert(\"list\") } }, data(){ return{ msg:'msg' } } } 路由传参 两种方法的路由配置 { path: '/edit', name: 'edit', component: Edit }, 1.name+params 修改 // 接收 this.$route.params.id 2.path+query 修改 // 在地址栏填写参数也是使用该方式获取 this.$route.query.id; 3.路由传参(地址栏) // 在路由的path后添加 :参数名 { path: '/edit/:id', name: 'edit', component: Edit } 该方法路由跳转的三种方式 修改 修改 this.$router.push({ path: '/edit/111', }) // 接收 this.$route.params.id 路由跳转 this.$router.push(路由的path) 同样适用于传参 方式一:根据路由的name匹配路由 路由配置 { path: '/edit', name: 'edit', component: Edit }, 跳转路由 this.$router.push({ name:'edit', params:{ id : 888 } }) 获取路由中的参数 不会表现在地址栏上 this.$route.params.id 方式二:根据路由的path匹配路由 路由配置 { path: '/edit', name: 'edit', component: Edit }, 跳转路由 this.$router.push({ path:'/edit', query:{ id : 111 } }) 获取参数 this.$route.query.id 表现在地址栏 http://localhost:8080/edit?id=111 嵌套路由 ​ 在一个组件中引入了另一个组件,相当于父与子的关系 我是父页面 去子页面 我是子页面 ​ router/index.js 配置 const routes = [ // path:访问路径 // name:路由名称 // component:要跳转到的组件 需要导入 { path: '/father', name: 'father', component: () => import('../views/father.vue'), children:[ { // 注意子路由不能加 / path: 'son', name: 'son', component: () => import('../views/son.vue'), }, // 子路由path的第二种写法,访问路径加上父组件的路径 // { // path: '/home/son', // name: 'son', // component: () => import('../views/son.vue') // } ] } ] Vuex 什么是Vuex,为什么要有Vuex ​ Vuex是组件之间数据共享的一种机制 ​ 使用父子传值或兄弟传值,太麻烦不好管理,有了Vuex想要共享数据,只需要把数据挂在到vuex就行,想要获取数据,直接从vuex上拿就行,vuex中的数据被修改后其他引用了此数据的组件,也会同步更新 在项目中使用vuex ​ 安装vuex: npm install vuex ​ 使用 在store/index.js文件 import Vue from 'vue' import Vuex from 'vuex' Vue.use(Vuex) export default new Vuex.Store({ // 公共区域数据 state: { num : 10, }, mutations: { }, actions: { }, modules: { } }) 在main.js文件中将store挂在到vm实例上 import Vue from 'vue' import App from './App.vue' import router from './router' import store from './store' Vue.config.productionTip = false new Vue({ router, store, render: h => h(App) }).$mount('#app') 访问vuex中的数据及方法 state,mutations,getters 官方文档:https://vuex.vuejs.org/zh/guide/getters.html getters是从state中派生的数据,对state中的数据进行过滤修改,并且可以传参返回 在store/index.js中定义变量存储数据 import Vue from 'vue' import Vuex from 'vuex' Vue.use(Vuex) export default new Vuex.Store({ // 公共区域数据 state: { num : 10, }, mutations: { // 第一个参数默认是state addNum(state){ state.num++ } }, }) 在components中创建两个组件 --> add: --> add: num: // 解构表达式 import {mapState,mapMutations} from 'vuex' export default { name: \"add\", // 官方推荐的使用方式,进行结构,将公共属性的值作为计算属性 computed:{ ...mapState(['num','msg']) }, methods:{ add(){ this.$store.state.num++ }, ...mapMutations(['addNum']) } } ​ minus.vue // 解构 将vuex中的数据解构成计算属性使用 import {mapState} from 'vuex' export default { name: \"add\", methods:{}, computed:{ ...mapState(['num']) } } ​ router/index.js // 将这两个组件导入 import add from '@/components/add.vue' import minus from '@/components/minus.vue' const routes = [ { path:'/访问路径', name:'home', // 这里是简写方式次里是简写方式 相当于 add:add,minus:minus // 前面是路由名称,后面是组件名称,相同时可以简写 components:{ add, minus } }, ] ​ App.vue:用于显示变量,观察其修改后两个组件中数据是否同步更新 // 这里的name对应路由配置中components中所写即可 ​ actions ​ 登录案例 登录 账号: 密码: --> import {mapActions} from 'vuex' export default { methods:{ login(){ // 官方并不推荐直接调用actions中的方法 this.$store.dispatch('asyncLogin',this.user) }, ...mapActions(['asyncLogin']), }, name: \"Login\", data(){ return{ user:{} } }, } store/index.js mutations: { // 第一个参数默认state login(state,user){ console.log(user) } }, actions: { // 第一个参数默认context asyncLogin(context,user){ // 调用的mutations中的方法,进行封装使之可以提交异步请求 context.commit('login',user) } }, mutations传参问题 ​ 如果在调用mutations中的方法时,方法有多个参数,后面的参数是undefined ​ 将参数封装成对象进行传值 注意事项 ​ 如果想获取有返回值的方法(对state中的数据有所更改后),可以在getters中创建方法,并 返回处理后的结果,使用this.$store.getters.方法名获取 ​ vue推荐使用mutations来对state中的数据进行修改,mutations中的方法第一个参数必 须是state,也就是当前vuex中的state,可以直接调用 ​ actions:包裹mutations中的方法,异步调用 ​ 例子: mutations:{ // 第一个参数必须是state addNumS(state,step){ state.num += step }, }, actions: { // 第一个参数必须是context step:参数 addNumSAysc(context,step){ // 利用context来调用mutations的方法来操作state中的数据 context.commit('addNumS',step) } }, 组件之间传值 父传子 ​ 父组件 我是父组件 // 导入子组件 import son from '@/views/Son' export default { name: \"Parent\", data(){ return{ msg : '我是父组件中的数据' } }, // 注册使用 components:{son} } ​ 子组件 我是子组件: export default { name: \"Son\", // 定义props属性用来接收父组件的数据 props:['sonMsg'] } 子传父 ​ 父组件 我是父组件 我是子组件传递到父组件的数据: // 导入子组件 import son from '@/views/Son' export default { name: \"Parent\", data(){ return{ msg : '我是父组件中的数据', sonMs: '' } }, methods:{ sonSMsg(msg){ this.sonMs = msg } }, // 注册使用 components:{son} } ​ 子组件 我是子组件中的按钮 export default { name: \"Son\", data(){ return{ msg:'我是子组件中的数据' } }, methods : { sendMsgToParent(){ this.$emit('fromSon',this.msg) } } } 非父子组件之间传值 ​ 设立公共数据文件 ​ Bus.js // 导入vue import vue from 'vue' // 创建vue对象 export default new vue() ​ Demo01.vue // 导入 兄弟组件 为了将其显示在同一个页面 import demo02 from '@/views/Demo02' // 导入Bus.js 公共数据文件 import bus from './Bus.js' export default { name: \"Demo01\", data(){ return{ msg : '我是demo01中的数据' } }, methods:{ sendMsgToDemo02(){ // 固定格式 bus.$emit('fromDemo02',this.msg) } }, components:{demo02} } ​ Demo02.vue 我是demo02: 我是传递过来的数据:插值表达式 msg // 导入Bus.js 公共数据文件 import bus from './Bus.js' export default { name: \"Demo02\", data(){ return{ msg : '' } }, created() { // 固定格式 data就是传递过来的数据 bus.$on('fromDemo02',data => { this.msg = data }) } } 导航守卫 导航守卫的主要作用就是为了拦截导航路由,使之可以跳转或者取消,有多种方式注册路由导航钩子 全局的 在 main.js中注册一个全局的路由导航钩子 /** * to : 路由将要跳转的路由的信息 * from : 路径跳转前的路径信息 * next : * next() : 放行 * next(false) : 返回原来的页面 * next('路径') : 改变路由跳转地址 跳转指定路由 */ router.beforeEach((to,from,next)=>{ }) 单个路由独享的 { path: '/login', name: 'login', component: Login, beforeEnter(to,from,next) { } } 组件级别的 export default { name: \"Login\", beforeRouteEnter(to, from, next) { console.log(\"准备进入登录\"); next(); }, beforeRouteUpdate(to,from,next){ console.log('路由发生改变,组件被复用时触发') }, beforeRouteLeave (to, from, next) { console.log(\"准备离开登录\"); next(); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"vue/vue理论知识.html":{"url":"vue/vue理论知识.html","title":"Vue理论知识","keywords":"","body":"1.MVVM中,MVVM分别代表什么?作用是什么2.简述什么是单页,以及单页的优缺点3.Vue项目中的src文件夹一般放置那些文件及文件夹4.简述vue的声明周期及钩子函数5.前后台分离,怎么解决跨域问题6.Vue-router的作用是什么7.Vue中父子组件之间如何传值是怎么实现的8.Vue如何传参9.Vuex怎么实现数据共享10.Vue的全家桶有哪些11.Vue的导航守卫是什么?有什么作用?12.Vuex的五大核心属性13.var let const的区别14.Vue中常用的指令15.Vue中的事件修饰符16.什么是js的冒泡事件17.什么是计算属性,如何理解18.import.export导入和导出1.MVVM中,MVVM分别代表什么?作用是什么 ​ MVVM是model-view-viewModel的简写,是MVC的改进版 ​ 为了分离model(模型)和view(视图),再由viewModel将v和m连接起来 ​ 当model(模型)发生改变时,通过MVVM框架自动更新view视图状态 ​ 当view(视图)发生改变时,通过MVVM框架自动更新model模型数据 2.简述什么是单页,以及单页的优缺点 ​ 总:项目中只有一个html页面,由多个组件构成 ​ 优点:速度快,用户体验好,修改内容不会刷新整个页面,因此,spa对服务器的压力也会变小;前后端分离;页面效果炫酷(比如切换页面时的专场动画) ​ 缺点:初次加载耗时长;提高页面复杂度;不利于seo;导航不可用,如果需要使用需要自行完成后退前进功能 3.Vue项目中的src文件夹一般放置那些文件及文件夹 ​ assets:静态资源目录 ​ components:功能组件 ​ views:页面组件 ​ store:vuex的数据 ​ router:路由文件 ​ App.vue:入口页面 ​ main.js:全局的配置文件 4.简述vue的声明周期及钩子函数 ​ vue的生命周期就是该vue实例从创建到销毁的过程 ​ beforeCreat:创建完成之前,数据未完成初始化 ​ created:创建完成后,数据已完成初始化,一般用于页面渲染 ​ beforeMount:双向绑定前 ​ mounted:双向绑定后 ​ beforeUpdate:更新前 ​ updated:更新后 ​ beforeDestroy:销毁前 ​ destroyed:销毁后 5.前后台分离,怎么解决跨域问题 ​ 1.jsonp:叫古老的方式,利用script标签可跨域的特性,缺点是只能发送get请求 ​ 2.nginx:配置代理服务器,使其可以访问目标服务器,然后再将目标服务器返回的数据返回到我们的客户端,可以发送各种请求 ​ 3.比较常用的cors,需要后端设置Access-control-Allow-Origin头,可以自行配置可跨域的地址,缺点是可能会发生两次请求 6.Vue-router的作用是什么 ​ vue-router是vue.js官方的路由管理器 ​ :完成组件之间的切换 ​ this.$route : 完成组件之间的传参 ​ this.$router.push():完成组件之间的跳转 7.Vue中父子组件之间如何传值是怎么实现的 ​ 1.父传子:在子组件中设置props属性用于接受父组件传递的数据 ​ 2.子传父:子组件触发自身的方法,使用$emit调用父类的监听的方法 ​ 3.非父子组件:需要设立公共的文件 8.Vue如何传参 ​ 1.query+path 传参: 获取值: this.$route.query.参数名 ​ 2.params+name 传参: 获取值: this.$route.params.参数名 ​ 3.路由传参(地址栏) 在路由组件的配置上添加 { path:'/path/:参数名' name: component: } 获取值: this.$route.params.参数名 9.Vuex怎么实现数据共享 ​ vue整合vuex,在main.js中以组件的方式导入store.js文件 ​ 在store.js文件的state区域保存数据,使之可以在任何位置可被访问 ​ 使用store.js文件中的mutations可以更新state区域的数据,通过读写完成数据的共享 10.Vue的全家桶有哪些 ​ Vue的两大核心: ​ 组件化:将一个整体应用,拆分成多个可复用的个体(组件) ​ 数据驱动:在修改数据的前提下,不操作dom,直接影响bom显示 ​ vue-router:路由,组件之间的切换 ​ vue-cli:构建vue单页应用的脚手架工具 ​ vuex:状态数据 ​ axios:vue官方推荐的发送http请求的工具包 11.Vue的导航守卫是什么?有什么作用? ​ vue-rouer提供的导航钩子,主要用来拦截导航,完成跳转或取消,有多种方式可以在路由导航发生时执行路由导航钩子 ​ 全局的: ​ router.beforeEach在全局注册一个before钩子 ​ 单个路由独享的: ​ 在路由配置中直接定义一个beforeEnter路由导航钩子 ​ 组件级别的: ​ beforeRouteEnter,beforeRouteUpate,beforeRouteLeave ​ 直接在路由组件中定义路由导航钩子 12.Vuex的五大核心属性 ​ Vuex是专门为vue.js应用设计的状态管理架构 ​ 1.state:基本数据 ​ 2.getters:从基本数据派生的数据 ​ 3.mutations:更改数据的提交方式,同步 ​ 4.actions:像一个装饰器,用来包裹mutations,使之可以异步 ​ 5.modules:模块化vuex 13.var let const的区别 ​ var是es3提供的创建变量的关键字,定义的变量为全局变量 ​ let是es6新增的定义变量的关键字,定义的变量是局部变量 ​ const定义的变量的常量 14.Vue中常用的指令 ​ v-if:当条件为true时,显示该元素 ​ v-show:修改页面元素的css样式完成显示或隐藏 ​ v-on:为标签添加事件 ​ v-model:双向绑定,用于数据和视图的同步 ​ v-bind:将数据绑定的标签的属性上 ​ v-for:遍历数组 15.Vue中的事件修饰符 ​ .stop:阻止冒泡 ​ .prevent:禁止默认事件 ​ .capture:捕获 ​ .once:只执行一次 ​ .self:只在自身触发 16.什么是js的冒泡事件 ​ 当触发页面中标签的事件时,会同时触发其所有父与子标签的事件,直到跟标签 17.什么是计算属性,如何理解 ​ 计算属性本质是一个有返回值的方法,在页面渲染时,可以将该方法当做一个属性使用,当data中的数据未发生改变时,计算属性直接读取缓存 18.import.export导入和导出 ​ 是es6中,module主要构成的两个命令 ​ export:用于导出模块中的变量的接口 ​ import:用于在一个模块中导入另一个含有export接口的接口 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"frontend/es6/es6.html":{"url":"frontend/es6/es6.html","title":"ES6","keywords":"","body":"ES6var,let和constvarletconst字符串模板解构表达式数组对象函数函数参数默认值箭头函数对象中的函数进行简写箭头函数结合解构表达式map和reducemapreduce扩展运算符set和mapsetmapclass(类的基本用法)创建class类的继承修饰器(Decorator)ES6 ECMAScript 和 JavaScript 的关系是，前者是后者的规格，后者是前者的一种实现（另外的 ECMAScript 方言还有 JScript 和 ActionScript）。日常场合，这两个词是可以互换的。 var,let和const var for (var i = 0; i 控制台打印结果 可以看到在循环外依然可以获得使用var定义的变量i的值并且进行了一次自增,可以得出var定义的变量是全局变量 let 将var改为let for (let i = 0; i 控制台打印结果 此时的打印结果 i未定义 说明在循环外是拿不到使用let定义的变量的值的 let定义的变量是局部变量 const const num = 1; console.log(num); num = 2; console.log(num); 在idea中这样写是会报编译错误的,同时可以看到控制台也会报同样的错误 不能重新复制给常量,同时该常量在声明时必须进行初始化 字符串模板 `` 符号中的字符串可以任意换行 let str = `java PHP python c++ `; console.log(str); 解构表达式 在ES6可以使用一中格式从数组或者对象中提取值,然后对变量赋值 数组 let arr = [1,2,3]; // 之前是通过数组的索引拿值,现在可以有直接将数组中的值赋值到对应的变量上 let[a,b,c] = arr; console.log(a,b,c) // 也可以进行单个变量的结构并赋值 const[d] = arr; console.log(d); 对象 let person = { name : 'zhangsan', age : 24, hobbies : ['游泳','网球'] }; // 结构对象,跟数组不同的是将中括号改为了大括号 // 如果需要该名称按照name的写法便可改变,前面的name是对象中的属性 // 后面是新的变量名 const{name:newName,age,hobbies} = person; // 使用新的变量名获取值 console.log(newName,age); console.log(hobbies); 函数 函数参数默认值 function fun(a = 1) { console.log(a) } fun(); fun(2) 箭头函数 // 一个参数时 let print = function (obj) { console.log(obj); }; // 可以省略小括号 let print2 = obj => console.log(obj); // 两个参数时 let sum = function (a , b) { return a + b; }; // 简写为 当有返回值并且只有返回值一行代码,可以省略大括号以及return 反之必须加大括号 let sum2 = (a,b) => a+b; console.log(sum2(1,2)) // 没有参数,使用小括号进行占位 let noParam = () => console.log('es6') // 多行代码,用大括号括起来 let fun = () =>{ console.log('PHP'); console.log('python'); }; fun(); 对象中的函数进行简写 let person = { name: \"jack\", // 以前： eat: function (food) { console.log(this.name + \"在吃\" + food); }, // 箭头函数版： // 这里拿不到this eat2: food => console.log(person.name + \"在吃\" + food), // 简写版： eat3(food){ console.log(this.name + \"在吃\" + food); } } 箭头函数结合解构表达式 let person = { name:'zhangsan' } function printName(person) { console.log(person.name) } printName(person) // 进行结合 var printName2 = ({name}) => console.log('使用箭头函数打印的:'+name) printName2(person) map和reduce map map(): 接收一个函数,将原数组中的所有元素经过该函数处理后放入新数组返回 // 将数组中的所有字符转为数字 let arr = ['1','2','3'] let newArr = arr.map(ele => parseInt(ele)) console.log(newArr) reduce reduce(): 接收一个函数(必须)和一个初始值(可选),该函数接收两个参数 ​ 第一个参数是上一次reduce处理的结果 ​ 第二个参数是数组中要处理的下一个参数 reduce会将数组的元素从左向右依次经过该函数处理后进行返回,函数的第一个参数是上一次经过函数处理的返回值,如果没有初始值,会从前两个参数开始,如果有初始值,从初始值开始 let arr = [1,2,-3,4] // 会将数组从左向右进行相加 console.log(arr.reduce((a,b) => a+b)) // 效果同上一样,只不过第一次是a是该函数的初始值 console.log(arr.reduce((a,b) => a+b,10)) 扩展运算符 扩展运算符是三个点(...),将数组转为用逗号分隔的参数数列 console.log(...[1,2,4]) var array = [6,7,8] console.log(5,...array) // 传参 function add(a,b) { return a+b; } var arr = [1,2,3] var result = add(...arr) console.log(result) // 与结构表达式结合 const [start,...others] = [1,2,3,4] console.log(start,others) // 将字符串转成字符数组 const str = 'java' console.log(...str) set和map set 本质与数组类似,但set中不能存储相同的元素,与java中set集合类似 // Set构造函数可以接收一个数组或空 let set = new Set(); set.add(1);// [1] // 接收数组 let set2 = new Set([2,3,4,5,5]); 常用方法 set.add(1);// 添加 set.clear();// 清空 set.delete(2);// 删除指定元素,并不是索引 set.has(2); // 判断是否存在 set.forEach(function(){})//遍历元素 set.size; // 元素个数。是属性，不是方法。 map map中的键和值可以是任何类型,如果添加时有相同的key则会将对应的value替换成新值 // map接收一个数组，数组中的元素是键值对数组 const map = new Map([ ['key1','value1'], ['key2','value2'], ]) console.log(map) // 或者接收一个set const set = new Set([ ['key1','value1'], ['key2','value2'], ]) const map2 = new Map(set) // 或者其它map const map3 = new Map(map); console.log(map2) console.log(map3) 常用方法 map.set(key, value);// 添加 map.clear();// 清空 map.delete(key);// 删除指定元素 map.has(key); // 判断是否存在 map.forEach(function(key,value){})//遍历元素 map.size; // 元素个数。是属性，不是方法 map.values() //获取value的迭代器 map.keys() //获取key的迭代器 map.entries() //获取entry的迭代器 用法： for (let key of map.keys()) { console.log(key); } 或： console.log(...map.values()); //通过扩展运算符进行展开 class(类的基本用法) 创建class 自定义类并创建对象调用类属性和方法 class Person{ // 构造方法 constructor(name,age = 20){ this.name = name; this.age = age; } // 普通方法,需要创建对象调用 printAge(){ console.log(this.age) } // 静态方法,可以使用类名直接调用 static printPerson(obj){ console.log('我是静态方法'+obj) } } Person.printPerson(123) let person = new Person('zhangsan'); person.printAge() 类的继承 结合上个类使用 class Programmer extends Person{ constructor(){ super('java程序员',30) this.address = '河北' } } let programmer = new Programmer(); console.log(programmer.address) programmer.printAge() Programmer.printPerson('子类调用父类继承的静态方法') 修饰器(Decorator) 是一个函数,用来修改类的行为,需要配合转码器进行使用 //通过@符号进行引用该方法，类似java中的注解 @T class User { constructor(name, age = 20){ this.name = name; this.age = age; } } //定义一个普通的方法 function T(target) { console.log(target); //target对象为修饰的目标对象，这里是User对象 target.country = \"中国\"; //为User类添加一个静态属性country } console.log(User.country); //打印出country属性值 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"frontend/node/node.html":{"url":"frontend/node/node.html","title":"Node的安装","keywords":"","body":"Node.js什么是Node.js安装与配置Node.js 什么是Node.js ​ Node.js是一个服务器端的JavaScript运行环境,可以让前端使用Node.js提供HTML,CSS,JS等资源访问,提供npm插件用于管理所有js资源,和java中的maven类似 安装与配置 最新版本: https://nodejs.org/zh-cn/download/ 历史版本: https://nodejs.org/zh-cn/download/releases/ windows下载msi文件后傻瓜式安装 查看node版本: node -v node -version 10版本方式 切换源工具 安装完node后会自带一个npm,但默认的源在国外,访问会比较慢,切换成国内源 安装nrm npm install -g nrm 查看当前源 nrm ls 切换源 nrm use taobao 安装(下载)包,版本是可选项,下载后在node_modules文件夹里 npm install [@] 全局安装 npm install -g 全局卸载 npm uninstall -g yarn install时忽略引擎版本 yarn install --ignore-engines Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"frontend/mock/mock.html":{"url":"frontend/mock/mock.html","title":"Mock","keywords":"","body":"Mock1.什么是mock.js2.入门使用2.1测试2.2全局使用3.使用EasyMock3.1 环境准备3.2 创建接口3.3 在Vue中配合axios使用3.4 可能出现的问题Mock 1.什么是mock.js ​ Mock.js是一款模拟数据生成器,旨在帮助前端工程师独立于后端进行开发,可以拦截ajax请求生成模拟数据进行接口测试,其模拟的数据有随机性,可以自行配置,支持的数据类型丰富,包括不限于文本,邮箱,图片,颜色等 ​ 官方文档很详细 2.入门使用 了解mock.js功能后,还需要结合项目使用,该示例在vue下 2.1测试 安装 npm install mockjs 在script标签中使用 // 使用 Mock var Mock = require('mockjs') var data = Mock.mock({ // 属性 list 的值是一个数组，其中含有 1 到 10 个元素 'list|1-10': [{ // 属性 id 是一个自增数，起始值为 1，每次增 1 'id|+1': 1 }] }) // 输出结果 参数4为数据打印格式,空格 console.log(JSON.stringify(data, null, 4)) 2.2全局使用 需要先安装axios和mockjs npm install axios npm install mockjs 在src下创建mock/mock.js文件 示例 // es6语法引入mock模块 import Mock from 'mockjs'; // 导出接口 export default Mock.mock( '/api/get', { 'list|1-10':[{ 'id|+1': 1, 'age|1-10': 10 }] //还可以自定义其他数据 } ); Mock.mock( '/api/post', { 'list|3':[{ 'number|+1': 1, 'string': '@string(3,5)' }] } ) 在main.js中全局引入 名字随便起,mock会自动拦截ajax请求 import mymock from './mock/mock.js'; 创建vue文件并且使用 : : export default { name: \"MockDemo\", data() { return { list : [], arr : [], } }, methods: { async fun1(){ let {data} = await this.axios.get('/api/get') this.list = data.list }, async fun2(){ let {data} = await this.axios.post('/api/post') this.arr = data.list } }, created() { this.fun1() this.fun2() } } 启动测试 npm run serve 3.使用EasyMock 采用自行搭建的方式 3.1 环境准备 环境准备,其中node版本可能需要使用8(从10降下来的) 克隆EasyMock并且安装依赖 $ git clone https://github.com/easy-mock/easy-mock.git $ cd easy-mock && npm install 运行 npm run dev 默认端口7300,可以在config/default.json中修改配置 3.2 创建接口 访问http://127.0.0.1:7300 注册账号登陆后,创建接口 自定义数据模板 3.3 在Vue中配合axios使用 默认路径配合axios的配置 在main.js指定请求路径前缀 import axios from 'axios' // axios.defaults.baseURL='后端真实接口' // 设置为下图的Base URL即可 axios.defaults.baseURL='http://localhost:7300/mock/5e684a733d64e467945f8c30/example' methods: { async fun(){ let {data} = await this.axios.get('/xxxServie') console.log(data.list) } }, 可以看到模拟数据和使用真实数据,只需要更改main.js中的axios的默认请求路径即可 mockjs会自动拦截请求 3.4 可能出现的问题 node的版本过高,需要使用8的版本 redis和mongo的环境变量设置 注册服务或者开启服务需要使用管理员命令窗口,有两种命令行可用 win+x+a 或者在文件夹中 shift+右键 EasyMock运行后报错,可以尝试修改其config/default.json中的 该错误可能是node8.9.4版本的错误 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"frontend/nuxt/nuxt.html":{"url":"frontend/nuxt/nuxt.html","title":"Nuxt","keywords":"","body":"Nuxt1.安装及创建1.1 环境准备1.2 创建项目1.3 项目结构1.4 编译运行2.路由2.1静态路由2.2动态路由3.布局文件4.结合Axios4.1使用axios4.2使用 @nuxtjs/axios设置baseURL5.异步数据Nuxt Nuxt是一个服务器端渲染技术(SSR server side render),基于Vue之上 官网 : https://zh.nuxtjs.org/guide ​ 之前使用的Vue是客户端渲染,也就是客户端向服务器发送请求,服务端将数据返回给客户端,客户端将数据填充到占位符中,但这种方式不利于SEO ​ SEO:搜索引擎只会采集页面中的静态数据,对网站进行收录,可以让更多人访问到,并不会采取ajax返回的数据,服务器端渲染技术可以处理这个缺点 ​ 服务器端渲染技术,其实在学传统web开发时就已经接触过了,jsp时代便是使用tomcat进行编译,将数据提前填充拼接到HTML中返回给客户端,客户端直接渲染HTML就可以了,以及其他的模板引擎,比如freemark等 ​ Nuxt使用Node作为服务器进行渲染,但也并不是所有代码都运行在Node中,比如alert、localStorage、window、location、document这些客户端特有的代码(当然也有使用运行的时机,在之前单纯的Vue中就是可以的),该方式固然对服务器性能要求高,所以根据业务进行选用即可,比如新闻,博客,资讯等 1.安装及创建 1.1 环境准备 安装node 安装vue-cli，nuxt是基于vue之上的 Vue文档 https://cli.vuejs.org/zh/guide/installation.html npm install -g vue-cli 安装nuxt项目 yarn global add create-nuxt-app 1.2 创建项目 npx create-nuxt-app 创建项目的选项 同时Nuxt也支持热更新,即在修改代码后会自动重新编译,并且影响页面显示 1.3 项目结构 1.4 编译运行 # 安装项目所有依赖 npm run install # 服务在热重载状态下运行在localhost:3000 npm run dev # 编译成上线项目并且启动服务 npm run build npm run start 2.路由 2.1静态路由 没有参数的路由,在Nuxt中页面组件都是放在pages中,并且会根据文件夹及文件名称进行路由,如果vue页面名称为index则默认为当前文件夹的默认访问页面,页面组件名称一般为小写(大写也可访问),并且在浏览器输入地址路由是也可不区分大小写 文件名 URL访问 pages/index.vue / pages/home.vue /home pages/goods/index.vue /goods pages/goods/list.vue /goods/list 2.2动态路由 带有参数的路由地址 2.2.1 /路由地址/参数 页面组件: pages/goods/_id.vue 访问地址: http://localhost:3000/goods/123 获取数据 : this.$route.params.id // 此处的id同样根据组件的名称获取 如果需要获取多个参数如 /pages/路由/参数1/参数2 只需进行文件夹的嵌套,只需在前面加_(下划线)即可,如下 /pages/路由/_文件夹/ _xxx.vue 获取方式和上面一样,根据文件夹名称和组件名称分别获取(去掉下划线)即可 第一个参数: 第二个参数: 可能出现的问题 ERROR [Vue warn]: Invalid component name: \"_id\". Component names should conform to valid custom element name in html5 specification. 解决方法:将组件name中的_去掉 2.2.2 /路由地址?key=value 页面组件: pages/home.vue 访问地址: http://localhost:3000/home?id=123&name=111 获取数据 : id:{{this.$route.query.id}} name:{{this.$route.query.name}} 同样可以使用 this.$router.push的方式跳转路由并且传递参数,同样支持params和query两种方式 3.布局文件 在项目的layouts文件夹中默认有default.vue文件,项目中所有页面默认使用该布局 定义布局在layouts中创建布局文件(.vue) 在head中设置头信息 页头 // 引入该布局的页面组件会在这里显示 页尾 // 添加头信息(添加在布局文件和页面组件都可以),导入css和js export default { name: \"MyLayout\", head:{ title : 'XXX页面', meta:[ {charset:'utf-8'}, {name:'keywords',content:'关键字'}, {name:'description',content:'描述'} ], link:[ {rel:'stylesheet' , href :\"/style/header.css\"}, {rel:'stylesheet' , href :\"/style/footer.css\"} ], script:[ {type:'text/javascript',src:'js/jquery-1.8.3.min.js'} ] }, data() { return {} }, methods: {} } 在需要该布局的页面组件中引用 export default { // 根据布局组件名称引用 layout : 'MyLayout', name: \"home\", } 1.可以在nuxt.config,js中可以使用head设置所有页面公共的head信息(标题、编码、meta等) 2.在每个页面中可以使用head属性设置这个页面的head信息 3.如果页面中的head信息如果添加hid属性,就会把公共的相同的hid属性要盖掉，否则就添加 两次 4.结合Axios 8个生命周期函数,除了以下两个之外,其余6个基本在客户端执行,所以选择在(mounted)挂载前初始化数据 创建前beforeCreated:执行两次,客户端和服务端都会执行一次 创建后created:执行两次,客户端和服务端都会执行一次 4.1使用axios 安装axios npm install axios 该方式需要注意的是如果在多个页面同时引用的话会被打包两次,在nuxt.config.js中配置避免该情况 build: { /* ** You can extend webpack config here */ extend (config, ctx) { }, vendor: ['axios'] } import axios from 'axios' export default { name: \"list\", data(){ return{ persons:[] } }, methods: { async findAll() { let {data} = await axios.get('url') this.persons = data.list }, }, mounted() { this.findAll() } } 4.2使用 @nuxtjs/axios 在项目创建时会有该选项,也可手动安装 npm install @nuxtjs/axios 在nuxt.config.js中进行配置 modules: [ '@nuxtjs/axios', ], axios: { // proxyHeaders: false }, 使用 // import axios from 'axios' export default { name: \"list\", data(){ return{ persons:[] } }, methods: { async findAll() { // 少了axios中的data属性,直接解构数据即可 console.log(this.$axios.$get('url')) let {list} = await this.$axios.$get('url') console.log(list) this.persons = list }, }, mounted() { this.findAll() }, } 设置baseURL 最简单的方式,在nuxt.config.js中修改配置 modules: [ // Doc: https://axios.nuxtjs.org/usage '@nuxtjs/axios', ], /* ** Axios module configuration ** See https://axios.nuxtjs.org/options */ axios: { baseURL: \"http://127.0.0.1:7300/mock/5e702522021bfd53ec2f7309/leyou\" }, 使用 async loginSubmit(){ let {data} = await this.$axios.$post('/auth-service/login',this.loginForm) } 在src下创建SysConfig.js文件 var sysConfig = { apiURL: '', staticURL: '', baseURL: 'http://127.0.0.1:7300/mock/5e702522021bfd53ec2f7309/' }; var person = { name : 'Tao' } export {sysConfig,person} 其他组件中引入使用 import {sysConfig} from \"@/plugins/SysConfig\"; import VerifyCode from '@/components/VerifyCode' export default { name: \"regist\", layout:\"loginAndRegister\", components:{ VerifyCode }, data(){ return{ regForm:{} } }, methods:{ async register(){ let {data} = await this.$axios.$post(sysConfig.baseURL+'/web-service/regist',this.regForm) console.log(data) if (data.errno == 1){ this.$router.push(\"/login\") } }, } } 5.异步数据 ​ 使用 AJAX 获取的数据都属于异步数据，这种数据默认是无法直接写在页面中的，也就是说在页面 的源代码中是看不到 AJAX 的数据的（在网页源代码中并没有）。 ​ AJAX获取的数据并不利于SEO，所以为了解决这个问题，Nuxt中提供了一个 asyncData ，可以让 我们在服务器端获取 AJAX 的数据，返回给客户端一个已经拥有AJAX数据的页面 // 与methods平级 async asyncData({$axios}){ let {list} = await $axios.$get('url') console.log(list) // 特殊的返回数据方式,必须要有返回值 return {persons:list} } 接收参数数据 async asyncData({$axios,params,query}){ console.log(params.id) console.log(query.id) let {list} = await $axios.$get('url') console.log(list) return {persons:list} } 分别使用params和query的方式传递参数 在其他页面组件中添加点击事件 goList(){ this.$router.push({ name : 'list', params: { id : 456 } }) // this.$router.push({ // path : 'list', // query: { // id : 888 // } // }) } 使用地址栏的方式 http://localhost:3000/list?id=888 配合this.$route.query.key获取数据 浏览器展示数据 开发工具控制台打印,使用console.log的方式,但是打印在了开发工具中,并没有在浏览器中,说明在服务器端已经获取到数据了 此时在看网页源代码已经有ajax请求的数据了(使用vue和nuxt比较死数据时,nuxt的网页源代码也会有数据,而普通的vue的网页源代码中并没有) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"frontend/other/html.html":{"url":"frontend/other/html.html","title":"HTML","keywords":"","body":"HTMLHTML常见元素HTML版本HTML元素分类块级元素block内联元素inline内联块状inline-blockHTML元素嵌套关系HTML元素默认样式和定制化HTML 前端三大件:HTML,CSS,JavaScript行为 HTML是CSS的基石 如何理解HTML? 可以把HTML看作是一个文档,元素也就是描述文档的结构,有区块和大纲 HTML常见元素 Document 链接 百度 谷歌 表格 表头1 表头2 表头3 数据1 数据2 数据3 数据1 数据3 数据1 数据2 数据3 数据2 数据3 表单 一 二 游泳 跑步 跳绳 选项一 选项二 搜索 时间 日期 普通按钮 提交按钮一 重置按钮 HTML版本 HTML5新增内容,主要为了更加语义化,并且语法没有像XHTML那么严格 比如新区块标签:section article nav aside HTML元素分类 这里主要按照样式分 块级元素block 大多为大纲元素 设置display:block就是将元素显示为块级元素 常见:、、...、、、、、、 、 特点 1.每个块级元素都会独占一行 2.元素的高度、宽度、行高以及顶和底边距都可设置 3.元素宽度在不设置的情况下,默认是其父容器的100% 内联元素inline 可能不一定有规则的形状,没有尺寸概念 大多为文本元素 块状元素也可以设置display:inline变为行内元素 常见:、、、、、、、、、、 从名字上看就大概知道其特点 1.和其他元素都在一行上 2.元素的高度、宽度、行高及顶部和底部边距不可设置 3.元素的宽度就是它包含的文字或图片的宽度,不可改变 内联块状inline-block 将元素设置为内联块状:display:inline-block 常见:、以及表单元素 同时具备内联元素、块状元素的特点 1.和其他元素都在一行上 2.元素的高度、宽度、行高以及顶和底边距都可设置 HTML元素嵌套关系 块级元素可以包含行内元素 块级元素不一定能包含块级元素 行内元素一般不能包含块级元素 HTML元素默认样式和定制化 html的元素在浏览器渲染时都有其默认的样式,如果需要统一可以使用CSS Reset进行重置 简便方法,*选取所有元素,将其内外边距都设置为0 *{ margin:0; padding:0; } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"frontend/other/JSON对象的两个方法.html":{"url":"frontend/other/JSON对象的两个方法.html","title":"JSON对象","keywords":"","body":"JSON对象的两个方法JSON.parse(string)JSON.stringify(obj)数组toJSON问题JSON对象的两个方法 JSON.parse(string) ​ 接受一个JSON字符串并将其转换成一个JavaScript对象 JSON.stringify(obj) ​ 接受一个JavaScript对象并将其转换为一个JSON字符串 // 创建一个字符串 var userStr = '{\"name\":\"taoqz\",\"gender\":\"man\",\"age\":\"18\"}'; // 打印字符串并且验证其类型 console.log(userStr) // {\"name\":\"taoqz\",\"gender\":\"man\",\"age\":\"18\"} // 可以查看对象的类型 console.log(typeof (userStr)); // string // 将一个标准的json字符串转为JavaScript对象 var userToObJ = JSON.parse(userStr) // {name: \"taoqz\", gender: \"man\", age: \"18\"} // age: \"18\" // gender: \"man\" // name: \"taoqz\" // __proto__: Object console.log(userToObJ) // object console.log(typeof (userToObJ)); // 将一个JavaScript对象转换为一个JSON字符串 var objToStr = JSON.stringify(userToObJ); console.log(objToStr) // string console.log(typeof objToStr) 数组 // 在数组上同样适用 const arr = ['Java', 'Python', 'PHP'] console.log(arr) console.log(typeof arr) // object const arrStr = JSON.stringify(arr); console.log(arrStr) console.log(typeof arrStr) // string JSON.parse()方法的第二个参数 console.log('---------------------------') let empStr = '{\"name\":\"taoqz\",\"gender\":\"man\",\"age\":18}' // JSON.parse()可以接受第二个参数,可以对对象值的属性进行操作 const newEmp = JSON.parse(empStr, (key, value) => { console.log('key:' + key + \" value:\" + value) // return value; // 如果value是string类型,该键值对将不会被过滤 if (typeof value === 'string') { // 可以对value进行操作 // return value.toUpperCase(); // 返回undefined 将会排除该键值对 return undefined } return value; }) // 最终结果只有age console.log(newEmp) JSON.stringify的三个参数 console.log('---------------------------') let emp = { name: 'taoqz', gender: 'man', age: 18, hobbies:[ 'swim', 'run' ], getHeight:function(){ return '1.78'; } }; // 分别接受了三个参数,第一个参数是要被转换的JavaScript对象 // 第二个参数是一个方法,同样支持可以通过对key进行判断及对value进行操作 // 第三个参数是用来在打印时进行格式化 let empStr2 = JSON.stringify(emp,function (key,value) { // 将会在序列化时排除age这个key if (key === 'age') { return undefined // 因为序列化时,默认是不会序列化方法的,这会使方法加入序列化 }else if(typeof(value) == 'function'){ return Function.prototype.toString.call(value) } return value; },4); // { // \"name\": \"taoqz\", // \"gender\": \"man\", // \"hobbies\": [ // \"swim\", // \"run\" // ], // \"getHeight\": \"function(){\\n return '1.78';\\n }\" // } console.log(empStr2) toJSON问题 console.log('--------------------') let myObj = { name : 'myObj', age : 18, toJSON: function () { return 'toJSON!!' } } // 只会序列化toJSON方法的返回值 // \"toJSON!!\" console.log(JSON.stringify(myObj)) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"jwt/JWT.html":{"url":"jwt/JWT.html","title":"JWT","keywords":"","body":"JWT为什么使用jwt流程token包含的内容JWT详解使用结合RSA生成加密token配合zuul网关实现鉴权JWT ​ 官网:https://jwt.io/ ​ JWT，全称是Json Web Token， 是JSON风格轻量级的授权和身份认证规范，可实现无状态、分布式的Web应用授权；它是分布式服务权限控制的标准解决方案！ 为什么使用jwt ​ 在之前的单体项目中,用户登录后将信息存储到session中,从而判断当前用户是否登录限定请求资源,但现在是前后端分离,如果还是用session,后端服务由于要保证高可用使用了集群,此时便需要实现session之间的共享,使用jwt避免这些麻烦的操作 流程 ​ 用户请求登录服务后,请求成功颁发jwt凭证,也就是token,将token保存到客户端中,此后的每次请求客户端都会携带此凭证到独立的验证授权中心进行校验 token包含的内容 JWT的token包含三部分数据： Header：头部，通常头部有两部分信息： 声明类型type，这里是JWT（type=jwt） 加密算法，自定义(rs256/base64/hs256) 我们会对头部进行base64加密（可解密），得到第一部分数据 Payload：载荷，就是有效数据，一般包含下面信息： 用户身份信息-userid,username（注意，这里因为采用base64加密，可解密，因此不要存放敏感信息） 注册声明：如token的签发时间，过期时间，签发人等 这部分也会采用base64加密，得到第二部分数据 Signature：base64加密，签名，是整个数据的认证信息。一般根据前两步的数据，再加上服务的的密钥（secret，盐）（不要泄漏，最好周期性更换），通过加密算法生 JWT详解 jwt的一个有规则的token 它有三部分组成：Header.payload.signature,每部分都是通过base64加密而成的 jwt每个部分都是可以解密的 JWT详解 base64编码原理 Base64编码之所以称为Base64，是因为其使用64个字符来对任意数据进行编码，同理有Base32、Base16编码。标准Base64编码使用的64个字符如下： 这64个字符是各种字符编码（比如ASCII码）所使用字符的子集，并可打印。唯一有点特殊的是最后两个字符。 Base64本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续6比特（2的6次方=64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。假设我们对Hello！进行Base64编码，按照ASCII表，其转换过程如下图所示： 可知Hello！的Base64编码结果为SGVsbG8h，原始字符串长度为6个字符串，编码后长度为8个字符，每3个原始字符经编码成4个字符。 但要注意，Base64编码是每3个原始字符编码成4个字符，如果原始字符串长度不能被3整除，怎么办？使用0来补充原始字符串。 以Hello！！为例，其转换过程为： Hello!! Base64编码的结果为 SGVsbG8hIQAA 。最后2个零值只是为了Base64编码而补充的，在原始字符中并没有对应的字符，那么Base64编码结果中的最后两个字符 AA 实际不带有效信息，所以需要特殊处理，以免解码错误。 标准Base64编码通常用 = 字符来替换最后的 A，即编码结果为 SGVsbG8hIQ==。因为 = 字符并不在Base64编码索引表中，其意义在于结束符号，在Base64解码时遇到 = 时即可知道一个Base64编码字符串结束。 如果Base64编码字符串不会相互拼接再传输，那么最后的 = 也可以省略，解码时如果发现Base64编码字符串长度不能被4整除，则先补充 = 字符，再解码即可。 解码是对编码的逆向操作，但注意一点：对于最后的两个 = 字符，转换成两个A 字符，再转成对应的两个6比特二进制0值，接着转成原始字符之前，需要将最后的两个6比特二进制0值丢弃，因为它们实际上不携带有效信息。 总结： 1、base64的编码/加密原理 答：原理：将键盘输入的字符用base64编码表示 过程：将键盘输入字符的ascii码值，转成的对应8位二进制，将该二进制6个一组拆分，并计算拆分之后的十进制值，找出十进制值在base64编码中对应的字母，即完成base64加密 成。用于验证整个数据完整和可靠性 使用 依赖 0.7.0 2.9.6 io.jsonwebtoken jjwt ${jjwt.version} joda-time joda-time ${joda-time.version} 工具类 import io.jsonwebtoken.Claims; import io.jsonwebtoken.JwtBuilder; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; import org.joda.time.DateTime; import javax.crypto.spec.SecretKeySpec; import javax.xml.bind.DatatypeConverter; import java.security.Key; public class JWTUtil { /** * 获取token中的参数 * * @param token * @return */ public static Claims parseToken(String token,String key) { if (\"\".equals(token)) { return null; } try { return Jwts.parser() .setSigningKey(DatatypeConverter.parseBase64Binary(key)) .parseClaimsJws(token).getBody(); } catch (Exception ex) { return null; } } /** * 生成token * * @param userId * @return */ public static String createToken(Integer userId,String key, int expireMinutes) { SignatureAlgorithm signatureAlgorithm = SignatureAlgorithm.HS256; long nowMillis = System.currentTimeMillis(); //生成签名密钥 byte[] apiKeySecretBytes = DatatypeConverter.parseBase64Binary(key); Key signingKey = new SecretKeySpec(apiKeySecretBytes, signatureAlgorithm.getJcaName()); //添加构成JWT的参数 JwtBuilder builder = Jwts.builder() // .setHeaderParam(\"type\", \"JWT\") // .setSubject(userId.toString()) .claim(\"userId\", userId) // 设置载荷信息 .setExpiration(DateTime.now().plusMinutes(expireMinutes).toDate())// 设置超时时间 .signWith(signatureAlgorithm, signingKey); //生成JWT return builder.compact(); } } 结合RSA生成加密token配合zuul网关实现鉴权 github:https://github.com/TaoQZ/jwt-rsa-auth.git 非对称加密 https://blog.csdn.net/jijianshuai/article/details/80582187 加密技术是对信息进行编码和解码的技术，编码是把原来可读信息（又称明文）译成代码形式（又称密文），其逆过程就是解码（解密），加密技术的要点是加密算法，加密算法可以分为三类： 对称加密，如AES 基本原理：将明文分成N个组，然后使用密钥对各个组进行加密，形成各自的密文，最后把所有的分组密文进行合并，形成最终的密文。 优势：算法公开、计算量小、加密速度快、加密效率高 缺陷：双方都使用同样密钥，安全性得不到保证 非对称加密，如RSA 基本原理：同时生成两把密钥：私钥和公钥，私钥隐秘保存，公钥可以下发给信任客户端 私钥加密，持有公钥才可以解密 公钥加密，持有私钥才可以解密 优点：安全，难以破解 缺点：算法比较耗时 不可逆加密，如MD5，SHA 基本原理：加密过程中不需要使用密钥，输入明文后由系统直接经过加密算法处理成密文，这种加密后的数据是无法被解密的，无法根据密文推算出明文。 RSA算法历史：底层-欧拉函数 1977年，三位数学家Rivest、Shamir 和 Adleman 设计了一种算法，可以实现非对称加密。这种算法用他们三个人的名字缩写：RSA Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"jwt/认证、授权、鉴权、权限控制.html":{"url":"jwt/认证、授权、鉴权、权限控制.html","title":"认证、授权、鉴权、权限控制","keywords":"","body":"认证、授权、鉴权、权限控制认证(identification)授权(authorization)鉴权(authentication)权限控制(access / permisson control)总结参考链接认证、授权、鉴权、权限控制 认证(identification) ​ 认证是指根据操作者特有的识别信息,确认其身份,常见的比如用户名和密码,身份证,手机号,邮箱,指纹,虹膜等 ​ 也就是为了确认当前用户信息 授权(authorization) ​ 授权一般是指用户在成功通过认证后,系统颁发的唯一标识信息,用户可以根据这个授权信息对系统资源进行操作,而不用每次去手动认证。最常见的就是服务端颁发一个token。 鉴权(authentication) ​ 鉴权是指对一个操作者的授权信息的鉴别确认过程,由于不需要用户每次手动去认证,通过服务端颁发的授权信息进行自动认证,而授权信息也是有可能会被伪造的,所以还需要鉴别授权信息在系统中是否是真实存在的。 权限控制(access / permisson control) ​ 权限控制也可看作为两部分,权限和控制,权限一般会定义预置好是一个可执行操作的集合,一般会对角色分配权限,控制是指对当前操作者所执行的操作进行判断,决定是否允许当前操作的执行。 总结 定义 英文 实现方式 认证 确认声明者的身份 identification 根据声明者独特的识别信息 授权 获取用户的委派权限 authorization 颁发一个授信媒介，不可被篡改，不可伪造，受保护 鉴权 对所声明的权限真实性进行鉴别的过程权限是一个抽象的逻辑概念，定义和配置可执行的操作，而控制是具体的实现方式，通过一定的方式控制操作的允许和禁止 authentication 鉴权和授权是一一对应关系，解析授信媒介，确认其合法性、有效性 权限控制 权限是一个抽象的逻辑概念，定义和配置可执行的操作，而控制是具体的实现方式，通过一定的方式控制操作的允许和禁止 access/permission control 实现方式多样，根据具体情况来实现。 参考链接 http://www.hyhblog.cn/2018/04/25/user_login_auth_terms/ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/Java的基本程序设计结构.html":{"url":"java/base/Java的基本程序设计结构.html","title":"Java的基本程序设计结构","keywords":"","body":"Java的基本程序设计结构注释数据类型四种整型两种浮点型char字符类型boolean类型变量变量定义变量初始化常量运算符类型转换数值的二元操作强制类型转换运算符自增与自减运算符逻辑运算符位运算符括号与运算符级别字符串日期和时间的转换符switch选择语句大数值数组多维数组Java的基本程序设计结构 注释 Java中的注释不会出现在可执行程序中,提供了三种注释 // : 单行注释 /**/ : 多行注释 /* / : 文档注释,可以生成doc文档 数据类型 Java是一种强类型语言,意味着每一个变量声明时必须指定一个类型 Java中有8种基本数据类型 四种整型 类型 占用存储大小 取值范围 默认值 int 4个字节 -2147 483 648 - 2147 483 647 0 short 2个字节 -32768 - 32767 0 long 8个字节 -9 223 372 036 854 775 808 - 9 223 372 036 854 775 807 0L byte 1个字节 -128 - 127 0 可以使用其包装类的静态属性MIN_VALUE和MAX_VALUE获取其取值范围 long类型的数值需要在后面加l或者L表示long类型 int类型的值可以这样表示,加下划线更加易读,java编译器会去除这些下划线 int a = 2147_483_647; 在java中所有的数值类型所占的字节数量与平台无关 两种浮点型 类型 占用存储大小 取值范围 默认值 float 4字节 1.4E-45 - 3.4028235E38(有效位数6~7) 0.0f double 8字节 4.9E-324 - 1.7976931348623157E308(有效位数15位) 0.0 // 浮点型默认使用double类型 // 不会报错 double b = 1.0; // 后缀d可加可不加 double b = 1.0d; // 在编译器内会报错,需加f后缀 // float f = 1.0; // 正确写法 float f = 1.0f; 浮点型转换为整型 // 转换时会丢弃小数部分,如果想要四舍五入加上0.5后再转即可 // 12 int i = (int)12.7; // -12 int i2 = (int)-12.7; 想要精确的进行浮点型的计算使用 // 一律使用String类型的构造 BigDecimal bigDecimal = new BigDecimal(\"100.00\"); BigDecimal bigDecimal1 = new BigDecimal(\"100.000\"); // false,equals表示两个属于同一个数但换算值不同 System.out.println(bigDecimal.equals(bigDecimal1)); // 0 // 0:相等 -1:表示调用方小与参数 1:表示调用方大于参数 System.out.println(bigDecimal.compareTo(bigDecimal1)); char字符类型 表示一个字符,可以是汉字也可以是Unicode字符 类型 占用存储大小 取值范围 默认值 char 2字节 0-65535 \\u0000 可以与int类型相互转换 // 小转大,自动类型提升 char c = 'A'; int i = c; // 可以直接为char赋值int类型值 // 隐式转换 char c1 = 65; // 但如果声明了类型则需要强转 int i = 65; char c = (char)i; 比较常用的ASCII表 boolean类型 用来判定逻辑条件,不能与整型相互转换 类型 占用存储大小 取值范围 默认值 boolean 1个字节 true or false false 变量 变量定义 java中是对大小写敏感的,即name和Name是两个变量,不可以使用java中的关键字作为变量名 java中每一个变量都要一个类型,变量名在变量后,其定义规则如下(可参考阿里巴巴java开发手册) 不要使用$和_作为变量名的开始和结束,虽然两者都可以使用但是请避免使用 其他的命名规范 类名 : UpperCamelCase,即每一个单词首字母大写 变量名(方法,参数,成员和局部) : lowerCamelCase,即变量名的第一个小写其余单词首字母大写,可以在一行中定义多个变量,但为了代码可读性请不要这么做 包名 : 统一小写 常量 : 常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。 变量初始化 局部变量在使用之前必须先赋值(显式初始化),成员变量是有默认值的,在数据类型一章中有声明 // 表示将一个字符串赋值给一个String类型的变量name String name = \"taoqz\"; 常量 在变量类型前加final关键字,被final修饰的成员变量必须在定义时进行初始化(除了构造方法也可以使用构造代码块的方式),局部变量需在使用之前初始化(所有的局部变量在使用之前都必须初始化) 被final修饰的变量为常量,只能赋值一次,不可被修改 被final修饰的方法不可被重写 被final修饰的类不可被继承 运算符 常用的+ - * / 表示加减乘除,%表示取余操纵 java提供了java.lang.Math类,该类提供了许多数学运算的函数 类型转换 实心箭头表示无信息丢失的转换,虚线箭头表示会有精度损失 数值的二元操作 ·如果两个操作数中有一个是double类型,另一个操作数就会转为double类型。 ·否则，如果其中一个操作数是float类型，另一个操作数将会转换为float类型。·否则，如果其中一个操作数是long类型，另一个操作数将会转换为long类型。·否则，两个操作数都将被转换为int类型。 强制类型转换 小转大自动类型提升,大转小需要强制转换 隐式类型提升:一个小与变量类型的值在赋值时,如果该值在该值所属的范围内会自动类型提升,该值的类型可以是(byte/char/short/int),java中整型默认使用int类型,浮点默认使用double,所以在使用long和float时需要在后面指出l或f来进行区分 自动类型提升 char c = 'A'; int a = c; System.out.println(a); 隐式类型提升 char c = 65; System.out.println(c); 强制类型转换 int i = 300; byte b = (byte)i; // 44 大转小,会出现信息丢失问题 System.out.println(b); int i = 200; byte b = 100; b += i; // 44 结果一样,但是 += 会自动进行强制类型转换 System.out.println(b); 运算符 自增与自减运算符 以自增为例,自减道理相同 int i = 10,result; // 先运算再自增 10 result = i++; // 先自增再运算 11 // result = ++i; System.out.println(result); 逻辑运算符 以&符号做示例 int i = 10; // & 与 表示有一方为则为true,没有短路效果,后面的表达式还会执行 if (i != 0 & i++ > 10){ // && 与 表示同时为true时,才会执行，如果前面是false后面的表达式无论如何都不会执行有短路效果 // if (i == 0 && i++ > 10){ System.out.println(\"执行了...\"); } System.out.println(i); 总结: & : 条件都为true时才执行,没有短路效果,后面的表达式无论如何都会执行 && : 条件都为true时才执行,有false则不执行,有短路效果,后面的表达式无论如何都不会执行 | : 有true则true,没有短路效果,后面的表达式无论如何都会执行 || : 有true则true,有短路效果,后面的表达式无论如何都不会执行 ! : 取反,true则false,false则true 位运算符 待补充二进制相关知识 int i = 5; // int类型应为32位,为了方便使用8位 // 0000 0101 // 左移两位,低位补0 0001 0100 转为十进制为20 System.out.println(i > 2); 括号与运算符级别 如果是同一级别的运算符,从左向右依次计算(除了表中的右结合运算符除外) // true System.out.println(false && true || true); // true 下面两种表达式等价 System.out.println(true || true && false); System.out.println(true || (true && false)); 字符串 Java中用来表示字符串的类String是被final修饰的,是不可变字符串,也就是每一次操作都会生成一个新的字符串对象 字符串比较(==,equals()) ==常用来比较基本数据类型的值和对象的地址值是否相同 equals()方法继承自Object类,String类中常用来比较连个字符串的字符序列是否相同 常见的面试题 String st1 = \"abc\"; String st2 = \"abc\"; // true System.out.println(st1 == st2); // true System.out.println(st1.equals(st2)); 步骤:使用直接赋值的方式,会在常量池中创建一个字符串\"abc\"对象,当str2创建时发现常量池中已经有了会直接赋值给str2,所以两者指向的内存地址是相同的 String str = new String(\"abc\"); 这句话会创建两个对象,使用new创建的对象都是存储在堆内存中,而字串\"abc\"是一个常量应该在常量池中创建,根据API中的String(String str)构造的解释来看,堆中的对象其实是参数字符串的副本 String st1 = new String(\"abc\"); String st2 = \"abc\"; // false System.out.println(st1 == st2); // true System.out.println(st1.equals(st2)); 根据上面使用new创建字符串的情况来看,此处的str1 == str2自然是false,因为str1指向的是堆内存空间,str2指向的是常量池 String st1 = \"a\" + \"b\" + \"c\"; String st2 = \"abc\"; // true 常量池有常量优化机制 a b c三个字符串拼接后变成abc,在常量池中会在创建一个abc的字符串常量,而st2则直接引用 System.out.println(st1 == st2); // true System.out.println(st1.equals(st2)); 下面使用对象与+拼接时会使用StringBuffer或者StringBuilder对象来进行拼接,再使用toString方法将对象转换为String字符串对象,所以st3指向的是堆内存空间的字符串对象 String st1 = \"ab\"; String st2 = \"abc\"; String st3 = st1 + \"c\"; // false System.out.println(st2 == st3); // true System.out.println(st2.equals(st3)); 日期和时间的转换符 循环 java中有三种循环方式 int i = 10; while (i-- > 0){ // while (--i > 0){ System.out.println(i); } System.out.println(i+\" ===\"); int i = 10; do { i--; System.out.println(\"我总会执行一次\"); }while (i > 10); // for (int i = 10; i > 0; i--){ int i; for (i = 10; i > 0; i--){ System.out.println(i); } 可以使用break和continue语句控制循环的次数,并且支持在循环上定义标签,根据标签操作具体的循环体 break:终止本次循环 continue:跳过本次循环继续下一次循环 flag: for (int i = 0; i switch选择语句 switch语句中支持char,byte,short,int,从JDK7开始可以使用字符串控制switch语句 case语句具有穿透性,也就是说如果不加break语句,即使case匹配到后还会继续执行,直到遇到break语句或者执行到default执行完毕 String str = new Scanner(System.in).nextLine(); switch (str){ case \"min\": System.out.println(\"min\"); break; case \"mid\": System.out.println(\"mid\"); break; case \"max\": System.out.println(\"max\"); break; default: System.out.println(\"no match\"); } 大数值 // 两种大数值都在java.math包下,并不能使用+-这样的运算符,而是提供了对应的方法 // 计算结果可以直接打印(重写了toString方法) // BigInteger实现了任意精度的整数运算 BigInteger add = BigInteger.valueOf(1213).add(BigInteger.ONE); System.out.println(add.toString()); // 0.8999999999999999 System.out.println(2.0 - 1.1); // BigDecimal实现了任意精度的浮点数运算 // 建议使用String参数的构造 BigDecimal bigDecimal = new BigDecimal(\"2.0\"); BigDecimal add1 = bigDecimal.subtract(new BigDecimal(\"1.1\")); System.out.println(add1.toString()); System.out.println(add1); 数组 数组是一种可以存储任意类型的数据结构,可以存储基本数据类型也可以存储引用数据类型 可以通过数组的索引访问元素索引从0开始,但数组一旦创建其长度不可变,如果使用指定长度的方式创建数组,数组中每个元素都有其默认值,基本数据类型参照数据类型一篇,引用数据类型则为null 获取数组的长度使用其属性length,并不是方法 java还提供了类java.util.Arrays该类是一个专门操作数组的工具类,提供了大量的方法用于排序,比较,复制数组等操作 创建数组的三种方式 // 1.创建时指定数组长度 int[] arr1 = new int[10]; // 2.使用new的方式创建并且给出值 boolean array2[] = new boolean[]{false,true,false}; // 3.直接赋值 String array3[] = {\"1\",\"2\",\"3\"}; char[] array4 = {'A',65,'a'}; System.out.println(array4[1]); System.out.println(array3[0]); System.out.println(array2.length); 数组存储在堆内存中,数组同样支持将其赋值给另一个数组类型,赋值给arr2的是arr在内存的地址值,所以当操作arr2时,arr中的元素也会发生变化 int[] arr = {1,2,5}; int[] arr2 = arr; arr2[1] = 10; System.out.println(arr); System.out.println(arr2); System.out.println(arr == arr2); System.out.println(Arrays.toString(arr)); 多维数组 // 定义二维数组 // 表示创建一个二维数组,该二维数组中有两个数组每个数组的长度为2 int[][] arr1 = new int[2][2]; // 也可以直接赋值,并且每个数组的长度可以很灵活 int[][] arr = { {1,2,3}, {4,5,6}, {7,8} }; // 遍历二维数组 for (int i = 0; i Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/对象和类.html":{"url":"java/base/对象和类.html","title":"对象和类","keywords":"","body":"对象和类UML符号方法重写重载权限修饰符static静态构造器代码块包类的导入静态导入类设计技巧对象和类 面向对象程序设计(简称OOP),面向对象的程序是由对象组成的，每个对象包含对用户公开的特定功能部分和隐藏的实现部分。 类(class)是构造对象的模板或蓝图。我们可以将类想象成制作小甜饼的切割机，将对象想象为小甜饼。由类构造(construct)对象的过程称为创建类的实例(instance) ​ ---摘至 Java核心技术第10版 封装:将类内部的变量定义为私有,提供公开的访问接口(setter,getter)和功能(方法) UML符号 方法 方法定义在类的成员位置 一个简单的没有返回值,没有参数的方法,方法必须有返回值没有时要写void public void fun(){} 有参数和返回值的方法 public String calc(int a,int b){ return String.valueOf(a+b); } 静态方法 public static void print(String message){ System.out.println(message); } 重写 方法重写是在有类的继承或者对接口的实现的前提,对父类或者父类拥有的可被访问的方法的内部功能进行重新实现,当然也可以选择增强,在方法的内部使用super(代表父类)关键字调用原方法 重写的注意事项: 子类如果和父类在同一个包中,子类可以重写父类的所有非使用private和final的方法(public,default(不写权限修饰符),protected) 子类如果和父类不在同一个包中,子类可以重写父类中使用public和protected修饰的方法 子类的返回值必须大于等于父类的返回值,子类的参数必须小于等于父类的参数 方法的名称和参数必须和父类的相同 重写的方法的访问权限不能小于被重写方法的访问权限 被final修饰的方法不能被重写 构造函数不能重写 被static修饰的方法不能被重写,但可以被重新声明(也就是不能加@Override关键字) 如果被重写的方法没有抛出异常,那么重写方法时无论如何都不能抛出异常,如果被重写的方法抛出了异常,那么重写的方法可以选择不抛出或者抛出的异常类型小与等于被重写方法的异常 // 涉及继承相关的知识,到继承一章补充 // 该关键字可加可不加,加了后如果和父类的方法不同便会提示报错 @Override public void print(){ // 使用super关键字调用父类的成员变量和方法(可以在任意位置调用) System.out.println(super.name); super.print(); System.out.println(\"son\"); } 重载 重载指在同一个类或者其派生类中,有多个同名的方法,但这些方法参数类型和参数个数,顺序各不相同,跟返回值无关(可以相同也可以不相同),与访问权限修饰符无关,最常见的就是构造函数重载,重载方法可以抛出不同类型的异常 例如 public void fun(){ } public String fun(String message){ return message; } public void fun(String a,int b){ } public BigDecimal fun(int a, String b){ return BigDecimal.valueOf(a).add(new BigDecimal(b)); } 权限修饰符 java中4中权限修饰符分别为public、protect、默认(不写)、private 访问权限 当前类 同包 不同包子类 其他包 public √ √ √ √ protected √ √ √ 默认(不写) √ √ private √ static静态 被static修饰的变量属于类变量,类共享 被static修饰的方法属于静态方法(类方法),可以直接使用类名调用 静态只可以访问静态成员,不可以访问非静态成员 构造器 构造器和类同名 每个类可以有一个以上的构造器(默认是有一个空参构造的),重载 构造器可以有0个,1个或多个参数 构造器没有返回值 构造器总是伴随着new操作一起调用 请不要在构造器中创建和类的成员变量同名的局部变量 public class Emp { private String name; public Emp() { // 编译错误 String name = name; this.name = name; } } 在一个构造器中可以调用另一个构造器,使用this关键字,调用构造器的代码必须在调用处构造器的第一行(因为怕构造器后面的代码对变量有操作,所以先调用构造进行初始化,因为所有非无参构造的第一行默认调用无参构造,变量的初始化就是由无参构造做的) public class Emp { private String name; public Emp() { this(\"sdf\"); this.name = name; } public Emp(String name) { this.name = name; } } 代码块 不仅仅可以在构造函数和创建对象时为变量赋值,还可以在类的位置创建代码块为变量赋值 构造代码块 : 定义在类的成员位置,多个构造代码块根据定义的位置依次执行,每次创建对象时都会调用 静态代码块 : 定义在类的成员位置,使用static关键字修饰,也因其特性,只在类第一次被使用时调用 public class Emp { static { // 编译错误 静态成员不能访问非静态成员 // name = \"taoqz\"; age = 18; System.out.println(\"静态代码块\"); } { name = \"taoqz\"; System.out.println(\"构造代码块\"); } { name = \"zzz\"; System.out.println(\"构造代码块\"); } private String name; public static Integer age; public Emp() { System.out.println(\"构造方法\"); } public Emp(String name) { this.name = name; } public String getName() { return name; } public void setName(String name) { this.name = name; } } Emp emp = new Emp(); System.out.println(emp.getName()); Emp emp1 = new Emp(); System.out.println(emp1.getName()); System.out.println(Emp.age); // 虽然可以使用对象调用静态成员,但不推荐这么做 System.out.println(emp.age); 静态代码块 构造代码块 构造代码块 构造方法 zzz 构造代码块 构造代码块 构造方法 zzz 18 18 包 Java允许使用包将类组织和分开管理,命名规则为全部小写,推荐使用公司域名的反写 类的导入 一个类可以使用所属包中的所有类，以及其他包中的公有类(public class) 语法为 // import关键字 全路径类名(包名+类名) import lombok.Data; 但如果想引入一个包中的多个类就会显得很臃肿和麻烦 // 可以使用*号表示导入该包下的所有类 import java.time.*; 但如果引入的多个包中拥有同名的类,那么需要手动处理,使用全类名创建对象 静态导入 有些工具类提供了很多的静态方法,这些方法都可以使用类直接调用,但如果大量使用该类中的方法,代码也会显得很臃肿,静态导入则解决了该问题 // Math.abs(-1); // Math.sqrt(2); // 静态导入后,代码简洁很多 System.out.println(abs(-1)); System.out.println(sqrt(2)); 类设计技巧 一定要保证数据私有,不要破坏其封装性 要对数据进行初始化,毕竟有着最头疼的问题NullPointerException 避免类的职责过多,可以选择将其拆分 优先使用不可变类,保证在多线程环境下数据安全问题 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/泛型.html":{"url":"java/base/泛型.html","title":"泛型","keywords":"","body":"泛型定义泛型类方法类型变量的限定注意事项泛型擦除不能抛出或捕获泛型类的实例不同泛型参数的泛型类通配符泛型与反射泛型 ​ 泛型是Java SE5.0中新增内容,泛型很好的解决了杂乱的使用Object类型然后在强制类型转换的操作,提高代码的可读性(比如如果没有泛型,使用集合类时内部只能维护一个Object类数组,在获取值时还需强制转换为指定的类型),指定泛型后,可以在编译时期对类型进行检查(编译出错的类无法运行),避免使用错误类型的对象 // Java7 及以后省略构造后的泛型声明 ArrayList objects = new ArrayList<>(); 定义泛型 ​ 类型变量使用大写形式(小写也可用不推荐)，且比较短，这是很常见的。在Java库中，使用变量E表示集合的元素类型，K和V分别表示表的关键字与值的类型。T（需要时还可以用临近的字母U和S）表示“任意类型” 类 public class Person { private T name; private U age; public T getName() { return name; } public void setName(T name) { this.name = name; } public U getAge() { return age; } public void setAge(U age) { this.age = age; } } Person per = new Person<>(); per.setName(\"taoqz\"); per.setAge(10); String name = per.getName(); System.out.println(name); ​ 一个类上可以声明多个多个泛型变量多个泛型变量使用逗号分隔,也可以限定泛型变量的类型,使用extends,多个限定使用&分隔,要注意类要在接口之前,并且限定类型中只能写一个类,因为只有java中只有单继承...,如果没有限定默认上限为Object interface Demo1{ } class Demo2 { } interface Demo3 { } // 编译出错,类的声明要在接口前 //class Demo { class Demo { } 方法 方法的泛型变量需要在方法的返回值之前定义使用<>包围,声明后的泛型变量可以在参数或返回值使用 // 没有返回值的写法 // public static void getMiddle(T... a) { public static T getMiddle(T... a) { return a[a.length / 2]; } // String middle = MyTest.getMiddle(arr); // 可省略 String middle = getMiddle(arr); System.out.println(middle); 类型变量的限定 有时需要对类,方法中的类型变量加以约束 // 泛型类需要继承或实现指定的类和接口,多个类或者接口时使用 & 分隔 public static T max(T[] a) { // public static T max(T[] a) { // 使用该方法需要实现comparable接口 Arrays.sort(a); return a[a.length-1]; } @NoArgsConstructor @AllArgsConstructor public class Person implements Comparable{ // 不能定义泛型静态域 // private static T name; private T name; private U age; public T getName() { return name; } public void setName(T name) { this.name = name; } public U getAge() { return age; } public void setAge(U age) { this.age = age; } @Override public int compareTo(Object o) { return (int)((Person)o).getAge(); } @Override public String toString() { return \"Person{\" + \"name=\" + name + \", age=\" + age + '}'; } } @Test public void demo2() { Person p1 = new Person<>(\"zs\", 20); Person p2 = new Person<>(\"lisi\", 18); // 不能创建参数化类型的数组 // Person pers = new Person<>[]; // 编译错误 Person[] people = {p1, p2}; Person max = max(people); System.out.println(max); } 注意事项 泛型擦除 泛型只在编译时期有效,在运行时期会被擦除(替换为Object或者有多个类型时使用第一个类型) // 方法冲突,泛型擦除后,和Object中的equals无异 public boolean equals(T value){ return value.equals(\"123\"); } ArrayList strings = new ArrayList<>(); strings.add(\"name\"); // strings.add(123); // 编译错误 // 使用反射后可以添加任意类型 Method add = ArrayList.class.getMethod(\"add\",Object.class); Object invoke = add.invoke(strings, new Person(\"zs\",10)); System.out.println(invoke); System.out.println(strings); 不能抛出或捕获泛型类的实例 // 不能继承异常相关类 //public class Emp extends Exception { //public class Emp extends Throwable { public class Emp { public void fun(T t) { try { } catch (T e) { // 编译出错 } } public void fun2(T t) throws Throwable { try { // 可以编译通过,合法 } catch (Throwable e) { // 编译出错 throw t; } } } 不同泛型参数的泛型类 Person p1 = new Person<>(\"zs\",19); // Person p2 = p1; // 不同的泛型参数之前的类 没有关系 Person pp = p1; Person p2 = pp; // 可以正常运行 System.out.println(p2); 通配符 有时候我们需要限定泛型的类型在一个范围内,通配符可以解决这个问题 通配符无法在类上声明,主要在方法的参数或返回值上使用 public static void main(String[] args) { // Pair empPair = new Pair<>(); // Emp emp = new Emp(); // empPair.t = emp; // emp.name = \"taoqz\"; // 这种情况是不能当参数的,因为Manager虽然是Emp的子类,但加泛型后便是两种没有任何关系的类型 Pair empPair = new Pair<>(); fun(empPair); } public static void fun(Pair empPair){ System.out.println(empPair.t.name); } public static void main(String[] args) { // Pair empPair = new Pair<>(); // Manager manager = new Manager(); // 也可写成多态的方式 Pair empPair = new Pair<>(); Emp manager = new Manager(); manager.name = \"zz\"; empPair.t = manager; fun(empPair); } // 此时可以借助通配符来限定类型 表示传入的类型必须是继承自Emp或者是Emp类 也就是限定了类型的最大范围 public static void fun(Pair empPair){ System.out.println(empPair.t.name); } // 表示必须是Emp类型 或者是Emp的父类,限定了类型的最小范围 public static void fun2(Pair empPair){ System.out.println(empPair.t); } 泛型与反射 待做.... Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/集合.html":{"url":"java/base/集合.html","title":"集合","keywords":"","body":"集合迭代器List集合ArrayListVectorLinkedListSet集合Java集合继承图集合 ​ Java是一门面向对象的语言,自然少不了要处理对象,如果需要处理多个对象不可避免的需要使用一个容器(集合)来装载(其实对象的本质也是装在数据的容器)。 ​ 之前学过一种可以存储基本数据类型和对象的容器为数组,但是数组有个缺点是,一旦创建必须指定长度或者指定数组存储的内容其长度是不可变的,所以变衍生出另一种可以存储对象的容器便是集合,其长度可变,集合也提供了丰富的API方便我们操作数据,但集合也有个缺点,集合只能存储引用数据类型,但是每个基本数据类型都有其对应包装类,并且有自动拆装箱机制后影响不大。 ​ 同时由于我们对数据存储的方式或元素有要求,又分为多种不同数据结构的集合,单其本质还是存储数据,Java中将集合的共性抽取出来提供了一个接口Collection,其中常用的实现类 ​ RandomAccess接口 代表可以随机访问:通过索引顺序访问 接口中定义了一些基础的方法 迭代器 ​ Collection接口的父接口是Iterable,其中的iterator()方法会返回一个迭代器Iterator,迭代器中有三个常用的方法 // Iterator也是一个接口,不同的集合都有不同的实现 public interface Iterator {} 其中forEachRemaining()方法是java8新增,可以使用lambda表达式的方式调用方法,其内部实现还是依靠另外两个方法 hasNext():是否有下一个元素,如果有返回true否则false next():获取下一个元素并向后移动一个位置(如果没有元素会抛出NoSuchElementException异常) default void forEachRemaining(Consumer action) { Objects.requireNonNull(action); while (hasNext()) action.accept(next()); } 使用迭代器时需要注意的是不可以在迭代期间对集合做改变,以ArrayList内部实现的迭代器为例 ArrayList integers = new ArrayList<>(); // HashSet integers = new HashSet<>(); integers.add(1); integers.add(2); integers.add(3); Iterator iterator = integers.iterator(); // 使用foreach方式同样会出现以下错误,因为foreach本质上也是迭代器 while (iterator.hasNext()){ System.out.println(iterator.next()); if (iterator.next() == 2){ integers.add(4); } } // 会报并发修改异常 1 Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at xyz.taoqz.chapter6.demo.MyTest.main(MyTest.java:19) Java中认为集合在迭代时不应该对其做修改,可以看看ArrayList内部的迭代器的实现 private class Itr implements Iterator { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() { return cursor != size; } @SuppressWarnings(\"unchecked\") public E next() { checkForComodification(); int i = cursor; if (i >= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet 其中modCount是集合内部维护的一个变量,用来记录集合发生变化的次数 protected transient int modCount = 0; ​ 在迭代器中将其赋值给了迭代器中的expectedModCount,在每一次调用next()时都会检查该变量是否和集合内部的变量一致,如果不一致抛出ConcurrentModificationException()异常,但其内部又实现了remove方法,可以使用迭代器的方法对集合进行删除操作,其中调用了集合内部的remove() ArrayList.this.remove(lastRet); 并且对维护的变量进行了重新赋值,所以并不会抛出并发修改异常 expectedModCount = modCount; 如果想添加元素可以使用普通for循环,或者在使用迭代器时添加后即刻停止迭代 ArrayList integers = new ArrayList<>(); integers.add(1); integers.add(1); integers.add(3); // 需要注意的是,每次修改集合中的元素,在for循环中integers.size()便会重新获取 // 而变量i则会根据规则走,所以需要注意在修改或添加时避免漏掉数据 for (int i = 0; i iterator = integers.iterator(); while (iterator.hasNext()){ if (iterator.next().equals(2)){ integers.add(4); break; } } ​ 在ArrayList内部有一个实现了Iterator接口的内部类,除了以上一种还有一个就是ListIterator接口的实现类,同时又继承了Iterator接口的实现类,有更加丰富的功能(LinkedList和Vector也有) ArrayList integers = new ArrayList<>(); integers.add(1); integers.add(2); integers.add(3); // ListIterator iterator = integers.listIterator(); // 可以指定索引 正向:从哪个索引开始迭代 反向: 从倒数第几个开始遍历 // 他们会共用同一份cursor变量 ListIterator iterator = integers.listIterator(1); while (iterator.hasNext()){ // 还可以获取每次的索引 // System.out.println(iterator.next()+\" \"+iterator.nextIndex()); if (iterator.next().equals(2)){ // 修改指定索引的值 // iterator.set(4); // 添加到指定索引后 iterator.add(5); } } System.out.println(integers); System.out.println(\"=============\"); while (iterator.hasPrevious()){ System.out.println(iterator.previous()+\" \"+iterator.previousIndex()); } 功能类似的Enumeration接口 Vector是在Java早期的集合和ArrayList作用基本一致,但是Vector的方法都是线程同步的,而ArrayList不是 Vector vector = new Vector<>(); vector.add(3); vector.add(1); vector.add(2); Enumeration elements = vector.elements(); while (elements.hasMoreElements()){ System.out.println(elements.nextElement()); } // 源码 public Enumeration elements() { return new Enumeration() { int count = 0; public boolean hasMoreElements() { return count List集合 ​ Collection中主要的集合有两种List和Set,List集合的特点是有序(存储和去除的顺序一致)可重复 ​ 前面做例子主用的都是ArrayList,底层是数组结构,线程不安全 ​ LinkedList : 底层是链表结构,线程不安全 ​ Vector : 底层是数组结构,线程安全 ArrayList 源码分析 // 默认的初始容量 (再扩容时才会使用到) private static final int DEFAULT_CAPACITY = 10; // 当集合的长度被指定为0时返回的空数组 private static final Object[] EMPTY_ELEMENTDATA = {}; // 默认空参构造时返回的空数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; // 实际存储元素的数组,ArrayList的底层就是数组 transient Object[] elementData; // 实际储存元素的个数 private int size; // 指定容量大小的构造 public ArrayList(int initialCapacity) { // 构建指定大小的数组 if (initialCapacity > 0) { this.elementData = new Object[initialCapacity]; // 如果指定为0 使用EMPTY_ELEMENTDATA空数组 } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } // 空参构造使用DEFAULTCAPACITY_EMPTY_ELEMENTDATA作为存储元素的数组使用 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } // 在尾部添加元素 public boolean add(E e) { // 扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // size自增同时为数组赋值 elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { // 如果使用的空参构造,比较默认的容量和参数作比较取较大值 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } // modCount自增,修改次数++ private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code // 如果比原来的数组长度大 if (minCapacity - elementData.length > 0) // 调用实际的扩容方法 grow(minCapacity); } // 设置数组最大容量 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; private void grow(int minCapacity) { // overflow-conscious code // 获取数组现在的长度 int oldCapacity = elementData.length; // 扩大到其自身的1.5倍 int newCapacity = oldCapacity + (oldCapacity >> 1); // 如果扩容后小于参数值,使用参数值 if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } // System类方法,发方法为本地方法,有c/c++编写,ArrayList的底层全靠其来做动态增长 // src: 原数组 srcPos: 原数组开始索引 dest: 复制至新的数组 destPos: 新数组的索引起始 length: 复制元素个数 public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } // 在指定索引位置添加元素 public void add(int index, E element) { rangeCheckForAdd(index); // 扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // 复制数组,并且将指定index位置空出来 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 为空出来的指定索引赋值 elementData[index] = element; // 集合长度加一 size++; } // 检查是否索引越界 private void rangeCheckForAdd(int index) { if (index > size || index = size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } // 返回实际元素,如果get参数为负数,由此处抛出索引越界异常 E elementData(int index) { return (E) elementData[index]; } // 修改指定索引上的值,并返回旧值,该方法不会对modCount变量操作 public E set(int index, E element) { rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue; } // 删除指定索引上的值 public E remove(int index) { rangeCheck(index); // 删除会修改modCount的值 modCount++; E oldValue = elementData(index); // 拷贝数组 int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 将--size位置上的元素赋值为null同时将size-1 让GC进行垃圾回收 elementData[--size] = null; // clear to let GC do its work return oldValue; } // elementData[--size] = null; 操作是因为System.arraycopy()方法的特性 LinkedList integers = new LinkedList<>(); int[]arr = {1,2,3}; System.arraycopy(arr,1,arr,0,2); System.out.println(Arrays.toString(arr)); // console [2, 3, 3] 总结 ​ ArrayList基于在底层复制数组实现动态扩容,add和remove方法都会对modCount变量做修改所以在使用迭代器时避免使用这两个方法,add的时候会自动扩容但是在remove时不会自动减少容量,如果需要减少容量可以手动调用trimToSize()方法 // 如果实际存储元素数量小于数组长度 // 将数组容量缩小至数组实际存储元素的个数 public void trimToSize() { modCount++; if (size Vector Vector是jdk1.2的类,其结构和后来的ArrayList一样,但Vector是线程安全的 // 底层也是数组 protected Object[] elementData; // 扩容追加的个数 protected int capacityIncrement; // 带有扩容指数的构造 public Vector(int initialCapacity, int capacityIncrement) { super(); if (initialCapacity 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); } LinkedList 上面说的ArrayList和Vector底层都是数组结构,而LinkedList底层是链表结构,双向链表,内部类实现为Node private static class Node { // 本节点存储的数据 E item; // 分别记录下一个和上一个节点的位置 Node next; Node prev; Node(Node prev, E element, Node next) { this.item = element; this.next = next; this.prev = prev; } } LinkedList集合继承图 LinkedList还实现了Queue接口,也提供了很多队列的操作方式 源码解析 transient int size = 0; // 记录链表的头节点和尾节点,通过这两个节点可以任意操作链表(只有头节点便可满足大部分需求(单向链表)) transient Node first; transient Node last; // 分别提供了空参构造和有一个实现了Collection接口的实现类参数的构造 public LinkedList(Collection c) { this(); addAll(c); } public boolean add(E e) { linkLast(e); return true; } // 具体的添加方法,会默认添加到尾部,如果添加时尾部是空的说明是空链表 // 将头和尾指向同一个节点 void linkLast(E e) { final Node l = last; final Node newNode = new Node<>(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } // 检查添加的索引是否超出链表长度,或者是否是负数 private boolean isPositionIndex(int index) { return index >= 0 && index node(int index) { // assert isElementIndex(index); // 如果索引小与长度的1/2,从头开始遍历查找,相反从尾部开始遍历 if (index > 1)) { Node x = first; for (int i = 0; i x = last; for (int i = size - 1; i > index; i--) x = x.prev; return x; } } // addAll(c),最终会调用 public boolean addAll(int index, Collection c) { // 检查索引 checkPositionIndex(index); // 转为数组,拿到数组长度,可以循环添加到链表中 Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; // 临时变量 // pred:上一个节点 succ:下一个节点 Node pred, succ; // 如果添加的位置在尾部,将上一个节点指向last if (index == size) { succ = null; pred = last; // 如果指定了索引,通过索引位置查找链表中的元素 // 找到后确定添加元素所需的前后(上下)节点 } else { succ = node(index); pred = succ.prev; } // 迭代数组,赋值 for (Object o : a) { @SuppressWarnings(\"unchecked\") E e = (E) o; Node newNode = new Node<>(pred, e, null); // 如果上一个元素等于null,说明last为null,也就是此时的链表是空的,因为add()方法默认添加到尾部,将头部指向用循环创建的第一个节点,否则向后添加 if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } // 如果succ为null,说明没有下一个节点,将尾部节点指向循环后的最后一个节点 // 否则将添加完成后的一段链表中的最后一个节点的下一个节点指向通过node(index)找到的元素节点,将找到的元素节点的上一个节点指向添加后的最后一个节点 if (succ == null) { last = pred; } else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } // get最终调用的node(index),方法,方法返回指定位置的节点匀元素 public E get(int index) { checkElementIndex(index); return node(index).item; } // 本质个get相同只是切换了指定位置节点保存的元素 public E set(int index, E element) { checkElementIndex(index); Node x = node(index); E oldVal = x.item; x.item = element; return oldVal; } // 删除指定元素,还有一个删除指定索引位置的元素的方法,本质一样都会调用unlink方法 // 元素为null和元素不为null时分别进行遍历删除,不为null时使用equals判断 public boolean remove(Object o) { if (o == null) { for (Node x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node x = first; x != null; x = x.next) { if (o.equals(x.item)) { unlink(x); return true; } } } return false; } // 删除节点 E unlink(Node x) { // assert x != null; // 记录该元素的前后节点 final E element = x.item; final Node next = x.next; final Node prev = x.prev; // 如果前一个节点为null,说明此节点为头节点,将头节点更改为该节点的下一个节点 if (prev == null) { first = next; } else { // 将前一个节点的下一个节点指向本节点的下一个节点 prev.next = next; // 将本节点的上一个节点的引用消除 x.prev = null; } // 如果next为null说明此节点为尾节点 if (next == null) { // 将链表的尾节点更换为该节点的上一个节点 last = prev; } else { // 将此节点的下一个节点的上一个节点引用指向本节点的上一个节点 next.prev = prev; // 将本节点的下一个节点引用消除 x.next = null; } x.item = null; size--; modCount++; return element; } ArrayList和LinkedList的使用场景 ArrayList由于采用数组作为数据结构,在进行增删时通过使用数组复制来完成动态的添加与删除,在大量的数据下处理较慢,在增删较多时推荐使用LinkedList Set集合 Set集合的特点的是元素不可重复 常用子类 HashSet:底层是哈希表结构(是一个元素为链表的数组) TreeSet:底层是红黑树(是一个自平衡的二叉树),保证元素的排序方式 LinkedHashSet:底层由哈希表和链表组成 Java集合继承图 引自Java核心技术卷I Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/继承.html":{"url":"java/base/继承.html","title":"继承","keywords":"","body":"继承构造访问域类型转换抽象类Object类拆装箱Integer类可变参枚举反射继承的注意事项继承和抽象的区别继承 ​ 通过继承可以基于现有的类创建一个新的类,继承父类所有公开的成员还可添加新的成员变量和方法进行扩展 ​ is a是继承的一个明显的特征,比如有一个水果类和一个苹果类,而苹果也是一种水果 ​ 继承使用extends关键字,java中只有单继承,但支持多重继承(一个类只可以继承一个类,但其父类还可以继承其他的类,达到多重继承的效果),java中每一个基类都默认继承Object,即Object类是所有类的超类 简单的写个继承的例子 public class Father {} public class Son extends Father{} 构造 ​ 在创建每个类时,默认会有一个空参构造,但如果自定义了一个任意的有参数的构造,那么这个空参构造便不复存在,构造函数最大的意义就是为了在创建对象时为成员变量初始化,基于这条规则可能在继承时遇到的问题,同时构造器不能被继承(只能在类内部通过super调用,且必须在子类的构造的第一行中,如果一个类想要被继承,必须有一个可被子类访问的构造函数,该构造函数的权限可以为public、protected、和default(不声明)) ​ 下面的例子是父类有一个String类型的变量作为参数的构造方法,而类Son继承了这个类,该类会默认调用父类的空参构造,而父类此时已经没有了空参构造,所以Son类无论写不写这个空参构造都会编译失败 public class Father { private String name; public Father(String name) { this.name = name; } } public class Son extends Father{ // 编译出错 public Son() { } } 解决方法 子类同样提供一个有参构造,并手动调用父类的构造,使用super关键字可以调用父类中的构造和成员 public class Son extends Father{ public Son(String name) { super(name); } } 访问域 同包:子类可以访问父类中任意非私有的成员变量和成员方法 不同包:子类可以访问父类中使用public或protected修饰的成员变量和成员方法 多态 父类引用指向子类对象是最常见的说法 多态中最容易模糊的就是变量和方法的访问问题 public class Father { public Integer age; LocalDateTime localDateTime; protected String sex; private String name; static Integer zz = 10; public Father() { name = \"father\"; } public static void staticFun(){ System.out.println(\"father staticFun\"); } public void print(String message){ System.out.println(\"father:\"+message); } } public class Son extends Father { Integer age = 10; Double height = 115.5; // static Integer zz = 20; @Override public void print(String message) { System.out.println(\"son:\"+message); } public static void staticFun(){ System.out.println(\"son staticFun\"); } public void say(){ System.out.println(\"son: say()\"); } } public class MyTest { public static void main(String[] args) { // 父类引用指向子类对象 Father fa = new Son(); // 成员变量 编译时看左边(在编辑器内根本访问不到子类特有的成员变量) 运行时看右边(即使子类中有和父类同名的变量,运行时还是会走父类的变量) System.out.println(fa.age); // 成员方法 编译看左边(访问不到子类中特有的方法) // fa.say(); // 成员方法的运行 首先多态的情况调用方法时会首先去父类中查看有没有该方法,如果没有直接编译出错 // 如果父类有,在调用时会看子类是否有该方法,如果没有调用父类方法,如果有调用子类方法,这也是多态的最大特点之一 // 成员方法 编译看左边 运行看右边 // son:message fa.print(\"message\"); // 静态函数 编译和运行都看左边(父类),对于静态成员不推荐使用对象访问 fa.staticFun(); // System.out.println(fa.zz); } } 总结: 1.成员变量 : 编译和运行都看左边 2.成员方法 : 编译看左边,如果父类有编译通过,运行时如果子类没有重写该方法,则调用父类的方法,如果子类重写了该方法,则调用子类重写后的方法 3.静态成员 : 首先静态成员(变量和方法)都不推荐使用对象方法,编译和运行都看左边 类型转换 Father father = new Son(); // class xyz.taoqz.chapter3.Son 类型依然是子类类型(这个是没有想到的~~) System.out.println(father.getClass()); // 编译错误 // father.say(); // 向下转型,可以获得更多方法 Son s = (Son) father; s.say(); // 如果不知道某个变量属于哪个类型可以使用instanceof关键字进行判断后再进行转换 if(变量 instanceof 类型){} 抽象类 ​ 抽象类可以将同一类事物的共同属性抽取出来,作为更加抽象的基类供子类进行扩展,抽象类不能创建对象这也让抽象类的抽象得到了体现 ​ 抽象类使用abstract修饰,命名规则:抽象类命名使用Abstract或Base开头 //public final abstract class BasePerson { public abstract class BasePerson { public String name; public Integer age; public void print(){} public abstract void fun(); } public class Student extends BasePerson { @Override public void fun() { System.out.println(\"必须重写\"); } } ​ abstract关键字不能和final关键字共用,因为abstract本身的意义就是为了其他子类具象化,如果加了final关键字则不能进行继承,那设立抽象类也就没有了意义,抽象方法同理 ​ 抽象类不能实例化对象,也就是不能new,抽象类中可以有抽象方法和非抽象方法,但是如果有抽象方法,子类在继承时必须实现该方法(Java8中提供的default方法只能在接口中使用) Object类 ​ Object类是Java中所有类的始祖,Java中没有明确使用extends关键字继承其他父类默认继承自Object,但是不需要使用extends声明,并且如果声明继承了其他父类,那么该父类如果没有继承其他类也会默认继承Object,所以说Object类中的所有方法都可以被任意类继承使用,可以使用Object类型的变量引用任何对象的类 拆装箱 ArrayList array = new ArrayList<>(); ​ 上面的代码将不能通过编译期,因为集合只能存储引用数据类型,但有时确实有刚需,所有的基本类型都有与之对应的一个类,Java又在JDK5之后引入了自动拆装箱 基本数据类型 对应的包装类 byte Byte short Short int Integer long Long boolean Boolean char Character float Float double Double 其中表示数值的包装类都继承子java.lang.Number类,对象包装器是不可变的都由final修饰 ArrayList array = new ArrayList<>(); array.add(18); 上面代码将一个基本数据类型赋值给一个泛型为Integer的集合中,做了自动装箱,实际 array.add(Integer.valueOf(18)); 不仅提供了自动装箱,还有自动拆箱 // 将Integer的数值赋值给一个基本数据类型int int num = array.get(0); // 实际为 num = array.get(0).intValue(); // 同样支持算术运算符合赋值运算符中 Integer count = 10; int result = count + 8; 注意事项 Integer类 使用包装类比较数值是否相等时,请使用其重写后的equals()方法,以Integer举例 Integer num1 = 10; // // 等同于 // num1 = Integer.valueOf(10); // 创建了一个新的Integer对象 Integer num2 = new Integer(10); // == 在比较基本数据类型时比较的是其值是否相等 // 比较引用数据类型时比较的是其地址值是否相同 System.out.println(num1 == num2); // false // equals比较两个引用数据类型的值是否相同 System.out.println(num1.equals(num2)); // true Integer num1 = 127; Integer num2 = 127; System.out.println(num1 == num2); // true System.out.println(num1.equals(num2)); // true Integer num1 = 128; Integer num2 = 128; System.out.println(num1 == num2); // false System.out.println(num1.equals(num2)); // true 可以看到很奇怪的两个127的Integer对象使用==号可以得出相等而128则不行,查看Integer类源码,其中重要的几个方法 // 被包装的那个基本数据类型的数值 private final int value; // 构造方法,总是创建一个新的Integer对象 public Integer(int value) { this.value = value; } // 调用该方法,返回内部包装的int值 public int intValue() { return value; } // 将其转为Integer类型,在调用intValue()返回数值,作比较 public boolean equals(Object obj) { if (obj instanceof Integer) { return value == ((Integer)obj).intValue(); } return false; } // Integer num = 10 // 等同于 num = Integer.valueOf(10); // 本质便是调用该方法,可以看到有个IntegerCache类,该类便可解决前面的疑问 public static Integer valueOf(int i) { if (i >= IntegerCache.low && i = 127; } private IntegerCache() {} } Integer类的缓存数值范围可以修改,其他类则不可以 可以在编辑器中添加参数 -XX:AutoBoxCacheMax=200 指定范围 也正因为这些对应的包装类都是引用类型,所以还需特别注意空指针 除了Integer类以外,其他的整型对应的包装类内部都有一个内部类,缓存了-127 -- 128的数 Character类也提供了缓存,对对应int值在 0-127 范围的字符做了缓存 // Character类源码 public static Character valueOf(char c) { if (c 由于Java中方法都是值传递,如果想使用方法改变一个变量的值,比如 org.omg.CORBA包下提供了暴露内部value的对应的\"包装类\" @Test public void demo9(){ int num = 10; change(num); change2(num); IntHolder intHolder = new IntHolder(num); change3(intHolder); System.out.println(intHolder.value); } public void change(int x){ x *= 10; } // Integer为 public void change2(Integer x){ x *= 10; } public void change3(IntHolder x){ x.value *= 10; } 可变参 在Java5之前每个方法都有固定数量的参数,在Java5后提供了可变参数的方法,其参数可以为0,1或者多个,可变参数必须放在参数列表的最后,这是因为如果之后还有同类型的形参,那该参数将永远拿不到实参(即使不同类型也必须将其放在参数列表的最后) 其本质和数组没有什么区别,在传参或者取值时都可以使用数组的方式进行操作 @Test public void demo(){ String[] strs = {\"Hello\",\"Java\",\"!!!\"}; print(1,strs); } public void print(int a,String... num){ System.out.println(num[1]); } 枚举 ​ 枚举:本身也是一个类,编译后的文件也是由.class结尾,声明时将对应的class的位置替换为关键字enum,所有自定义的枚举类都继承自类java.lang.Enum,在有需要常量的情况下可以考虑使用枚举类 java类不可直接继承Enum枚举类 定义一个枚举类 和普通的类差别不大,也可以拥有成员变量和成员方法 // 将对应的class替换为enum public enum Size { // 编译失败,枚举值必须在枚举类的第一行中定义 // private String number; // 枚举类中的常量 SMALL(\"S\"),MEDIUM(\"M\"),LARGE(\"L\"),EXTRA_LARGE(\"XL\"); // 成员变量 private String number; // 构造方法,默认是private也必须为private Size(String number){ // super关键字并不能在枚举类中使用 // super(\"z\",1); this.number = number; } public String getNumber() { return number; } } 常用的方法 首先看一下所有枚举类共同的父类Enum中的方法 // 静态方法 返回包含全部枚举值的数组 Size[] values = Size.values(); for (Size value : values) { // 直接打印时调用的就是toString方法 // System.out.println(value); System.out.println(value.toString()); // 和toString方法的输出一致,都是常量的String字符串名称,toString内部也是直接返回name System.out.println(value.name()); // SMALL MEDIUM LARGE EXTRA_LARGE // 返回常量在枚举类中定义的顺序,从0开始 System.out.println(value.ordinal()); // 0 1 2 3 // 自定义的方法,返回枚举值的构造中的变量 System.out.println(value.getNumber()); // S M L XL // System.out.println(value.compareTo()); // System.out.println(value.equals()); } // 静态方法,根据枚举类型和常量名称 返回一个枚举常量 Size smAll = Size.valueOf(Size.class, \"SMALL\"); System.out.println(smAll); 其中的两个比较方法可以去看Enum类中的具体实现compareTo()、equals() // 类Enum 是一个抽象类 // 看源码得知,获取枚举类中常量的信息的两个变量都是私有其不可变的,但提供了两个公开的不可变的方法来获取对应值 private final String name; public final String name() { return name; } private final int ordinal; public final int ordinal() { return ordinal; } // 唯一的构造,修饰符为protected(枚举类不能通过super访问,但类不提供构造不能被继承) protected Enum(String name, int ordinal) { this.name = name; this.ordinal = ordinal; } // Enum类重写了equals方法,所以比较枚举时可以不必使用该方法直接使用 == 比较即可 public final boolean equals(Object other) { return this==other; } // 不能被克隆 /** * Throws CloneNotSupportedException. This guarantees that enums * are never cloned, which is necessary to preserve their \"singleton\" * status. * * @return (never returns) */ protected final Object clone() throws CloneNotSupportedException { throw new CloneNotSupportedException(); } // 根据其定义顺序比较大小 public final int compareTo(E o) { Enum other = (Enum)o; Enum self = this; if (self.getClass() != other.getClass() && // optimization self.getDeclaringClass() != other.getDeclaringClass()) throw new ClassCastException(); return self.ordinal - other.ordinal; } 反射 ​ java.lang.reflect包下提供了丰富的关于反射的工具集,可以动态操纵Java代码结构,反射多用于工具类和框架中 ​ 在代码中反射的基础是先获取Java类的字节码对象及Class对象,对应的类便是Class类,获取一个类Class对象有三种方式 /** * 获取Class对象的三种方式 * * @throws ClassNotFoundException */ @Test public void fun1() throws ClassNotFoundException { // 使用Class类的静态方法 参数为 类的全路径名称 // 该方法会抛出一个 ClassNotFoundException异常 Class personClass1 = Class.forName(\"xyz.taoqz.chapter3.reflect.domain.Person\"); System.out.println(personClass1); // class xyz.taoqz.chapter3.reflect.domain.Person // 使用实例对象的getClass()方法,该方法继承自Object Person person = new Person(); Class personClass2 = person.getClass(); System.out.println(personClass2); // class xyz.taoqz.chapter3.reflect.domain.Person // 每个类都会隐含有一个静态的 class属性 // 通过该方式获取类的Class对象很方便 不用创建对象也不会抛出异常 Class personClass3 = Person.class; System.out.println(personClass3); // class xyz.taoqz.chapter3.reflect.domain.Person // 获取接口的Class对象 Class myInterfaceClass = MyInterface.class; System.out.println(myInterfaceClass); //interface xyz.taoqz.chapter3.reflect.domain.MyInterface // 生成Class对象时,会先判断内存中是否已经加载 // 获取的都是同一个Class对象,所以结果都为true System.out.println(personClass1 == personClass2); System.out.println(personClass2 == personClass3); } 反射不仅可以获取类的Class对象,还可以获取基本数据类型和数组的Class对象,只不过有些特殊 /** * 获取基本数据类型和数组的类对象 */ @Test public void demo() { Class integerClass = int.class; // 基本数据类型和其包装类进行比较 System.out.println(integerClass == Integer.class); // false System.out.println(Integer.TYPE == integerClass); // true // 基本数据类型 System.out.println(double.class); // double // 包装类数组类型 System.out.println(Double[].class.getName()); // [Ljava.lang.Double; System.out.println(Integer[].class.getName()); // Ljava.lang.Integer; // 基本数据类型数组的class对象的名称有点特殊 System.out.println(int[].class.getName()); // [I System.out.println(double[].class.getName()); // [D System.out.println(float[].class.getName()); // [F // 二维数组 // 只有同一类型并且维度相同的数组,才会共享同一份字节码文件 String[][] strings = new String[2][]; Class c1 = strings.getClass(); String[] strs = new String[3]; String[] strs2 = new String[3]; Class c2 = strs.getClass(); Class c3 = strs2.getClass(); System.out.println(c1 == c2); // false System.out.println(c2 == c3); // true } Integer.TYPE的JDK源码实现 /** * The {@code Class} instance representing the primitive type * {@code int}. * * @since JDK1.1 */ @SuppressWarnings(\"unchecked\") public static final Class TYPE = (Class) Class.getPrimitiveClass(\"int\"); /* * Return the Virtual Machine's Class object for the named * primitive type. */ static native Class getPrimitiveClass(String name); 除了Class表示类的对象外,还有其他类来表示类中的其他结构 Constructor : 构造方法 Parameter : 方法中的参数 Method : 成员方法 Field : 成员变量 (可以拿到包括使用静态和final修饰的成员变量) 由于参数没有修饰符的概念,所以排除它其他几种类都提供了XXX.getDeclaredXXXs()的方法,无视修饰符拿到类中所有对应的成员,如果需要运行,同样调用XXX.setAccessible(true)方法即可,而普通的获取方法只能获取使用public修饰的成员 多态时使用反射获取的是子类的Class对象,所以操作的结构也只是子类中的内容 /** * Class对象中常用的方法 */ @Test public void getConstructors() throws Exception { // 获取类的Class对象 Class perClass = Class.forName(\"xyz.taoqz.chapter3.reflect.domain.Person\"); // 创建实例 // 使用该方式创建实例时,该类中必须有一个默认的空参构造,因为该方法底层调用的也是类的空参构造,不然会报错 很重要!!! Object obj = perClass.newInstance(); System.out.println(obj instanceof Person); // true // 获取类中所有使用public修饰的构造方法 Constructor[] constructors = perClass.getConstructors(); for (Constructor constructor : constructors) { System.out.println(constructor.getName()); // 获取构造方法的参数 数组 Parameter[] parameters = constructor.getParameters(); // 获取每个参数的类型(如果是引用类型,打印其) 和名称(arg+参数在类中定义的顺序从0开始) for (Parameter parameter : parameters) { System.out.println(parameter.getType().getName()); // 默认调用toString()方法 会有 class 前缀 System.out.println(parameter.getType() + \"==\" + parameter.getName()); } } // 指定参数类型获取构造 Constructor constructor = perClass.getConstructor(String.class, boolean.class, Integer.class, int.class); // 通过构造方法创建对象 Object zs = constructor.newInstance(\"zs\", true, 10, 20); System.out.println(zs); // 获取指定参数的构造 无视修饰符 Constructor declaredConstructor = perClass.getDeclaredConstructor(String.class); // 获取类中所有的构造方法 无视修饰符 // Constructor[] declaredConstructors = perClass.getDeclaredConstructors(); // 如果要运行类中私有成员 需要使用该方法 declaredConstructor.setAccessible(true); Object lisi = declaredConstructor.newInstance(\"lisi\"); System.out.println(lisi); } @Test public void getFields() throws Exception { Class perClass = Class.forName(\"xyz.taoqz.chapter3.reflect.domain.Person\"); Object obj = perClass.newInstance(); // 获取类中使用public修饰的成员变量 Field[] fields = perClass.getFields(); System.out.println(fields.length); for (Field field : fields) { System.out.println(field.getName()); } // 获取指定的变量,并赋值到对象中 Field count = perClass.getField(\"count\"); count.set(obj,20); System.out.println(obj); // 获取所有非public修饰的成员变量 Field[] declaredFields = perClass.getDeclaredFields(); for (Field declaredField : declaredFields) { System.out.println(declaredField.getName()); } Field name = perClass.getDeclaredField(\"name\"); name.setAccessible(true); name.set(obj,\"taoqz\"); System.out.println(obj); // 获取对象中指定的字段值 Class personClass = Person.class; Person person = personClass.newInstance(); person.setCount(10); Field age = personClass.getDeclaredField(\"count\"); age.setAccessible(true); int anInt = age.getInt(person); System.out.println(anInt); } @Test public void getMethods() throws Exception { Class perClass = Class.forName(\"xyz.taoqz.chapter3.reflect.domain.Person\"); Object obj = perClass.newInstance(); // 获取本类中所有方法,也就是不会获取父类中的方法 Method[] declaredMethods = perClass.getDeclaredMethods(); for (Method declaredMethod : declaredMethods) { System.out.println(declaredMethod); // public boolean xyz.taoqz.chapter3.reflect.domain.Person.equals(java.lang.Object) System.out.println(declaredMethod.getName()); // equals System.out.println(declaredMethod.getModifiers()); // 获取方法的修饰符对应的数值 System.out.println(Modifier.toString(declaredMethod.getModifiers())); // 打印修饰符 System.out.println(declaredMethod.getReturnType().getName()); // 获取方法返回值类型 System.out.println(\"========================================\"); } // Constructor constructor = perClass.getConstructor(String.class, Integer.class, Integer.class); Constructor constructor = perClass.getConstructor(String.class, boolean.class, Integer.class, int.class); Object instance = constructor.newInstance(\"张三\", true, 10, 1); System.out.println(instance instanceof Person); System.out.println(instance); // 获取类中指定方法并执行 Method print = perClass.getMethod(\"print\", null); Object result = print.invoke(instance, null); System.out.println(result); System.out.println(instance); Method sMethod = perClass.getMethod(\"sMethod\", String.class); Object result2 = sMethod.invoke(instance, \"李四\"); System.out.println(result2); // Method print1 = perClass.getMethod(\"print\", String.class); Method print1 = perClass.getDeclaredMethod(\"print\", String.class); print1.setAccessible(true); Object zz = print1.invoke(instance, \"123\"); System.out.println(zz); } 继承的注意事项 将公共操作和域放在超类中 尽量不要使用protected,因为其并不能很好的保护封装性,因为子类也是可以无限派生的 继承和抽象的区别 比如动物都有吃的方法,而每种动物的吃法不同,如果使用普通的类进行继承,那么方法内写什么都不太合适,所以干脆不要方法体,任由子类发挥 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/接口、内部类.html":{"url":"java/base/接口、内部类.html","title":"接口、内部类","keywords":"","body":"接口、对象克隆、内部类接口接口与抽象类的区别注意事项对象克隆clone()浅克隆clone()深克隆序列化内部类成员内部类局部内部类匿名内部类接口、对象克隆、内部类 接口 继承时类与类之间的关系是is a,而接口则是like a,接口更加强调功能 定义一个接口 public interface MyInterface { // public static final String name = \"\"; // 默认使用 public static final 修饰 String name = \"\"; default void print(String mess){ System.out.println(\"print\"+mess); } // 方法默认使用public abstract修饰 // public abstract void say(); void say(); static void eat(){ System.out.println(\"eat\"); } } 接口与抽象类的区别 接口: ​ 接口使用interface定义,接口也是一种特殊的类,没有构造方法,不能创建对象,接口中不能定义静态(static method)方法和实例域,接口中只有常量并且默认使用public static final修饰,接口中的方法默认使用public修饰,在java8之前接口中只能有抽象方法,java8之后可以声明使用default修饰的带有默认方法体的方法 抽象类: ​ 抽象类使用class关键字定义,可以定义普通的成员变量,有构造方法,不能创建对象,有构造方法的原因是为了让其子类进行初始化,抽象类中可以有抽象方法也可以有普通方法 ​ 抽象类中可以定义静态方法和实例域 java中类与类之间只有单继承,可多重继承,类可以实现多个接口 extends关键字要在implements关键字前,实现多个接口使用逗号分隔。接口与接口之间可以多继承。所以多使用接口可以使类的扩展性更强,普通类必须重写抽象类和接口中的抽象方法 注意事项 如果类实现了多个接口,接口中重复的默认方法(方法名和参数列表都相同),实现类必须重写这个方法 public interface MyInterface { default void print(){ System.out.println(\"interface print\"); } } public interface MyInterface2 { default void print(){ System.out.println(\"MyInterface2 print\"); } } public class SubClass implements MyInterface,MyInterface2{ // 如果实现的接口中有相同默认方法(方法名,参数相同) // 子类必须重新实现方法,在方法体内可以任意调用接口中的默认方法 @Override public void print() { MyInterface.super.print(); MyInterface2.super.print(); } } 2. 如果实现类的超类中的方法由和实现的接口中的默认方法相同,将会永远调用类中的方法,也就是类优先规则,所以不要再接口中重新定义和Object类中相同的方法 public class SuperClass { public void print(){ System.out.println(\"class print\"); } } public interface MyInterface { default void print(){ System.out.println(\"interface print\"); } } public class SubClass extends SuperClass implements MyInterface{ public static void main(String[] args) { SubClass subClass = new SubClass(); subClass.print(); // class print } } 对象克隆 对象克隆分为浅克隆和深克隆,浅克隆和深克隆的根本区别就是是否复制了对象中的引用类型而不是复制了一份引用 java中实现克隆的方式有两种,一种是使用Object类中的clone()方法,另一种则是使用序列化,前者需要类实现Cloneable接口,后者需要实现Serializable接口,区别是序列化克隆是深克隆,而使用clone()则需要类中的引用类型的成员变量也支持克隆(实现Cloneable接口),并且手动创建其对象,如果一个类中引用了多个其他类的对象,或者这些对象中又有多层的引用,使用clone()的方式将会很麻烦 clone()浅克隆 @Data @AllArgsConstructor @NoArgsConstructor public class OrderItem implements Cloneable{ private Integer id; private Integer count; private Product product; private BigDecimal totalPrice; private List products; @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } @Data @AllArgsConstructor @NoArgsConstructor public class Product { private String pid; private String name; private BigDecimal price; } Product product = new Product(); product.setPid(\"1001\"); product.setName(\"数码产品\"); OrderItem orderItem = new OrderItem(); orderItem.setId(128); orderItem.setProduct(product); System.out.println(orderItem); OrderItem cloneItem = (OrderItem) orderItem.clone(); System.out.println(cloneItem); System.out.println(\"============================\"); // 修改cloneItem中类型为Integer的变量id并没有影响原对象 // 包括String都是复制的值 cloneItem.setId(127); System.out.println(orderItem); System.out.println(cloneItem); System.out.println(\"============================\"); // 修改cloneItem中的引用的其他对象中的变量,此时原对象受到了影响 // 如果对象中引用了其他类的对象,复制的只是引用类型变量的地址值 cloneItem.getProduct().setName(\"生活用品\"); System.out.println(orderItem); System.out.println(cloneItem); clone()深克隆 @Data @AllArgsConstructor @NoArgsConstructor public class OrderItem implements Cloneable{ private Integer id; private Integer count; private Product product; private BigDecimal totalPrice; // 可以选择将修饰符由改为public,还可以将返回值改为对应的类型,避免类型转换 @Override // protected Object clone() throws CloneNotSupportedException { public OrderItem clone() throws CloneNotSupportedException { OrderItem orderItem = (OrderItem) super.clone(); orderItem.setProduct(product.clone()); return orderItem; } } @Data @AllArgsConstructor @NoArgsConstructor public class Product implements Cloneable { private String pid; private String name; private BigDecimal price; // @Override // protected Object clone() throws CloneNotSupportedException { // return super.clone(); // } @Override public Product clone() throws CloneNotSupportedException { return (Product) super.clone(); } } Product product = new Product(); product.setPid(\"1001\"); product.setName(\"数码产品\"); OrderItem orderItem = new OrderItem(); orderItem.setId(128); orderItem.setProduct(product); System.out.println(orderItem); System.out.println(\"==============\"); OrderItem cloneItem = orderItem.clone(); // 这次修改克隆的对象中的Product并没有影响原对象中的Product实现了深度克隆 cloneItem.getProduct().setName(\"生活用品\"); System.out.println(orderItem); System.out.println(cloneItem); // console OrderItem(id=128, count=null, product=Product(pid=1001, name=数码产品, price=null), totalPrice=null) ============== OrderItem(id=128, count=null, product=Product(pid=1001, name=数码产品, price=null), totalPrice=null) OrderItem(id=128, count=null, product=Product(pid=1001, name=生活用品, price=null), totalPrice=null) 序列化 在类中定义方法 public OrderItem myClone(){ OrderItem clone = null; try { ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); clone = (OrderItem) ois.readObject(); oos.close(); ois.close(); }catch (Exception e){ e.printStackTrace(); } return clone; } 在文件中读写 Product product = new Product(); product.setPid(\"1001\"); product.setName(\"数码产品\"); OrderItem orderItem = new OrderItem(); orderItem.setId(128); orderItem.setProduct(product); System.out.println(orderItem); ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(orderItem); FileOutputStream fos = new FileOutputStream(new File(\"d:\\\\temp\\\\objects.txt\")); fos.write(bos.toByteArray()); fos.flush(); fos.close(); BufferedInputStream fis = new BufferedInputStream(new FileInputStream(\"d:\\\\temp\\\\objects.txt\")); ObjectInputStream objectInputStream = new ObjectInputStream(fis); OrderItem clone = (OrderItem) objectInputStream.readObject(); clone.getProduct().setName(\"zzzzz\"); System.out.println(clone); fis.close(); 内部类 内部类: 一个类定义在另一个类的内部(成员位置),该类就叫做内部类,而包含这个内部类的类则被称为外部类,内部类被编译后会独自生成一个class文件,文件的名字: 外部类$内部类.class 内部类的特点: 内部类可以使用任意的权限修饰符以及final、static修饰符修饰,而外部类只能使用public或者不写修饰符默认本包访问,内部类可以直接访问外部类成员,包括私有成员 成员内部类 分为普通的成员内部类和静态成员内部类 普通的成员内部类不允许有静态成员,原因为我们常常设置一个静态的成员是为了让该类所有实例共享这块区域,而实例都保存了自己的一份内部类 静态成员内部类创建对象不需要new,可以使用在内部定义静态成员 public class Outer { private String name = \"my name is outer\"; public class Inner { private Integer age = 18; // 不能定义为static 方法也一样,都可以使用final修饰 // private static Integer age = 18; // 成员内部类可以访问任意的外部类的成员 public void printOuterName(){ System.out.println(name); Outer.staticMethon(); } } public static class StaticInner{ public static void print(){ System.out.println(\"static inner\"); } } // 外部类访问内部类成员时,必须创建其对象 public void printInnerAge(){ // 在外部类中创建内部类的方式 Inner inner = new Inner(); System.out.println(inner.age); } static void staticMethon(){ System.out.println(\"static method\"); } } Outer outer = new Outer(); // outer.printInnerAge(); // 也可以将外部类导出,省略前面的声明 // import xyz.taoqz.chapter4.innerclass.Outer.Inner; // Inner inner = new Outer().new Inner(); // 创建成员内部类对象的格式: 外部类.内部类 变量 = new 外部类().new 内部类(); Outer.Inner inner = new Outer().new Inner(); inner.printOuterName(); // 创建静态内部类对象的格式: 外部类.内部类 变量 = new 外部类.内部类(); // 静态内部类创建对象不需要new Outer.StaticInner staticInner = new Outer.StaticInner(); // 外部类.内部类.静态方法 Outer.StaticInner.print(); 局部内部类 public class InnerInterface { // void fun(); public void fun(){ System.out.println(\"父类\"); } } /** * 局部内部类 */ public static InnerInterface partInnerClassdemo(){ String name; // 和方法中的变量一样,不允许有任何权限修饰符,final除外(static也不能声明) // 作用域只在本方法中,但可以通过实现接口或者继承的方式,进行多态调用 // class PartInner implements InnerInterface{ class PartInner extends InnerInterface{ private String name = \"partInnerClass\"; private boolean flag = true; @Override public void fun() { super.fun(); System.out.println(\"我是局部内部类\"); } } PartInner partInner = new PartInner(); System.out.println(partInner.name); return partInner; } InnerInterface innerInterface = MyTest.partInnerClassdemo(); innerInterface.fun(); 局部内部类编译后生成的class文件 匿名内部类 匿名内部类: 是内部类的简化写法,本质是一个带具体实现的父类或接口的匿名的子类对象 当我们需要使用一个接口时,需要创建类实现该接口重写接口中方法,再创建子类对象调用重写后的方法,匿名内部类的语法可以方便快捷的创建一个其子类,匿名类的前提是必须继承一个父类或者父接口 格式 new 父类或者接口名{ // 方法重写 @Override public void method() { // 执行语句 } } public interface MyAnonymityInterface { int add(int a,int b); } @Test public void demo() { // 父类引用指向子类对象 MyAnonymityInterface my = new MyAnonymityInterface() { @Override public int add(int a, int b) { return a + b; } }; System.out.println(my.add(2,3)); // 直接使用子类对象调用方法,返回结果 int add = new MyAnonymityInterface() { @Override public int add(int a, int b) { return a + b; } }.add(1, 2); System.out.println(add); } public static void main(String[] args) { // jdk中常用的匿名内部类的写法 // 1. 创建一个线程 new Thread(new Runnable(){ @Override public void run() { for (int i = 0; i () { @Override public int compare(Integer o1, Integer o2) { return o2 - o1; } }); System.out.println(Arrays.toString(arr)); } 匿名内部类编译后也会生成class文件(因为在这个类里一共使用了4个匿名内部类的写法,对应的也生成了四个class文件) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/base/异常.html":{"url":"java/base/异常.html","title":"异常","keywords":"","body":"异常java中的异常体系抛出异常声明异常捕获异常finallyreturn常用方法方法重写自定义异常异常 异常是程序中的一些错误,但并不是所有的错误都是异常,比如在java中忘记写分号,这些都是可以避免的。java把异常信息封装成了一个类,当出了问题时,就会创建异常类对象并抛出异常相关的信息(如异常出现的位置、原因等) java中的异常体系 Throwable(是整个异常机制的顶级超类,所有的异常类都继承自它) Error:指程序中出现的严重错误,java程序通常不捕获错误 比如:OutOfMemoryError, StackOverflowError Exception:程序员可以进行处理的异常 RuntimeException:运行时期异常(可以处理,但不一定要处理) 常见:ClassCastException, ArrayIndexOutOfBoundsException, NullPointerException 非RuntimeException:检查性异常,要求在程序编译期间就要处理 常见:IOException, SQLException, InterruptedException RuntimeException和它的所有子类都属于运行时期异常。特点是:方法中抛出运行时期异常,方法中无需throws声明或捕获,调用者也无需处理此异常,但异常一旦发生,需要修改源代码。 抛出异常 关键字throw,比如在方法传参我们需要对参数进行操作时,要先判断其是否合法。 public static void print(String string){ if (string == null) // 手动抛出异常,调用其父类的构造传入异常信息 throw new NullPointerException(\"您传入的参数为null\"); System.out.println(string.length()); } public static void main(String[] args) { String name = null; // String name = \"法外狂徒张三\"; print(name); } // 异常所在的线程,异常名称,异常信息以及异常出现的位置 // 在位置信息中出现两行,一行是问题实际所在位置,另一行则是调用方,此处为main方法,如果main不做处理会交由jvm处理 // 并打印出异常信息 Exception in thread \"main\" java.lang.NullPointerException: 您传入的参数为null at xyz.taoqz.MyTest.print(MyTest.java:12) at xyz.taoqz.MyTest.main(MyTest.java:19) 但如果方法中抛出的异常是非运行时期异常,那么需要在该方法上进行声明或者捕获 public void fun(){ throw new IOException(); // 会报错,需要捕获或抛出 } 声明异常 当某个方法可能会抛出异常时,但不想在本方法中进行处理,所以需要告诉调用者可能会出现异常需要处理。 关键字throws,位置方法参数后,多个异常逗号分隔。 // 例如 public static void fun(int[] arr,int index) throws NullPointerException,IndexOutOfBoundsException{ System.out.println(arr[index]); } 重写和实现时方法可以声明的异常受父类或接口方法中的限制。例如父类中没有声明异常,而子类重写时想抛出非运行时异常,但其不能声明抛出,这时可以使用异常的转换。 RuntimeException(Throwable t):提供了构造 throw new RuntimeException(new IOException()); 捕获异常 Java不仅提供了抛出异常的机制,也提供了捕获异常的机制。 try块是监控区,一旦出现异常会由上而下寻找匹配的catch块,然后执行,若未找到匹配的catch块会向上抛出 基本语法 try { // 可能会出现异常的代码 } catch (异常名称 异常对象) { // 打印异常信息,或者做其他操作 // e.printStackTrace(); } // 也可以有多个catch区域,如果catch中的异常对象存在子父关系,子类异常需要在父类异常之前声明,要不然永远轮不到子类异常的执行 try { } catch (异常名称 异常对象) { // e.printStackTrace(); } catch (异常名称 异常对象) { // e.printStackTrace(); } public static void fun(int[] arr,int index) { try { System.out.println(arr[index]); } catch (NullPointerException | ArrayIndexOutOfBoundsException e) { e.printStackTrace(); } } public static void main(String[] args) { // int[] arr = {1,3,2}; int[] arr = null; fun(arr,3); } finally 和try..catch或者try..finally一起使用,表示无论是否发生异常一定会执行的代码块(比如释放资源) try { throw new SQLException(); } catch (SQLException e) { e.printStackTrace(); } finally { // 释放资源 } // 对代码进行异常检测,检测异常后没有catch(声明),所以一样会被jvm抛出,异常没有捕获处理,但开启的资源可能需要关闭使用finally public void method() throws Exception { try { throw new Exception(); } finally { // 释放资源 } } return 当try语句和finally语句中都有return语句时,在方法返回之前,finally语句的内容将被执行,并且finally语句的返回值将会覆盖原始的返回值。 finally不会执行的情况 在 finally 语句块第一行发生了异常。 因为在其他行，finally 块还是会得到执行 在前面的代码中用了 System.exit(int)已退出程序。 exit 是带参函数 ；若该语句在异常语句之后，finally 会执行 程序所在的线程死亡。 关闭 CPU。 https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.md#32-java-%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86 常用方法 getMessage():返回异常的详细信息字符串(构造中的字符串) toString():异常类及异常信息 printStackTrace():异常类,异常信息以及位置 方法重写 子类覆盖父类方法时,如果父类没有声明异常,子类也不能声明异常(这里的异常指的是非运性时期异常),如果子类出现异常只能捕获。 子类声明的异常不能大于父类声明的异常(子类或平级或不声明) 自定义异常 定义类,需要运行时异常继承RuntimeException不需要声明,需要非运行时异常继承Exception需要声明或捕获。 @Data public class Person { private String name; private Integer age; public Person(String name, Integer age) throws AgeIllegalExecption { if (age 200){ throw new AgeIllegalExecption(age+\",年龄值不合法\"); } this.name = name; this.age = age; } } public class AgeIllegalExecption extends Exception{ public AgeIllegalExecption() { super(); } public AgeIllegalExecption(String message) { super(message); } } try { Person person = new Person(\"张三\", 24); } catch (AgeIllegalExecption ex) { System.out.println(ex.getMessage()); } 断言 用于开发和测试阶段,比如对方法参数的校验,需要手动开启 vm参数: -ea 语法 public static void main(String[] args) { fun(10); } public static void fun(int x){ // assert x = 10) throw new AssertionError(); assert x = 10\"; // 等同于 if (x >= 10) throw new AssertionError(\"x >= 10\"); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/java8new_feature/默认方法.html":{"url":"java/java8new_feature/默认方法.html","title":"默认方法","keywords":"","body":"默认方法default关键字多个默认方法静态方法默认方法 默认方法就是接口可以有默认已实现方法,而且不需要实现类去实现其方法 default关键字 主要为了解决向接口中添加新功能扩展而不影响之前实现该接口的类 public interface DefaultOne { default void print(){ System.out.println(\"defaultOne\"); } } 创建实现类直接调用即可 多个默认方法 如果一个类实现了多个拥有同名默认方法的接口怎么办 会编译错误,解决办法有两种 第一种自然是实现类对方法进行重写 第二种使用super关键字指定调用哪个接口中的默认方法 public interface DefaultOne { default void print(){ System.out.println(\"defaultOne\"); } } public interface DefaultTwo { default void print(){ System.out.println(\"defaultTwo\"); } } public class MyBase implements DefaultOne,DefaultTwo{ @Override public void print() { // 可同时调用 // DefaultOne.super.print(); DefaultTwo.super.print(); System.out.println(\"新增内容\"); } } 静态方法 public interface DefaultOne { default void print(){ System.out.println(\"defaultOne\"); } static void run(){ System.out.println(\"run\"); } } 可以直接使用接口名调用 @Test public void demo(){ new MyBase().print(); DefaultOne.run(); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/java8new_feature/Lambda.html":{"url":"java/java8new_feature/Lambda.html","title":"Lambda","keywords":"","body":"Lambda表达式函数式接口自定义函数式接口方法引用创建数组内置的函数式接口SupplierConsumerPredicateFunctionComparator比较器Lambda表达式 Lambda 允许把函数作为一个方法的参数(将函数作为参数传递到方法中) 可以使代码变得更加简洁紧凑 重要特征 不需要声明参数的类型,编译器可以进行识别 当函数表达式只有一个参数时可以省略小括号(有多个不可省略) 如果函数表达式的实现主体只有一个语句,不需要大括号 如果函数表达式的主体只有一个返回值,编译器会自动返回(省略大括号及return) 对Lambda的个人理解:本质是一个已经实现了方法主体的对象,减少了实现类和简化了匿名内部类的代码 函数式接口 在Java8中可以写成函数表达式,是因为Java8提供了函数式接口,只有这些函数式接口才可以简写成Lambda表达式,并且Java编译器可以进行类型推断 函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。 函数式接口可以被隐式转换为 lambda 表达式。 自定义函数式接口 // 接口中只有并且只有一个抽象方法的接口就是一个函数式接口 @FunctionalInterface // 该注解会检查是否符合函数式接口的规范,符合规则不写也可以 public interface MyFunctionalInterface { // 默认public abstract void method(); // void fun(); } public class MyTest { public static void main(String[] args) { fun(() -> System.out.println(\"123\")); } private static void fun(MyFunctionalInterface myFunctionalInterface){ myFunctionalInterface.method(); } } 方法引用 方法引用使用一对::冒号,通过方法的名字来指向一个方法 @Data @NoArgsConstructor @AllArgsConstructor public class Bird { private int age; private String name; public static Bird Birdfactory(Supplier supplier){ return supplier.get(); } public static String getBirdName(Supplier supplier){ return supplier.get(); } public void print(){ System.out.println(name); } public String getName() { // System.out.println(name); return name; } public static void print2(Bird bird){ System.out.println(\"静态方法\"); } } public class MyTest3 { public static void main(String[] args) { // 构造器引用 ClassName::new Bird bird = Bird.Birdfactory(Bird::new); bird.setName(\"啄木鸟\"); bird.setAge(2); // 使用的是Supplier接口 Supplier supplier = Bird::new; Bird bird1 = supplier.get(); // 特定对象的方法引用 Supplier instance::method String birdName = Bird.getBirdName(bird::getName); System.out.println(birdName); // 为了演示其他对象也可以使用该方式进行方法的引用 // String str = \"qwe\"; // String birdName1 = Bird.getBirdName(str::toUpperCase); // System.out.println(birdName1); System.out.println(\"=================\"); List birds = Arrays.asList(bird, new Bird(5,\"八声杜鹃\"), new Bird(6,\"雎鸠\"), new Bird(3,\"沙百灵\") ); // 下面两者本质是一个消费者 Consumer 在forEach中会调用其accept()方法 // 特定类的任意对象的方法的引用 Class:new birds.forEach(Bird::print); System.out.println(\"=============\"); // 静态方法引用 Class::static_method // birds.forEach(Bird::print2); Function fun = Math::ceil; Double apply = fun.apply(100.1); System.out.println(apply); // 在这里的时候遇到了一个问题(理解不到位),使用forEach(Bird::getName)为什么不可以,因为forEach的参数是个Consumer消费者 // 只会调用方法不会对其返回值有操作 // 流操作对最终的集合没有影响 birds.stream().sorted(Comparator.comparing(Bird::getAge)).forEach(System.out::println); /* Bird(age=2, name=啄木鸟) Bird(age=3, name=沙百灵) Bird(age=5, name=八声杜鹃) Bird(age=6, name=雎鸠)*/ // List collect = birds.stream().sorted(Comparator.comparing(Bird::getName)).collect(Collectors.toList()); // System.out.println(collect); System.out.println(birds); // [Bird(age=2, name=啄木鸟), Bird(age=5, name=八声杜鹃), Bird(age=6, name=雎鸠), Bird(age=3, name=沙百灵)] } } 创建数组 // 指定长度 分配空间并初始默认值(不同的类型有不同的默认值) int[] one = new int[10]; // 显示初始化 int[] two = {1,2,3}; // 显示初始化 int[] three = new int[]{1,2,3}; // 数组一旦进行初始化,分配内存空间,其数组长度不可变 // 使用方法引用的方式创建数组 // 因为需要指定数组的长度,java使用了Function接口 利用了其方法的特点 传入第一个泛型类型的参数,返回值一个第二个泛型类型的值 Function functionStringArray = String[]::new; String[] apply = functionStringArray.apply(10); System.out.println(apply.length); System.out.println(apply[0]); 内置的函数式接口 JDK在java.util.function包中提供了大量常用的函数式接口 创建一个bean方便后面使用 @Data @AllArgsConstructor public class Person { private String name; private Integer age; } Supplier 该接口仅包含一个无参的方法,返回值和泛型一致 public class SupplierDemo { // 生产一个数据 返回值便为泛型 private static String getString(Supplier funtion){ return funtion.get(); } @Test public void fun(){ String name = \"java\"; String string = getString(name::toUpperCase); System.out.println(string); } } Consumer 接收一个参数无返回值,类似消费者 public class ConsumerDemo { // 消费一个数据 private static void printString(Consumer function,Person person){ function.accept(person); } // 将数据按照函数表达式的顺序完成操作 private static void consumerString(Consumer one,Consumer two,String zz){ one.andThen(two).accept(zz); } public static void main(String[] args) { Person person = new Person(\"张三\", 123); printString(s -> System.out.println(s.getName()),person); consumerString( s -> System.out.println(s.toLowerCase()+\"==one\"), s -> System.out.println(s.toUpperCase()+\"==two\"),\"cZxy\" ); } } Predicate 接收一个参数,返回一个布尔值结果 public class PredicateDemo { // 提供返回值为布尔类型的函数式接口 // test 判断 // negate 取反 private static boolean stringLength(Predicate funtion,String string){ return funtion.test(string); // return funtion.negate().test(string); } // 完成多个数据之间的逻辑判断 or或者 and并且 private static boolean stringContains(Predicate one,Predicate two,String string){ // or方法的参数也是一个函数表达式 return one.or(two).test(string); // return one.and(two).test(string); } public static void main(String[] args) { // 参数长度是否大于50 System.out.println(stringLength(s -> s.length() > 50,\"Hello World\")); // 参数是否以J开头或者包含W System.out.println(stringContains(one -> one.startsWith(\"J\"), two -> two.contains(\"W\"),\"Hello World \")); // 将字符串切割后 sex为女 名字长度为4的返回做输出打印 String[] array = { \"迪丽热巴,女\",\"古力娜扎,女\",\"马尔扎哈,男\",\"赵丽颖,女\"}; demo( sex -> sex.split(\",\")[1].equals(\"女\"), length -> length.split(\",\")[0].length() == 4,array).forEach(System.out::println); } private static List demo(Predicate sex,Predicate length,String[] string){ ArrayList strings = new ArrayList<>(); for (String s : string) { if (sex.and(length).test(s)){ strings.add(s); } } return strings; } } Function 接收一个参数T,返回一个结果R public class FunctionDemo { // apply参数是第一个泛型的类型,返回值是第二个参数的泛型的类型 // 当然两个泛型可以一样 private static void method(Function funtion,String num){ System.out.println(funtion.apply(num)+1); } public static void main(String[] args) { method(Integer::parseInt,\"50\"); String person = \"赵丽颖,20\"; example( age -> age.split(\",\")[1], Integer::parseInt, add -> add += 100, person ); } // 需求,拿到字符串中的数字转为Integer再进行数学运算 // 第一个参数 拿到字符串中的数字字符串 // 再将数字字符串转为Integer // 对数字进行数学运算 // 其中参数的泛型: 从第二个参数开始,第一个泛型的类型都是前一个参数的返回值类型 // andThen 和consumer中的作用类似,都是先执行什么操作然后在根据结果执行另一个操作 private static void example(Function split,Function parseInt,Function result,String age){ System.out.println(split.andThen(parseInt).andThen(result).apply(age)); } } Comparator比较器 public class ComparatorDemo { // java8之后改为函数表达式 // 返回值是一个函数表达式 private static Comparator myComparator(){ return (a, b) -> b.length() - a.length() ; } public static void main(String[] args) { String[] array = {\"asd\",\"qwer\",\"zz\"}; // java8之前的写法 // o1代表当前元素 o2代表其后的元素 返回值为1 o1向右放 -1 o1向左放 0相等 不动 // Arrays.sort(array, new Comparator() { // @Override // public int compare(String o1, String o2) { // return o2.length() - o1.length(); // } // }); Arrays.sort(array,myComparator()); System.out.println(Arrays.toString(array)); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/java8new_feature/Lambda访问外部变量.html":{"url":"java/java8new_feature/Lambda访问外部变量.html","title":"Lambda访问外部变量","keywords":"","body":"Lambda访问外部变量局部变量成员变量及静态变量访问接口的默认方法Lambda访问外部变量 局部变量 可以访问num变量和进行final修饰后的num变量,那么区别在哪里 可以看到如果变量在lambda表达式后进行修改会报错,也就是num变量必须是final的 在不进行修改的情况下为什么第一个注释掉的num也可用,是因为此时的num是隐式的final,也就是后续没有进行修改 public static void main(String[] args) { // Integer num = 123; final Integer num = 123; Function function = (param) -> Integer.parseInt(param)+num; Integer apply = function.apply(\"12\"); System.out.println(apply); // Error:(14, 80) java: 从lambda 表达式引用的本地变量必须是最终变量或实际上的最终变量 // num = 11; } 在lambda表达式中修改变量也是不可用的 public static void main(String[] args) { Integer num = 123; Function function = (param) -> { Integer result =Integer.parseInt(param)+num; // Variable used in lambda expression should be final or effectively final num = 1; return result; }; Integer apply = function.apply(\"12\"); System.out.println(apply); } 不允许声明一个与局部变量同名的参数或者局部变量 public static void main(String[] args) { Integer num = 123; // Variable 'num' is already defined in the scope Function function = (num) -> Integer.parseInt(num)+num; Integer apply = function.apply(\"12\"); System.out.println(apply); } 成员变量及静态变量 public class MyLambda { // 非包装类的基本数据类型,静态和非静态都是有默认值的 static int num; int number; public void print(){ // 静态变量 Function function1 = param -> { num = 1; return Integer.parseInt(param)+num; }; System.out.println(function1.apply(\"123\")); // 成员变量 Function function2 = param -> { number = 2; return Integer.parseInt(param)+number; }; System.out.println(function2.apply(\"456\")); } @Test public void fun(){ new MyLambda().print(); } } 访问接口的默认方法 // method方法和converter 都是将字符串转为Integer,为了演示Lambda是否可以直接调用默认方法 @FunctionalInterface public interface MyFunction { Integer method(String string); default Integer converter(String string){ return Integer.parseInt(string); } } @Test public void fun(){ MyFunction myFunction = param -> Integer.parseInt(param); Integer result1 = myFunction.method(\"123\"); System.out.println(result1); // 使用匿名对象的方式可以访问接口中的默认方法 MyFunction myFunction2 = new MyFunction(){ @Override public Integer method(String string) { return converter(string); } }; Integer result2 = myFunction2.method(\"456\"); System.out.println(result2); // lambda不能访问接口中的默认方法 // MyFunction myFunction3 = param -> converter(param); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/java8new_feature/日期时间API.html":{"url":"java/java8new_feature/日期时间API.html","title":"新的日期时间API","keywords":"","body":"Java 8 日期时间 APILocalDateLocalTimeLocalDateTimeDateTimeFormatterTemporalAdjustersZonedDateTime、Instant时间戳、时区SpringBoot针对新日期时间API的转换Java 8 日期时间 API ​ 在旧版的Java中,日期时间API主要在java.util包下,但其存在许多问题,比如java.util.Date是非线程安全的,所有的日期类都是可变的。从Java8开始,Java API中已经能够提供高质量的日期和时间支持,java.time包,并且日期时间都是不可变和线程安全的 LocalDate 该类是一个不可变的和线程安全的;表示日期,通常被视为年月日,该类不存储或表示时区 public class LocalDateDemo { public static void main(String[] args) { // LocalDate 不可变,线程安全的日期类 年月日 // 获取当前日期 默认格式:yyyy-MM-dd 根据系统的时钟和时区获取日期 LocalDate date = LocalDate.now(); // 年 int year = date.getYear(); // 月 int monthValue = date.getMonthValue(); // 日(号) int dayOfMonth = date.getDayOfMonth(); System.out.println(date); System.out.println(year+\"年/\"+monthValue+\"(\"+date.getMonth()+\")月/\"+dayOfMonth+\"日\"); // 星期 int week = date.getDayOfWeek().getValue(); System.out.println(\"星期:\"+week+\"===\"+date.getDayOfWeek()); // 当年中的第几天 int dayOfYear = date.getDayOfYear(); // 当月的总天数 int monthLength = date.lengthOfMonth(); // 当年的总天数 int yearLength = date.lengthOfYear(); System.out.println(\"几天是本年的第\"+dayOfYear+\"天\"); System.out.println(\"本月的总天数为:\"+monthLength); System.out.println(\"当年的总天数为:\"+yearLength); System.out.println(date.isLeapYear() ? \"闰年\" : \"平年\"); } @Test public void fun(){ // 在创建LocalDate时会对日期进行校验,如果任何字段的值超出范围，或者如果月的日期对于月份无效将抛出异常 // java.time.DateTimeException: Invalid date 'February 29' as '2019' is not a leap year // 2019年不是闰年,所以2月没有29号 // LocalDate of = LocalDate.of(2019, 2, 29); // 虽然2020年时闰年,但是2月份最多只有29天 // java.time.DateTimeException: Invalid date 'FEBRUARY 30' // LocalDate of = LocalDate.of(2020, 2, 30); LocalDate of = LocalDate.of(2020, 2, 29); System.out.println(of); // 根据一年中的第一天创建对象 LocalDate date = LocalDate.ofYearDay(2020, 32); System.out.println(date); // 参数需要一个正确的日期格式 年-月-日 // 严格按照yyyy-MM-dd的格式验证 需补0 // LocalDate parse = LocalDate.parse(\"1999-10-9\"); LocalDate parse = LocalDate.parse(\"1999-10-09\"); System.out.println(parse); // 查看指定日期是否在参数日期之前, 其他还有 之后,相等方法来比较日期 boolean after = parse.isBefore(LocalDate.now()); System.out.println(after); } } LocalTime 该类是一个不可变的和线程安全的;表示时间,通常被视为时分秒,该类不存储或表示时区 public class LocalTimeDemo { public static void main(String[] args) { // LocalTime 不可变线程安全的时间类 // 默认格式 时:分:秒:纳秒(nanosecond) LocalTime now = LocalTime.now(); System.out.println(now); // 指定时分秒创建对象 LocalTime of = LocalTime.of(18, 30,48); System.out.println(of); System.out.println(of.getHour()+\"时\"+of.getMinute()+\"分\"+of.getSecond()+\"秒\"); // 减去指定的Hour并返回对象 同样可减去分秒纳秒 LocalTime minusHours = of.minusHours(2); System.out.println(minusHours); // 增加指定的Minute 同样可增加时秒纳秒 LocalTime plusMinutes = of.plusMinutes(29); // of.plusNanos(12); System.out.println(plusMinutes); // 根据文本解析为LocalTime对象 // 严格按照HH:mm:ss的格式验证 需补0 // LocalTime parse = LocalTime.parse(\"09:20:6\"); LocalTime parse = LocalTime.parse(\"09:20:06\"); System.out.println(parse); boolean after = of.isAfter(now); System.out.println(\"是否在当前时间之后:\"+after); } @Test public void demo(){ LocalTime of = LocalTime.of(18, 30,38,199); // java.time.DateTimeException: Invalid value for HourOfDay (valid values 0 - 23): 24 // 从0-23来表示时间 // 将时间改为指定参数值,相当于进行set LocalTime localTime = of.withHour(6).withMinute(10); System.out.println(localTime); // 获取当前时间总秒数 int i = localTime.toSecondOfDay(); System.out.println(i); // 将秒数转为LocalTime对象 LocalTime ofSecondOfDay = LocalTime.ofSecondOfDay(i); System.out.println(ofSecondOfDay); // System.out.println(of); // System.out.println(of.getNano()); } } LocalDateTime 上面的两个类一个表示日期,一个表示时间,而该类更像是将两者结合起来,通过观看源码得知,它确实也是这么做的 构造是个私有的,一个参数是LocalDate另一个则是LocalTime public class LocalDateTimeDemo { public static void main(String[] args) { // 获取当前日期加时间 // 效果是其LocalDate和LocalTime的合体,同时代表了日期和时间 使用系统时区 LocalDateTime now = LocalDateTime.now(); // 打印格式 2020-03-16T20:49:02.450 System.out.println(now); // 同样可以根据字符串解析为对象 其中T是必须的表示时间的开始 // 并且严格按照yyyy-MM-ddTHH:mm:ss的格式进行验证,需补0 // LocalDateTime parse = LocalDateTime.parse(\"2001-12-9T10:22:36\"); LocalDateTime parse = LocalDateTime.parse(\"2001-12-09T10:22:36\"); System.out.println(parse); // 根据指定的日期和时间创建对象 LocalDateTime of = LocalDateTime.of(2020, 2, 20, 15, 30,36); System.out.println(of); // 通过查看其 of 方法的实现发现其实使用了之前两个日期时间类的构造 // 上面创建方式和本方式创建对象一致 LocalDate localDate = LocalDate.of(2020, 2, 20); LocalTime localTime = LocalTime.of(15, 30, 36); // 该类的构造方法的是私有的,该方式是因为该类提供了静态的of方法,在方法内部使用其构造方法new的方式创建对象 LocalDateTime localDateTime = LocalDateTime.of(localDate, localTime); // true System.out.println(of.isEqual(localDateTime)); // 同样LocalDate和LocalTime可以使用自身的 atXXXX 来进行组合返回一个完整的LocalTime对象 LocalDateTime localDateTime1 = localDate.atTime(localTime); LocalDateTime localDateTime2 = localTime.atDate(localDate); // 并且可以灵活转换 // LocalDate date = localDateTime1.toLocalDate(); // LocalTime time = localDateTime1.toLocalTime(); // 比较日期时间是否一致 System.out.println(localDateTime1.isEqual(localDateTime2)); } @Test public void demo(){ LocalDateTime now = LocalDateTime.now(); System.out.println(now); // withXXXX 用于修改为指定的日期或时间 // plusXXXX 用于将日期或时间和参数相加 // minusXXXX 用于将日期或时间减去相应的参数值 LocalDateTime withHour = now.withHour(2).plusMinutes(20).minusSeconds(5); System.out.println(withHour); } } DateTimeFormatter 在java8之前使用的SimpleDateFormat可能会出现线程安全的问题,而在Java8中新提供的DateTimeFormatter可以解决这个问题,该类也主用于日期时间的解析和格式化。这个类是不可变的和线程安全的。 public class DateTimeFormatterDemo { public static void main(String[] args) { // 按照格式进行相互转换 LocalDateTime now = LocalDateTime.of(2020,03,16,22,59,23); System.out.println(now); String format = now.format(DateTimeFormatter.BASIC_ISO_DATE); // 20200316 System.out.println(format); // 22:59:23 format = now.format(DateTimeFormatter.ISO_LOCAL_TIME); // 2020-03-16 System.out.println(format); LocalDate localDateParse = LocalDate.parse(\"20200316\", DateTimeFormatter.BASIC_ISO_DATE); // 2020-03-16 System.out.println(localDateParse); LocalTime localTimeParse = LocalTime.parse(\"22:59:23.017\", DateTimeFormatter.ISO_LOCAL_TIME); // 22:59:23.017 System.out.println(localTimeParse); } /** * 自定义格式 */ @Test public void demo(){ // 从模式创建的格式化程序可以根据需要多次使用，它是不可变的并且是线程安全的。 DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy/MM/dd\"); // 按照指定格式将日期格式化 String format = LocalDateTime.now().format(dateTimeFormatter); // 2020/03/16 System.out.println(format); // 根据指定的格式将字符串再转为对象 LocalDate parse = LocalDate.parse(format, dateTimeFormatter); // 2020-03-16 System.out.println(parse); // 根据给定字符串,创建匹配规则并转为时间对象 String time = \"18时12分40秒\"; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"HH时mm分ss秒\"); LocalTime localTime = LocalTime.parse(time, formatter); System.out.println(localTime); } } TemporalAdjusters java8之前的java.util包中的Date类,它提供了公开的修改时间的方法,因此可以轻松的修改时间 而在Java8中新的时间API,表示时间的类都是不可变的,类本身和变量都被final修饰,所以是不可变的,因此又提供了该类用于进行日期时间的复杂操作,但也正因为这些类都是不可变的所以使用该类产出的对象都是一个新的日期或时间对象 虽然没有直观的setXxxx方法,但是也提供了对应的withXxxx方法,和set方法功能一致,只不过同样是产出新的对象 import static java.time.temporal.TemporalAdjusters.*; public class TemporalAdjusterDemo { public static void main(String[] args) { LocalDateTime now = LocalDateTime.now(); System.out.println(now); LocalDateTime with; // 根据ordinal的值 进行计算将日期调整为计算后的week with = now.with(dayOfWeekInMonth(-6,DayOfWeek.SUNDAY)); // 创建一个新的日期,将日期调整为该月份的第一天 with = now.with(firstDayOfMonth()); // 创建一个新的日期,将日期调整为下个月的第一天 with = now.with(firstDayOfNextMonth()); // 创建一个新的日期,将日期调整为当年的第一天 with = now.with(firstDayOfYear()); // 创建一个新的日期,将日期调整为下一年的第一天 with = now.with(firstDayOfNextYear()); // 创建一个新的日期,将日期调整为第一个指定的星期 with = now.with(firstInMonth(DayOfWeek.SUNDAY)); // 创建一个新的日期,将日期调整为当月的最后一天 with = now.with(lastDayOfMonth()); // 创建一个新的日期,将日期调整为当年的最后一天 with = now.with(lastDayOfYear()); // 创建一个新的日期,将日期调整为当月的最后一个指定的星期 with = now.with(lastInMonth(DayOfWeek.THURSDAY)); // 创建一个新的日期,将日期调整为下一周指定的星期 with = now.with(next(DayOfWeek.MONDAY)); // 创建一个新的日期,将日期调整为下一个指定的星期,如果当前星期和参数一致则直接返回 with = now.with(nextOrSame(DayOfWeek.THURSDAY)); // 创建一个新的日期,将日起调整为上一个指定的星期 with = now.with(previous(DayOfWeek.FRIDAY)); // 创建一个新的日期,将日期调整为上一个指定的星期,如果该日期的星期和参数一致则直接返回 with = now.with(previousOrSame(DayOfWeek.WEDNESDAY)); System.out.println(with); // false // 因为java.time中的日期时间类的成员变量都是final的也就是不可变的 // 所以with方法并不是修改而是创建新的对象 System.out.println(with == now); // 可以看到java.util包中中的日期类,提供了public公共的修改访问方法,在并发量大的情况下可能会出现问题 // Date date = new Date(); // date.setTime(); } @Test public void demo(){ LocalDateTime now = LocalDateTime.now(); // Period可以对日期进行操作 LocalDateTime with = now.with(t -> t.plus(Period.ofWeeks(1))); System.out.println(with); } } ZonedDateTime、Instant 前面的日期时间对象都是本地时间但没有时区信息 Instant是世界标准时间且不含时区信息 ZonedDateTime则可以配合ZoneId处理时区,本质是用Instant存储时间,在根据时区进行处理 public class ZoneRulesDemo { public static void main(String[] args) { // 获取内置的所有时区信息 value为 区域/城市 比如 Asia/Shanghai 亚洲/上海 ZoneId.SHORT_IDS.forEach((key,value) -> System.out.println(key+\"===\"+value)); // TimeZone是java8之前的时区对象,将其转为最新的时区对象并获取当前系统的时区 ZoneId aDefault = TimeZone.getDefault().toZoneId(); System.out.println(aDefault); LocalDateTime now = LocalDateTime.now(); // 将当前时区转为指定时区 ZoneId of = ZoneId.of(\"America/Chicago\"); System.out.println(LocalDateTime.ofInstant(now.atZone(ZoneId.systemDefault()).toInstant(),of)); System.out.println(LocalDateTime.ofInstant(Instant.now(),of)); } /** * https://blog.csdn.net/u012107143/article/details/78790378 * 表示时间的主要类: String LocalDateTime Instant ZonedDateTime */ @Test public void demo(){ // 2020-03-16T16:05:59.793Z T代表时间的开始 Z表示这是一个世界标准时间 +00:00 System.out.println(Instant.now()); // 2020-03-17T00:07:08.203 本地时间,不含时区信息的时间 System.out.println(LocalDateTime.now()); System.out.println(LocalDateTime.now(ZoneId.of(\"+00:00\"))); // 2020-03-17T00:07:08.203+08:00[Asia/Shanghai] 本地时间并且显示时区信息 System.out.println(ZonedDateTime.now()); System.out.println(ZonedDateTime.now(ZoneId.of(\"+00:00\"))); // 三个类构造对象的不同方式 // 使用毫秒从1970-01-01T00开始获得的一个实例 Instant 世界标准时间 Instant instant = Instant.ofEpochMilli(System.currentTimeMillis()); System.out.println(instant); LocalDateTime localDateTime = LocalDateTime.of(2020, 02, 20, 18, 30, 16); System.out.println(localDateTime); // 根据指定时区创建对象 ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, ZoneId.of(\"Africa/Cairo\")); // ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, ZoneId.systemDefault()); System.out.println(zonedDateTime); } /** * 其中String和LocalDateTime是等价的,两者可以通过DateTimeFormatter进行相互转换 */ @Test public void stringLocalDateTimeDemo(){ String dateTime = \"2020/02/20 18:30:36\"; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy/MM/dd HH:mm:ss\"); LocalDateTime parse = LocalDateTime.parse(dateTime, formatter); System.out.println(parse); String format = parse.format(formatter); System.out.println(format); } /** * Instant和ZonedDateTime是等价的 */ @Test public void instantZonedDateTime(){ ZonedDateTime zonedDateTime1 = Instant.now().atZone(ZoneId.systemDefault()); ZonedDateTime zonedDateTime2 = Instant.now().atZone(ZoneId.of(\"America/Chicago\")); System.out.println(zonedDateTime1); System.out.println(zonedDateTime1.toInstant()); System.out.println(zonedDateTime1.toLocalDate()); System.out.println(Instant.now().atZone(ZoneId.of(\"+00:00\"))); System.out.println(zonedDateTime2); System.out.println(zonedDateTime2.toInstant()); System.out.println(zonedDateTime2.toLocalDate()); // 通过结果可以看出 根据系统时区和根据其他时区创建的ZonedDateTime对象内部的时间戳Instant和标准时间都是一致的 // 只不过加了时区之后,对时区进行了处理 // 相当于对Instant做了增强 /* 2020-03-17T01:02:56.253+08:00[Asia/Shanghai] 2020-03-16T17:02:56.253Z 2020-03-17 2020-03-16T17:02:56.334Z 2020-03-16T12:02:56.317-05:00[America/Chicago] 2020-03-16T17:02:56.317Z 2020-03-16 */ } @Test public void exchange(){ Instant instant = Instant.now(); LocalDateTime localDateTime = LocalDateTime.ofInstant(instant, ZoneId.systemDefault()); // 系统默认的时区就是该时区,另一种写法 ZonedDateTime zonedDateTime = ZonedDateTime.ofInstant(instant, ZoneId.of(\"Asia/Shanghai\")); // LocalDateTime转为ZonedDateTime ZonedDateTime of = ZonedDateTime.of(localDateTime, ZoneId.systemDefault()); // ZonedDateTime转为LocalDateTime LocalDate localDate = zonedDateTime.toLocalDate(); Instant instant1 = zonedDateTime.toInstant(); System.out.println(instant1); // LocalDateTime转换Instant时需要指定时区,时区必须是时区对应的时间 Instant instant2 = localDateTime.toInstant(ZoneOffset.of(\"+08:00\")); System.out.println(instant2); } } 时间戳、时区 理解时间戳和时区 时间戳:指的是Unix时间戳,是一种时间的表示方式,在地球的每一个角落都是相同的,从格林威治时间1970年01月01日00时00分00秒起至现在的总秒数 可以使用网站查询时间戳:http://tool.chinaz.com/Tools/unixtime.aspx 使用时间戳获取总秒数 @Test public void demo3(){ long epochSecond = Instant.now().getEpochSecond(); System.out.println(epochSecond); } 时区:时间戳在地球的任何位置都是相同的,但是相同的时间点会有不同的表达方式,就是时区的概念。比如我们在中国是白天,而在美国正是夜晚,但是我们过的时间都是一样的,这时便需要使用时区来相互转换获取对方时区的具体时间 @Test public void fun(){ // 获取所有jdk内置的时区信息 ZoneId.SHORT_IDS.forEach((e1,e2) -> System.out.println(e1+\"==\"+e2)); // 根据本地时区获取信息的时间 ZonedDateTime asiaShangha = ZonedDateTime.now(); // 根据指定的时区获取时间 这里使用的是美国纽约 ZonedDateTime americaNewYork = ZonedDateTime.ofInstant(Instant.now(), ZoneId.of(\"America/New_York\")); // 分别打印对方所在时区的具体时间 System.out.println(asiaShangha); System.out.println(americaNewYork); // 获取时间戳总秒数,可以看到是相同的 System.out.println(asiaShangha.toInstant().getEpochSecond()); System.out.println(americaNewYork.toInstant().getEpochSecond()); } 控制台打印信息 CTT==Asia/Shanghai ART==Africa/Cairo CNT==America/St_Johns PRT==America/Puerto_Rico PNT==America/Phoenix PLT==Asia/Karachi AST==America/Anchorage BST==Asia/Dhaka CST==America/Chicago EST==-05:00 HST==-10:00 JST==Asia/Tokyo IST==Asia/Kolkata AGT==America/Argentina/Buenos_Aires NST==Pacific/Auckland MST==-07:00 AET==Australia/Sydney BET==America/Sao_Paulo PST==America/Los_Angeles ACT==Australia/Darwin SST==Pacific/Guadalcanal VST==Asia/Ho_Chi_Minh CAT==Africa/Harare ECT==Europe/Paris EAT==Africa/Addis_Ababa IET==America/Indiana/Indianapolis MIT==Pacific/Apia NET==Asia/Yerevan 2020-03-18T11:15:09.033+08:00[Asia/Shanghai] 2020-03-17T23:15:09.034-04:00[America/New_York] 1584501309 1584501309 SpringBoot针对新日期时间API的转换 方式一:只针对某个字段,需要在每个需要格式化的字段上添加该注解 @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\") private LocalDateTime sendDate; 方式二:全局配置,针对所有的LocalDate以及LocalDateTime字段进行格式化 @Configuration public class ContactAppConfig{ private static final String dateFormat = \"yyyy-MM-dd\"; private static final String dateTimeFormat = \"yyyy-MM-dd HH:mm:\"; @Bean public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer(){ return builder ->{ builder.simpleDateFormat(dateTimeFormat); builder.serializers(new LocalDateSerializer(DateTimeFormatter.ofPattern(dateFormat))); builder.serializers(new LocalDateTimeSerializer(DateTimeFormatter.ofPattern(dateTimeFormat))); }; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/jdk17/jdk17.html":{"url":"java/jdk17/jdk17.html","title":"java17","keywords":"","body":"jdk17 默认关闭了一些包的访问，启动jar包时需要添加 vm options --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED --add-opens java.base/java.lang.invoke=ALL-UNNAMED --add-opens java.base/java.math=ALL-UNNAMED --add-opens java.base/sun.net.util=ALL-UNNAMED --add-opens java.base/java.io=ALL-UNNAMED --add-opens java.base/java.net=ALL-UNNAMED --add-opens java.base/java.nio=ALL-UNNAMED --add-opens java.base/java.security=ALL-UNNAMED --add-opens java.base/java.text=ALL-UNNAMED --add-opens java.base/java.time=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.util.concurrent=ALL-UNNAMED Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/pattern/singleton.html":{"url":"java/pattern/singleton.html","title":"单例模式","keywords":"","body":"单例模式饿汉式懒汉式线程不安全线程安全双重校验锁单例模式 单例设计模式:属于创建型模式,提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类,该类负责创建自己的对象,同时确保只有单个对象被创建。主要解决一个全局使用的类进行频繁的创建于销毁,耗费大量内存的问题。 注意事项:单例类只能有一个实例,单例类必须自己创建自己的实例并可以提供给其他对象使用 饿汉式 这种方式的优点是其为线程安全的,没有加锁,执行效率稍高一些 缺点则是类加载时就初始化,浪费内存(比如有很多类使用的是单例模式,程序在启动时就会创建这些对象,而不管你是否用到了这些类,从而导致程序启动较慢) public class Singleton1 { // 将构造函数私有化,让外部无法轻易的创建其对象(new的方式) private Singleton1(){} // static修饰的变量属于类,随着类的加载而加载,只加载一次 private static Singleton1 instance = new Singleton1(); // 提供一个公开的访问接口,可以提供给其他对象使用 // 由于构造函数是私有化的,所以只能使用static关键字,可以被类直接调用 public static Singleton1 getInstance(){ return instance; } } 懒汉式 懒汉式则是为了解决饿汉式,较为浪费内存的情况,只有在需要的时候才会创建对象,但懒汉式也有其缺点,可能会出现线程安全问题 线程不安全 public class Singleton2 { // 单例模式私有构造函数是必须的 private Singleton2(){} // 同样提供一个类变量 private static Singleton2 instance; // 提供一个公开的访问方法,但其是会出现线程安全问题的 public static Singleton2 getInstance(){ // 假如有两条线程t1和t2,此时t1抢到了执行权,进行判断并且判断实例为null,准备创建对象赋值时,被t2抢断 // t2拿到执行权后顺利的创建完对象,此时t1又抢到了执行权,也去做创建对象并赋值的操作,便出现了创建出不同实例的情况 if (instance == null){ instance = new Singleton2(); } return instance; } } 线程安全 为了解决上面的问题,想到了使用synchronized关键字,将其改造为同步方法 public class Singleton3 { private Singleton3(){} private static Singleton3 instance; // 我们在方法上添加synchronized关键字,使其变为静态同步方法 // synchronized需要一个锁对象,在静态方法上的锁对象是类对象(在非静态方法上的锁对象是this,谁调用这个方法this就指向谁),一个类的类对象也只有一个,符合做锁对象的要求 // 为什么可以确定是类对象,还是因为static关键字,static修饰的方法和变量初始化在创建对象之前,此时还没有this public static synchronized Singleton3 getInstance(){ if (instance == null){ instance = new Singleton3(); } return instance; } } 双重校验锁 上面加了同步方法的懒汉式虽然是线程安全的,但是其执行效率会变慢,因为每次调用该方法都会等待上一个线程的锁,为了解决这个问题,我们可以使用同步代码块 public class Singleton4 { private Singleton4(){} private volatile static Singleton4 instance; public static Singleton4 getInstance(){ // 在第一个懒汉式中得知,在此处可能会出现线程安全问题,所以我们把同步代码块加在这里 // 锁对象也就是当前的类对象,这样虽然是线程安全的,并且比上一个线程安全的懒汉式看起来执行效率会更高 // 但是每一条线程进来都会等待锁进行判断实例的存在,可以再做优化 synchronized (Singleton4.class){ if (instance == null){ instance = new Singleton4(); } } return instance; } } public class Singleton4 { private Singleton4(){} private static Singleton4 instance; public static Singleton4 getInstance(){ // 可以再同步代码块外面再加一层校验,这样其他线程不必等待锁进行判断,也保证了创建对象的安全性 if (instance == null){ synchronized (Singleton4.class){ if (instance == null){ instance = new Singleton4(); } } } return instance; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/pattern/proxy.html":{"url":"java/pattern/proxy.html","title":"代理模式","keywords":"","body":"代理模式静态代理JDK代理CGLIB动态代理总结资料代理模式 我们在学习Spring框架时,了解了其两个最大的优点就是IOC和AOP,其中的AOP用到的就是代理模式(动态代理)。在代理模式中一个类代表另一个类的功能,这种类型的设计模型属于结构型模式。 主要使用场景,当我们需要对对象中的方法进行增强时,为了不侵入原有的代码进行功能扩展,使方法的功能或者说职责更加清晰。 静态代理 该方式需要目标对象和代理对象要实现同一类接口,可以实现在不修改原有代码的基础上进行功能的扩展,但是如果有多个需要代理的类的对象或者接口中有大量方法需要实现,这就会需要创建很多的代理类并且进行实现,代码过于冗余。 // 需要被目标类和代理类实现的接口 public interface UserService { void update(); } // 目标类 public class UserServiceImpl implements UserService{ @Override public void update() { System.out.println(\"update\"); int i = 1/0; } } // 代理类 public class UserServiceProxy implements UserService{ // 在代理类的内部维护一个目标类的对象 private UserService target; public UserServiceProxy(UserService target){ this.target = target; } // 代理类和目标类实现了同一接口,并对方法实现重写 // 在代理类对象的方法中调用目标类对象的方法,从而对其进行增强 // 该方法模仿了一个事务的过程 @Override public void update() { System.out.println(\"start transaction\"); try { target.update(); } catch (Exception e) { System.out.println(\"rollback\"); return; } System.out.println(\"commit\"); } } // console start transaction update rollback JDK代理 jdk动态代理利用了java中的反射(主要涉及的类java.lang.reflect.Proxy),在运行时动态生成交由jvm进行处理,编译完成后不修改配置的话没有实际的class文件。目标类(被代理类)必须需要实现接口。 // 需要被目标类实现的接口 public interface UserService { void save(); void remove(); } // 目标类 public class UserServiceImpl implements UserService{ @Override public void save() { System.out.println(\"save\"); int i = 1/0; } @Override public void remove() { System.out.println(\"remove\"); } } // 创建一个类实现InvocationHandler接口,主要实现其invoke方法 public class MyProxy implements InvocationHandler { private Object target; public MyProxy(Object target){ this.target = target; } /** * @param proxy 方法被调用的代理实例 * @param method 被代理对象的方法 * @param args 方法的参数 * @return 运行结果 * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"start transaction\"); Object result; try { result = method.invoke(target, args); } catch (Exception e) { System.out.println(\"rollback\"); return null; } System.out.println(\"commit\"); return result; } } @Test public void demo() throws Throwable { UserService target = new UserServiceImpl(); MyProxy myProxy = new MyProxy(target); // 使用Proxy和自实现的InvocationHandler接口,返回代理类对象 UserService proxyInstance = (UserService) Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), myProxy); proxyInstance.save(); } CGLIB动态代理 cglib采用了非常底层的字节码技术为一个类创建子类,并在子类中对父类方法进行拦截。由于cglib是通过继承来实现代理的所以目标类不能是final的(被final修饰的类不能被继承),并且不会对类中的final方法进行代理(被final修饰的方法不能被重写) 需要导包 cglib cglib 3.2.5 // 可以不实现接口 public class UserService { public void save(){ System.out.println(\"save\"); } public void remove(){ System.out.println(\"remove\"); } } // 自定义方法拦截类 public class MyMethodInterceptor implements MethodInterceptor { /** * @param o 表示要进行增强的对象 * @param method 表示拦截的方法 * @param objects 数组表示参数列表，基本数据类型需要传入其包装类型，如int-->Integer、long-Long、double-->Double * @param methodProxy 表示对方法的代理，invokeSuper方法表示对被代理对象方法的调用 * @return 执行结果 * @throws Throwable */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\"before\"); // 注意这里是调用 invokeSuper 而不是 invoke，否则死循环，methodProxy.invokesuper执行的是原始类的方法，method.invoke执行的是子类的方法 Object result = methodProxy.invokeSuper(o, objects); System.out.println(\"after\"); return result; } } public class MyTest { public static void main(String[] args) { MyMethodInterceptor myMethodInterceptor = new MyMethodInterceptor(); Enhancer enhancer = new Enhancer(); // 设置超类,即指目标类,cglib是通过继承实现的 enhancer.setSuperclass(UserService.class); // 传入自实现的方法拦截类 enhancer.setCallback(myMethodInterceptor); // 创建对象 UserService proxy = (UserService) enhancer.create(); proxy.save(); } } 还可以创建多个MethodInterceptor实现类结合CallbackFilter,进行选择性(不同的)增强 public class MyMethodInterceptor2 implements MethodInterceptor { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\"我是经过过滤器到达的增强\"); methodProxy.invokeSuper(o,objects); return null; } } public class UserServiceFilter implements CallbackFilter { @Override public int accept(Method method) { if (method.getName().equals(\"remove\")){ // 根据setCallbacks方法中传入的MethodInterceptor的实现类数组的索引 return 1; } return 0; } } public class MyTest { public static void main(String[] args) { MyMethodInterceptor myMethodInterceptor = new MyMethodInterceptor(); MyMethodInterceptor2 myMethodInterceptor2 = new MyMethodInterceptor2(); Enhancer enhancer = new Enhancer(); // 设置超类,即指目标类,cglib是通过继承实现的 enhancer.setSuperclass(UserService.class); // 传入自实现的方法拦截类 enhancer.setCallbacks(new Callback[]{myMethodInterceptor,myMethodInterceptor2}); enhancer.setCallbackFilter(new UserServiceFilter()); // 创建对象 UserService proxy = (UserService) enhancer.create(); proxy.save(); proxy.remove(); } } 总结 静态代理:实现较简单,目标类和代理类需要实现一致的接口(其实就是进行包装一下),有多个目标类需要增强就要创建多个代理类。 jdk动态代理:代理类只需要实现InvocationHandler接口,实现其invoke方法,在其内部对目标类进行调用和增强,目标类必须要实现一个接口。 cglib:解决了jdk动态代理目标类必须实现一个接口的问题,并且cglib更加强大。同时目标类和代理类无需实现接口,但是目标类不能是final的,需要被增强的方法不能是final的。 资料 https://juejin.im/post/5c1ca8df6fb9a049b347f55c#heading-8 https://segmentfault.com/a/1190000011291179 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/thread/多线程入门.html":{"url":"java/thread/多线程入门.html","title":"多线程基础","keywords":"","body":"多线程进程与线程并行和并发jvm是不是多线程的入门案例创建多线程的几种方式继承Thread类实现Runnable接口实现Callable接口结合FutureTask类三种方式的比较线程名称多线程中的异常处理机制线程休眠守护线程线程优先级线程让步线程插入线程中断线程状态图多线程 进程与线程 进程:像QQ、微信、网易云音乐这些软件运行后就是一个进程。进程可以独立运行,进程有自己独立的内存空间,不与其他进程共享数据。一个进程可以包含1~N个线程 线程:线程是进程中的一个单元,一个子任务执行者,线程不能独立运行,必须依赖进程而运行,同一个进程中的线程可以共享数据。 并行和并发 并发:指在同一时间段内多个任务高速频繁切换执行 并行:指在同一时刻(间)内多个任务同时执行 jvm是不是多线程的 public class JvmThread { public static void main(String[] args) { for (int i = 0; i 从结果可以看出jvm至少启动了两个线程,一个是main一个是垃圾回收 入门案例 public class TestQuick { public static void main(String[] args) { // 使用匿名内部类的方式 new Thread(new Runnable() { @Override public void run() { for (int i = 0; i // 部分console main...0 Thread-0...0 main...1 Thread-0...1 Thread-0...2 Thread-0...3 Thread-0...4 Thread-0...5 Thread-0...6 Thread-0...7 main...2 Thread-0...8 main...3 Thread-0...9 创建多线程的几种方式 继承Thread类 public class MyThread extends Thread{ @Override public void run() { System.out.println(\"我是继承Thread类开启线程后执行的任务\"); } public static void main(String[] args) { MyThread myThread = new MyThread(); // 开启一个线程,并不是调用run方法 // 如果调用run方法就是交由main函数去调用,依次执行 myThread.start(); System.out.println(Thread.currentThread().getName()); } } 实现Runnable接口 public class MyRunnable implements Runnable{ @Override public void run() { System.out.println(\"我是实现Runnable接口开启线程后执行的任务\"); } public static void main(String[] args) { // 创建线程任务类 MyRunnable myRunnable = new MyRunnable(); // 将任务类对象传递到Thread的构造中并开启线程 Thread thread = new Thread(myRunnable); thread.start(); System.out.println(Thread.currentThread().getName()); } } 实现Callable接口结合FutureTask类 以上两种方式,开启线程后只能执行任务,拿不到执行任务的返回值,这种方式解决了这个问题 /** * 实现Callable接口,返回代表返回值的类型 */ public class MyCallable implements Callable { @Override public Integer call() throws Exception { System.out.println(\"我是实现Callable接口结合FutureTask开启线程后执行的任务\"); return 666; } public static void main(String[] args) throws Exception { // 创建任务类添加到FutureTask的构造中 MyCallable myCallable = new MyCallable(); FutureTask result = new FutureTask<>(myCallable); // 再将FutureTask添加到Thread的构造中开启线程 Thread thread = new Thread(result); thread.start(); // 打印主线程的名称 System.out.println(Thread.currentThread().getName()); // 获取自定义线程的返回值 System.out.println(result.get()); } } 三种方式的比较 首先这三种方式最后都需要通过Thread来间接或直接开启线程执行线程任务 继承Thread类(也实现了Runnable接口)的方式最简单,并且可以直接调用start()开启线程,但这种方式不宜扩展(也就是java中类的单继承多实现),实现Runnable接口和Callable接口可以解决这个问题 虽然实现Runnable接口可以解决日后有继承类的需求,但是如果我们需要拿到线程执行的任务的返回值,Runnable接口中run方法同样不支持,也便是实现Callable接口的好处,其提供了带返回值的call()方法 线程名称 设置获取线程名称 public class MyThread implements Runnable{ @Override public void run() { System.out.println(Thread.currentThread().getName()+\"===执行任务\"); } public static void main(String[] args) { MyThread myThread = new MyThread(); // 使用构造设置名称 Thread thread = new Thread(myThread,\"自定义线程名称\"); // 使用方法设置名称 // thread.setName(\"123\"); thread.start(); // 获取当前线程的名称 System.out.println(Thread.currentThread().getName()+\"===主线程的名称\"); } } 多线程中的异常处理机制 public class MyThread extends Thread{ @Override public void run() { for (int i = 0; i 从打印结果看,main线程出异常并不会影响自定义线程的执行。 同样自定义中如果有异常也不会影响main线程,可以得出jvm只会终止出异常的线程,并不会影响其他线程的执行 Exception in thread \"main\" 自定义线程==0 自定义线程==1 自定义线程==2 自定义线程==3 自定义线程==4 java.lang.ArithmeticException: / by zero at xyz.taoqz.threadexception.MyThread.main(MyThread.java:21) 线程休眠 调用Thread类的静态方法sleep(毫秒值),休眠只会让出CPU资源但不会让出锁资源 public class MyThread extends Thread{ @Override public void run() { for (int i = 0; i 守护线程 setDaemon(boolean)设置一个线程为守护线程,该线程不会单独执行,当其他非守护线程都执行结束后,自动退出(相当于下象棋,老帅如果死了就结束了,守护线程就是守护老帅的) 效果:其他非守护线程执行结束后,守护线程并不会立即停止,会有一个缓冲期 public class MyDaemonThread { public static void main(String[] args) { Thread t1 = new Thread(){ @Override public void run() { for (int i = 0; i 线程优先级 效果不明显,默认优先级是5 t1.setPriority(1); // t1.setPriority(Thread.MAX_PRIORITY); 10 // t2.setPriority(10); // t2.setPriority(Thread.MIN_PRIORITY); 1 // t2.setPriority(Thread.NORM_PRIORITY); 5 线程让步 yield()让出cpu资源,也就是回到就绪状态(效果不明显,因为让步的线程还有可能被线程调度程序再次选中) Thread.yield(); 线程插入 join(),当前线程暂停,等待指定的线程执行结束后,当前线程再继续 join(int),等待指定的毫秒后继续 public class MyThreadJoin { public static void main(String[] args) { Thread t1 = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i 线程中断 interrupt():可以在t2线程中使用t1线程对象调用该方法,通知t1停止,t1可以调用isInterrupted()判断状态,如果为true,便接到了通知,可以选择中断或者继续执行。如果该线程处于终止状态或者被中断过,那么调用isInterrupted()依然返回false。 当t2通知t1终止,但t1本身调用wait、join、sleep等阻塞方法的时候,就会抛出一个异常,同时会清除被标记的死亡状态(也就是isInterrupted的值改为false),继续执行,当然也可以选择捕捉后终止运行 java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at xyz.taoqz.threadinterrupt.MyInterrupt2$1.run(MyInterrupt2.java:17) at java.lang.Thread.run(Thread.java:748) public class MyThreadInterrupt { public static void main(String[] args) { Thread t1 = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i 线程状态图 可以使用getState()获取线程状态 NEW : 尚未启动的线程的线程状态 RUNNABLE : 可运行线程的线程状态 BLOCKED : 线程阻塞（等待锁，或者在同步代码块，同步方法中） WAITING : 等待线程的线程状态（处于等待状态的线程正在等待另一个线程执行特定操作，例如在对象上调用Object.wait()的线程在等待另一个线程使用该对象调用Object.notify()或notifyAll(),调用Thread.join()的线程在等待指定线程终止） TIMED_WAITING : 具有指定等待时间的等待线程的线程状态（调用Thread.sleep,Object.wait(timeout),Thread.join(timeout)） TERMINATED : 线程已完成执行(终止状态) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/thread/线程同步和通信.html":{"url":"java/thread/线程同步和通信.html","title":"线程同步和通信","keywords":"","body":"线程安全问题线程同步死锁等待唤醒机制Lock锁总结同步锁sleep()和wait()方法的区别为什么wait(),notify()和notifyAll()都定义在Object类中线程状态图线程安全问题 多个线程并发执行时其实是我们的CPU在多个线程之间高速切换执行,以至于让我们认为是在同时执行。Java使用的是抢占式的调度模式,即哪个线程抢到了CPU资源就会执行,并不会等待其他线程执行完毕,这就会引发线程安全问题,当多线程共同操作一份数据时,当t1执行到一半时被t2抢去CPU执行权并且将变量修改成t1不想要的数据。 使用经典的卖票案例演示线程安全问题 public class MyTest { public static void main(String[] args) { MyTicket myTicket = new MyTicket(); Thread t1 = new Thread(myTicket, \"窗口一\"); Thread t2 = new Thread(myTicket, \"窗口二\"); Thread t3 = new Thread(myTicket, \"窗口三\"); t1.start(); t2.start(); t3.start(); } static class MyTicket implements Runnable{ // 总票数 也就是共享变量 private int ticket = 100; // 用来记录窗口(线程)卖出的总票数 private Vector vector = new Vector<>(); @Override public void run() { while (true){ if (ticket > 0){ // 为了体现安全问题,休眠1秒 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"卖出了第\"+ticket--+\"张票\"); vector.add(ticket); }else { System.out.println(Thread.currentThread().getName()+vector.size()); return; } } } } } 窗口一卖出了第100张票 窗口三卖出了第99张票 窗口二卖出了第100张票 窗口一卖出了第98张票 窗口二卖出了第96张票 窗口三卖出了第97张票 窗口三卖出了第95张票 窗口二卖出了第94张票 窗口一卖出了第94张票 ....... 窗口一卖出了第0张票 窗口一124 窗口三卖出了第-1张票 窗口三125 窗口二卖出了第1张票 窗口二126 从这里可以看出的确有线程问题,会出现卖同一张票的还有卖出0和负票数的,可以使用我们线程同步解决这些问题。 线程同步 Java中使用synchronized关键字解决多个线程操作同一份数据时引发的安全问题,也就是同步代码块,同步代码块必须要有一个锁对象,并且如果想要多个线程之间共享数据不会出现安全问题时,要保证锁对象一致。大概原理就是t1拿到锁便开始执行,t2此时进入阻塞状态等待t1释放锁,等t1执行完毕后自动释放锁,此时t2便开始执行,循环... synchronized又分为同步代码块和同步方法 同步代码块 synchronized (锁对象){ 被同步的代码 } 同步方法 此时的锁对象为this public synchronized void fun(){ // 线程任务 } 此时的锁对象为类的class对象,也就是类名.class public static synchronized void fun(){ // 线程任务 } 卖票案例可改为 // 如果加在run方法上,某条线程拿到执行权直到执行完毕才会释放锁,其他线程也就没有意义了,当然也可以将同步代码块中的代码放到一个方法中,将方法设置为同步方法,在run方法中进行调用这个方法 @Override public void run() { while (true) { // 这里的锁对象使用的this当前对象,因为我们实现的runnable接口,只创建了一个对象开启的几个线程,也可在成员变量位置创建对象作为锁 synchronized (this) { // 在这里会出现问题,也就是会出现卖0,负数票的 // 比如窗口三抢到了CPU判断有1张票,准备去卖票,被窗口一抢去了,此时判断还有一张准备去卖时又被窗口二抢去 // 此时窗口二顺利的卖出了最后一张票,窗口一再抢到后直接卖票便出现了第0张,窗口三则出现了卖负数票的情况 // 所以选择在此处加同步代码块,保证任意线程在判断成功后可以顺利的卖出一张票 if (ticket > 0) { // 为了体现安全问题,休眠1秒 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \"卖出了第\" + ticket-- + \"张票\"); vector.add(ticket); } else { System.out.println(Thread.currentThread().getName() + vector.size()); return; } } } } 死锁 线程死锁是指两个或两个以上的线程互相持有对方所需要的资源,也就是锁嵌套,而synchronized特点是一个线程获得锁,在该线程未释放锁的情况下,其他线程获取不到这个锁会一直等待下去,造成无限的等待因此便造成了死锁 线程1拿着线程2需要的锁,线程2拿着线程1需要的锁 public class DieLock { public static void main(String[] args) { Object lock1 = new Object(); Object lock2 = new Object(); new Thread(new Runnable() { @Override public void run() { synchronized (lock1){ System.out.println(Thread.currentThread().getName()+\"获取到锁1\"); // 放弃cpu资源 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (lock2){ System.out.println(Thread.currentThread().getName()+\"获取到锁2\"); } } } },\"线程1\").start(); new Thread(new Runnable() { @Override public void run() { synchronized (lock2){ System.out.println(Thread.currentThread().getName()+\"获取到锁2\"); // 放弃cpu资源 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (lock1){ System.out.println(Thread.currentThread().getName()+\"获取到锁1\"); } } } },\"线程2\").start(); } } // console 线程1获取到锁1 线程2获取到锁2 等待(产生死锁) 等待唤醒机制 多个线程在处理同一个资源,但是处理的动作(线层的任务)却不相同。通过一定的手段使各个线程能有效的利用资源(线程之间的通信)。而这种手段即等待唤醒机制。 涉及方法: wait():令当前线程挂起并放弃CPU,同步资源下使别的线程可访问并修改共享数据,当前线程可在被唤醒后排队等待再次对资源的访问 notify():唤醒,随机唤醒一个正在排队等待同步资源的线程 notifyAll():唤醒全部,唤醒正在排队等待资源的所有线程，结束等待 唤醒也就是让线程获得执行资格,这些方法必须在同步中才有效,同时在使用时需要标明锁对象,都属于Object类中的方法,这些方法在使用时都需要标明锁,而锁又可以是任意对象,能被任意对象调用的方法在Object类中。 交替打印案例 public class AfterNateTest { // 设置一个标记 用于确定线程是否需要等待 private static boolean flag = false; // 锁对象 private static Object lock = new Object(); public static void main(String[] args) { // 两个原数组,设置长度不一致 String[] chars = {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}; Integer[] arr = {1, 2, 3, 4, 5, 6, 7, 8, 9}; // 开启两个线程传入不同的数组 new Thread(new Runnable() { @Override public void run() { print(chars, lock); } }).start(); new Thread(new Runnable() { @Override public void run() { print(arr, lock); } }).start(); } public static void print(Object[] array, Object lock) { for (int i = 0; i 效果 Thread-0===a Thread-1===1 Thread-0===b Thread-1===2 Thread-0===c Thread-1===3 Thread-0===d Thread-1===4 Thread-0===e Thread-1===5 Thread-0===f Thread-1===6 Thread-0===g [a, b, c, d, e, f, g] Thread-1===7 Thread-1===8 Thread-1===9 [1, 2, 3, 4, 5, 6, 7, 8, 9] 调用wait()会立即进入等待状态并释放锁资源,notify()/notifyAll()可以唤醒其他等待的线程,但是需要执行完所在的synchronized代码块中的代码 public class MyTest2 { public static void main(String[] args) { Object lock = new Object(); new Thread(new Runnable() { @Override public void run() { System.out.println(\"Thread1 start\"); synchronized (lock){ System.out.println(\"Thread1 wating\"); try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"Thread1 continue\"); System.out.println(\"Thread1 over\"); } } }).start(); new Thread(new Runnable() { @Override public void run() { System.out.println(\"Thread2 start\"); synchronized (lock){ lock.notify(); System.out.println(\"Thread2 running\"); try { System.out.println(\"Thread2 sleep\"); Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"Thread2 over\"); } } }).start(); } } // console Thread1 start Thread1 wating Thread2 start Thread2 running Thread2 sleep Thread2 over Thread1 continue Thread1 over Lock锁 java.util.concurrent.locks.Lock 机制提供了比synchronized代码块和synchronized方法更广泛的锁定操作, 同步代码块/同步方法具有的功能Lock都有,除此之外更强大,更体现面向对象。 Lock锁也称同步锁，加锁与释放锁方法化了，如下： // 加同步锁 public void lock() // 释放同步锁 public void unlock() public class MyTest3 implements Runnable{ private int ticket = 100; private Lock lock = new ReentrantLock(); @Override public void run() { // 卖票窗口一直开启 while (true){ // 上锁 lock.lock(); if (ticket > 0){ try { // 模拟出票时间 Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"==\"+ticket--); } // 释放锁 lock.unlock(); } } public static void main(String[] args) { MyTest3 myTest3 = new MyTest3(); new Thread(myTest3).start(); new Thread(myTest3).start(); } } 总结 同步锁 多个线程想保证线程安全,必须要使用同一个锁对象(可以是任意对象) 非静态同步方法的锁对象是this,静态同步方法的锁对象是类.class sleep()和wait()方法的区别 sleep:不释放锁(即使是在同步区域内),只释放cpu资源,在休眠时间内,不能唤醒,可以写在任意位置 wait:释放锁对象并释放cpu执行权,在等待的时间内,能唤醒,只能存在与同步中 为什么wait(),notify()和notifyAll()都定义在Object类中 这样锁对象便可以是任意对象(类都默认继承自Object) 线程状态图 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/thread/线程池.html":{"url":"java/thread/线程池.html","title":"线程池","keywords":"","body":"线程池使用实现Runnable接口方式实现Callable接口的方式线程状态图线程池 ​ 线程池,其实就是一个容纳多个线程的容器,其中的线程可以反复使用,避免频繁创建线程对象和线程而消耗过多的资源。 使用 通常线程池都是通过线程池工厂创建,在调用线程池中的方法获取线程,执行任务方法 主要涉及的类和方法 Executors:线程池创建工厂类 ExecutorService:线程池类 主要用两个互为重载的方法,可以分别传入Callable和Runnable接口的子类,并开启线程,调用任务方法(不能直接放入继承Thread的类) // 可获取返回值 Future submit(Callable task); // Thread类也是Runnable接口的子类 Future submit(Runnable task); 实现Runnable接口方式 public class MyRunnable implements Runnable{ @Override public void run() { System.out.println(\"拿到线程池中的线程\"); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"线程名称:\" +Thread.currentThread().getName()); System.out.println(\"完成任务,归还线程\"); } } public class MyTest1 { public static void main(String[] args) { // 根据线程池工厂创建线程池并初始化线程数量 ExecutorService service = Executors.newFixedThreadPool(2); MyRunnable runnable = new MyRunnable(); // 传入线程任务类,使用并开始线程池中的线程,如果开启数量大于线程池初始创建的数量 // 需要等待其他线程完成回到线程池中后再进行调用 service.submit(runnable); service.submit(runnable); service.submit(runnable); // 关闭连接 service.shutdown(); } } 实现Callable接口的方式 public class MyCallable implements Callable { private Integer num1; private Integer num2; public MyCallable(Integer num1, Integer num2) { this.num1 = num1; this.num2 = num2; } @Override public Integer call() throws Exception { System.out.println(Thread.currentThread().getName()); return num1+num2; } } public class MyTest2 { public static void main(String[] args) throws ExecutionException, InterruptedException { // 根据线程池工厂创建线程池并初始化线程数量 ExecutorService service = Executors.newFixedThreadPool(2); // 创建线程任务类 MyCallable myCallable1 = new MyCallable(1,2); MyCallable myCallable2 = new MyCallable(5,2); // 传入线程任务类,使用并开始线程池中的线程, Future submit = service.submit(myCallable1); Future submit1 = service.submit(myCallable2); // 获取返回值 System.out.println(submit.get()); System.out.println(submit1.get()); System.out.println(service); service.shutdown(); } } // console pool-1-thread-1 pool-1-thread-2 3 7 java.util.concurrent.ThreadPoolExecutor@29453f44[Running, pool size = 2, active threads = 0, queued tasks = 0, completed tasks = 2] 线程状态图 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"java/thread/threadpool/threadpool.html":{"url":"java/thread/threadpool/threadpool.html","title":"Threadpool","keywords":"","body":"线程池该怎样创建线程池?newFixedThreadPoolnewCachedThreadPool手动创建线程池线程池默认行为线程池 该怎样创建线程池? 应该手动new ThreadPoolExecutor创建线程池 newFixedThreadPool和newCachedThreadPool容易造成OOM,内存溢出 newFixedThreadPool newFixedThreadPool可以控制最大线程数量,但是LinkedBlockingQueue是一个Integer.MAX_VALUE长度的队列,可以认为是无界的,如果任务较多并且执行较慢时,队列很可能会快速积压,撑爆内存导致OOM newCachedThreadPool newCachedThreadPool的最大线程数量是Integer.MAX_VALUE,可以认为是没有上限的,而其工作队列是一个SynchronousQueue,是一个没有存储空间的阻塞队列,这意味着,只要有请求来,就必须找到一条工作线程处理,如果当前没有空闲线程会创建一条新的,这意味着,如果任务需要长时间执行,那么大量任务进来后会创建大量的线程,从而导致OOM 手动创建线程池 创建线程没有固定的公式,任何时候都是根据实际情况而定 当任务执行较慢时(计算或有IO操作),可以将线程数量适当调大一点 当任务执行较快但是任务数量较多时,可以将线程数适当放小,将工作队列数量放大 // 核心线程数: 可以设置线程启动后是否立刻创建 int corePoolSize = 2; // 最大线程数: 工作队列满时会扩容线程,该参数控制池中允许的最大线程数 int maximumPoolSize = 3; // 存活时间: 当线程数大于内核数时，这是多余的空闲线程将在终止之前等待新任务的最长时间 long keepAliveTime = 10; // keepAliveTime参数的时间单位 TimeUnit unit = TimeUnit.SECONDS; // 用于在执行任务之前保留任务的队列。此队列将仅保存execute方法提交的Runnable任务。 BlockingQueue workQueue = new ArrayBlockingQueue<>(2); // 执行程序创建新线程时要使用的工厂 (自定义)实现ThreadFactory接口 ThreadFactory threadFactory = new NameTreadFactory(); // 因达到线程边界和队列容量而被阻止执行时使用的处理程序,默认抛出 (自定义)实现接口RejectedExecutionHandler RejectedExecutionHandler handler = new MyIgnorePolicy(); ThreadPoolExecutor executor = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, handler); // 使用默认的线程工厂 new DefaultThreadFactory(); // 和默认的拒绝策略 RejectedExecutionHandler defaultHandler = new AbortPolicy(); // ThreadPoolExecutor executor = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, // workQueue); // corePoolSize 核心线程默认是不会回收的(即使是空闲状态),通过该方法让核心线程也受到keepAliveTime限制 // executor.allowCoreThreadTimeOut(true); // 核心线程默认不会在线程池创建时创建,可以通过该预启动所有核心线程 executor.prestartAllCoreThreads(); // executor.execute(); // 必须指定容器大小,且 > 0 ArrayBlockingQueue list = new ArrayBlockingQueue(3); list.add(1); // list.add(2); // list.add(3); // 队列满时,阻塞 // list.put(4); // 队列已满时,添加报错 // list.add(4); // 队列已满时,不添加不报错 // list.offer(4); // 队列为空时阻塞,等待消费 // list.take(); // 返回队列头位置元素 // list.peek(); // 返回队列头部元素,并将元素从集合中移除 // list.poll(); System.out.println(list); 线程池默认行为 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"mysql/mysql基础.html":{"url":"mysql/mysql基础.html","title":"mysql基础","keywords":"","body":"数据库什么是数据库?什么是表、行、列?以及它们的关系?简单了解mysqlmysql中sql分类什么是SQL?DDL:数据库定义语言DML:数据库操作语言DQL:数据库查询语言DCL:数据库控制语言MySQL数据类型整数类型日期类型字符串、二进制文本类型使用数据处理函数数据库三范式数据库 什么是数据库? 数据库就是存储数据的仓库,数据库按照特定的格式将数据存储在磁盘文件中,用户可以对数据库中的数据进行增删改查操作 什么是表、行、列?以及它们的关系? 表是某种特定数据类型的结构化清单,如果把这张表想象为一张网格,网格中垂直的列为表列,水平则为表行。假如现在有一张顾客表,每一行则是一个顾客的信息,比如手机号,住址等,而列则是所有顾客的某种信息,比如所有的名称 简单了解mysql mysql也是一种DBMS(数据库管理系统),数据库软件。同时mysql拥有最多的使用者,这得益于它的许多优点,开源,性能好,支持事务,可信赖,简单使用等。 mysql在使用时默认是不分大小写的 登录 # 这几个参数分别为用户名 密码 ip 端口 mysql -uroot -p123 -hlocalhost -P3306 在输入sql命令时必须以;(分号)或者\\g作为结束 # 切换到指定库 use databaseName; #查看该库下所有表 show tables; # 查看表结构 show columns from tableName; # 查看表结构 describe book; # 更改字符集 set names 字符编码; 虽然在安装时指定了数据的编码为utf-8,但是windows默认编码为gbk,使用 set names gbk;便可看中文数据 set names gbk; --等同于下面三条,,将client,connection,result的编码一致 set character_set_client=gbk; set character_set_connection=gbk; set character_set_results=gbk; 查看mysql内部设置的编码 show variables like 'character%'; 通过查看表结构得到 Field:字段名称 Type:数据类型 Null:是否允许为null Key:键信息 Default:默认值 Extra:其他信息(auto_increment自增) 前面说到表中应该至少有一个主键保证每行数据的唯一性,但是如果我们手动给主键值,太麻烦也不太显示,auto_increment自增则可以解决这个问题,每新添加一行数据都会自动分配一个可用编号 mysql中sql分类 什么是SQL? sql是专门用来和数据通信的语言,也就是说可以用sql来操纵数据,但是sql并不是某个特定厂商专有的语言,不过大部分的关系型数据库都支持sql,不过不完全相同 DDL:数据库定义语言 用来定义数据库对象,也就是创建、修改、删除,数据库、表、列等 关键字:create alter drop truncate等 操作对象:数据库,表 特点:负责\"骨架\"操作,不操作数据 DDL之数据库常用操作 # 创建数据库,使用安装时指定的默认编码 create database name; # 指定字符编码创建 create database name character set utf-8(字符) # 查看所有数据库 show database; # 查看数据库的定义信息 show create database name; # 切换使用指定数据库 use databaseName; # 删除数据库 drop database name; DDL之表操作 常用数据类型 类型 用途 int 表示整数 float/double 表示小数 date/datetime 表示时间 char/varchar 表示字符串 # 创建表 create table 表名( 字段名 类型(长度) [约束], 字段名 类型(长度) [约束] ... ) # 查看数据库中所有表 show tables; # 查看表结构 desc 表名; # 修改表结构 # 添加列 alter table 表名 add 列名 类型(长度); # 修改表列类型长度及约束 alter table 表名 modify 表名 类型(长度); # 修改列名 alter table 表名 change 旧列名 新列名 类型(长度) [约束]; # 删除列 alter table 表名 drop 列名; # 修改表名 alter table 表名 rename to 新表名; # 修改表的字符集 alter table 表名 character set 字符集; # 删除表 drop table 表名; DML:数据库操作语言 用来对数据库中表的记录进行更新(插入/修改/删除数据也会涉及到DQL) 关键字:insert update delete等 操作对象:行数据 常用操作 插入 注意事项 值与字段必须对应,个数相同,类型相同 值的数据大小必须在字段的长度范围内 除了数值类型外,其他的字段类型的值必须使用引号括起(建议单引号) 如果插入空值,可以不写字段,或者插入null #向表中插入某些字段 insert into 表(字段1,字段2,字段3...) values(值1,值2,值3); #向表中插入所有字段,字段的顺序为创建表时的顺序 insert into 表 values(值1,值2,值3...) 更新 注意事项 列名的类型与修改的值要一致 修改值的时候不能超过最大长度 除了数值类型类型外,其他的字段类型的值必须使用引号括起 -- 更新所有记录的指定字段 update 表名 set 字段名=值,字段名=值,...; -- 更新符号条件记录的指定字段 update 表名 set 字段名=值,字段名=值,...where 条件; 删除 delete from 表名 [where 条件]; truncate table 表名; 两者的区别 两种方式都可删除表中所有记录 delete一条一条删除,不清空auto_increment记录数,效率低,可回滚; truncate直接将表删除,重新创建表,会重置auto_increment的值,重新开始,效率高,不可回滚 DQL:数据库查询语言 用来查询数据库中表的记录 关键字:select from where等 select [distinct] *| 列名,列名 from 表 where 条件 别名查询 使用关键字as,可省略 表别名:select * from 表名 [as] 别名; 列别名:select 列名 [as] 别名 from 表名; 排序 通过order by语句可以将查询后的结果进行排序,放置在select语句的最后 select * from 表名 order by 排序字段 asc|desc; asc 升序(默认) desc 降序 聚合 之前做的查询都是横向的,都是根据条件进行一行一行的判断,而是用聚合函数查询是纵向查询,对一列值的计算,然后返回一个单一的值,聚合函数会忽略空值 count(字段):统计指定列不为null的记录行数 sum(字段):计算指定列的数值和,如果指定列类型不是数值类型,那么计算结果为0 max(字段):计算指定列的最大值,如果指定列是字符串类型,那么使用字符串排序运算 min(字段):计算指定列的最小值,如果指定列是字符串类型,那么使用字符串排序运算 max(avg):计算指定列平均值,如果指定列不是数值类型,那么计算结果为0 分组 使用group by字句对查询信息进行分组,其中分组字段可写多个 select 字段1,字段2... from 表名 group by 分组字段 having 分组条件; having用于对分组后的数据进行过滤,与where类似 having与where的区别 where是在分组前对数据进行过滤,主要针对行,where后面不可使用分组函数(统计函数) having是在分组后对数据进行过滤,主要针对列,having后面可以使用分组函数 分页 在数据量大的情况下,需要对数据进行分页显示 select 字段1,字段2... from 表名 limit m,n; m:整数,代表从第几条索引!索引!索引!(0)开始,计算方式(当前页-1)*每页显示条数 n:整数,代表查询多少条数据 例如 select 子段1 from 表名 limit 0,5; 代表第一页,一页5条 条件查询 执行顺序 书写顺序 select 字段 from 表名 where 条件 group by 字段 having 条件 order by 字段 执行顺序 1.from 表名 2.where 条件 3.group by 字段 字段值相同的数据会划分成一组 4.having 条件 将每组分别进行一次运算 5.select 字段 把每组中的第一条数据取出来,合并成一张新伪表,展示指定字段 6.order by 字段 对新伪表进行最后的排序 DCL:数据库控制语言 用来进行用户管理、权限管理 MySQL数据类型 整数类型 在涉及金额时建议使用decimal(9,2)类型,前面的9代表整数的长度(9-2),后面的2代表小数的长度,,不够两位补0,超出小数点的范围进行四舍五入 日期类型 TIMESTAMP:时间戳,可以在java8新日期时间API中了解 字符串、二进制文本类型 使用数据处理函数 文本处理函数 日期时间处理函数 数值处理函数 重要的加减日期函数 select subdate(now(),interval '3 5' day_hour ); # 30天内的信息 select * from Student where bir >= now() and bir # 根据指定日期格式化 select date_format(now(),'%Y/%m/%d %H:%i:%s'); 格式 描述 %a 缩写星期名 %b 缩写月名 %c 月，数值 %D 带有英文前缀的月中的天 %d 月的天，数值(00-31) %e 月的天，数值(0-31) %f 微秒 %H 小时 (00-23) %h 小时 (01-12) %I 小时 (01-12) %i 分钟，数值(00-59) %j 年的天 (001-366) %k 小时 (0-23) %l 小时 (1-12) %M 月名 %m 月，数值(00-12) %p AM 或 PM %r 时间，12-小时（hh:mm:ss AM 或 PM） %S 秒(00-59) %s 秒(00-59) %T 时间, 24-小时 (hh:mm:ss) %U 周 (00-53) 星期日是一周的第一天 %u 周 (00-53) 星期一是一周的第一天 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 %W 星期名 %w 周的天 （0=星期日, 6=星期六） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 %Y 年，4 位 %y 年，2 位 数据库三范式 什么是范式 设计关系数据库时，遵从不同的规范要求，设计出合理的关系型数据库，这些不同的规范要求被称为不同的范式，各种范式呈递次规范，越高的范式数据库冗余越小。 第一范式：数据库表中不能出现重复记录,每一列都是不可分割的原子数据项,同一列不能有多个值 例如：电话号码 - > 手机号 座机号 第二范式：一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。只要数据列中出现数据重复，就要把表拆分开来。(要求所有非主键字段完全依赖主键,不能产生部分依赖) 比如: 根据第一范式,表中记录不能重复,且需要主键,所以设置了联合主键有如下一张表 ,但是其中学生姓名依赖学生编号,教师姓名依赖教师编号,产生了部分依赖,所以需要将其拆分成两张表,另外还需新建一张表用来存储对应关系,经典的多对多关系 第三范式：在2NF基础上,非主键字段不能传递依赖于主键字段 例如这张表中确定主键学生编号,学生姓名依赖学生编号,班级名称依赖于班级编号,而班级编号依赖与学生编号,此时便产生了传递依赖的情况 需要将其拆分成两张表,一张学生表,一张班级表,在学生表中创建外键指向班级表中的主键班级编号,一对多 https://www.jianshu.com/p/3e97c2a1687b Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/mysql进阶.html":{"url":"mysql/mysql进阶.html","title":"mysql进阶","keywords":"","body":"约束主键约束主键的规则添加主键自动增长列注意事项联合主键非空约束唯一约束默认约束外键约束多表关系关联查询备份还原数据库约束 添加表约束的作用,是为了保证表中记录的有效以及完整性 约束分类 实体完整性:数据行约束,主键约束,唯一约束 域完整性:数据类型,默认约束,非空约束 引用完整性:外键约束 主键约束 表中的每行数据都应该有可以标识自己唯一的列。假如没有唯一的列或者说主键,更新或删除表中的特定行会很困难,没有安全的办法保证只涉及相关的行 主键的规则 primary key 每行都必须具有一个主键值 主键列不能为null,主键值必须唯一 添加主键 方式一:在创建表时,在字段后声明指定字段为主键 create table person( id int primary key , name varchar(5) ) 方式二:创建表时,在约束区域,声明指定字段为主键 create table person( id int , name varchar(5), primary key (id) ) 方式三:创建表后,修改表结构,指定字段为主键 create table person( id int , name varchar(5) ); alter table person add primary key (id); 删除主键:使用修改表结构的语句删除主键 alter table person drop primary key; 自动增长列 前面说主键的值在每一行都不能重复,从而保障数据的唯一性,如果我们手动去维护这个值将会很困难,也不现实。所以mysql为我们实现了这一点。 自增长列类型必须是整数型,并且一般为主键 添加自增长列 create table person( id int primary key auto_increment, name varchar(5) ); 设置自增后,在插入数据时,可以不为该列添加值,或者设置为null值 auto_increment默认从1开始,也可以修改起始值 alter table person auto_increment = 100; 注意事项 不更新主键列的值,不重复用主键列的值,不要使用可能会修改的值作为主键,虽然不是必须,但是尽量遵守 联合主键 联合主键也仅是一个主键 在复杂的情况下可以使用多个列作为主键,同样要求其字段值不能为null,而且不能重复(联合主键的多个列的值都相同) create table person( id int , name varchar(5), idCard varchar(50), primary key (id,name) ); 非空约束 非空约束not null强制列不接受null值,也就意味着使用非空约束的列始终包含值 create table person( id int not null , name varchar(5) not null , idCard varchar(50) ); 修改表结构 alter table person modify name varchar(5) not null ; 删除非空约束 alter table person modify name varchar(5); 唯一约束 unique约束唯一标识数据库表中的每条记录 unique和primary key约束都保证了列值的唯一性,primary key自动拥有unique约束 unique和primary key的区别,每个表中可以有多个唯一约束,但是主键只能有一个,唯一约束允许值为null(多个),而primary key则较为严格不允许值为null 添加唯一约束 方式一 create table person( id int , name varchar(5) unique , idCard varchar(50) ); 方式二 create table person( id int , name varchar(5) , idCard varchar(50), unique (name,idCard) ); 方式三 alter table person add unique (name); 默认约束 默认约束的作用是在添加数据时如果不指定值使用默认值 create table person( id int , name varchar(5) default 'zs', idCard varchar(50) ); alter table person modify name varchar(5) default 'zz'; 删除默认约束 alter table person modify name varchar(5); 外键约束 根据数据库的三范式中的第二范式,一张表中只能存储一种数据,如果有多种类型的数据需要拆分表 拆分表后需要建立关系,所以就有了外键,又分为主表和从表,在从表中建立外键指向主表的主键,从表外键的数据类型和长度必须和主表中一致 可以进行手动维护外键,不必建立强一致性的外键 多表关系 一对多:比如员工表和部门表,一个部门中有多个员工,而一个员工只属于一个部门,此时的部门便是一方,员工便是多方 多对多:比如学生表和课程表,一个学生可以选修多门课程,一门课程下又有多名学生,此时变为多对多的关系,需要建立一张中间表用来存储之前的关系 一对一:使用较少,因为一对一其实也可合成一张表 关联查询 两张表一张员工表,一张部门表,员工表中手动维护部门id CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `password` varchar(50) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(255) DEFAULT NULL, `birthday` date DEFAULT NULL, `email` varchar(50) DEFAULT NULL, `phone` varchar(50) DEFAULT NULL, `dept_id` int(11) DEFAULT NULL, `position_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 CREATE TABLE `dept` ( `id` int(11) NOT NULL AUTO_INCREMENT, `dept_name` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8 笛卡尔积 也叫交叉连接 # 会产生笛卡尔积,将所有的记录全部匹配一遍(也就是两张记录的乘积) select * from emp,dept; #符合条件的 select * from emp,dept where emp.dept_id = dept.id; 内连接 inner join # 两张表中同时符合条件的记录,才会显示内容 select * from emp,dept where emp.dept_id = dept.id; # 显示内连接 可以使用where和on进行条件筛选 select * from emp inner join dept where emp.dept_id = dept.id; select * from emp inner join dept on emp.dept_id = dept.id; select * from emp join dept on emp.dept_id = dept.id; 外连接 左外连接 left outer join = left join 左连接的结果集包括左表中的所有行,并且如果在匹配条件中左表的某行在右表中没有匹配行,则相关右表的列值都为null # 这条sql语句会报错,不能使用where进行条件过滤 select * from emp e left join dept d where e.dept_id = d.id; # 以下两条sql的执行结果是一致的 as name 是为表起别名(当关联查询时,表多的情况下可以选择使用),也可以省略as 关键字和outer关键字 select * from emp as e left outer join dept as d on e.dept_id = d.id; select * from emp e left join dept d on e.dept_id = d.id; 右外连接 right outer join = right join 右连接的结果集包括右表中所有的行,并且如果在匹配条件中右表的某行在左表中没有匹配行,则相关左表的列值都为null # 这条sql语句会报错,不能使用where进行条件过滤 select * from emp e right join dept d where e.dept_id = d.id; # 以下两条sql的执行结果是一致的 as name 是为表起别名(当关联查询时,表多的情况下可以选择使用),也可以省略as 关键字和outer关键字 select * from emp as e right outer join dept as d on e.dept_id = d.id; select * from emp e right join dept d on e.dept_id = d.id; union,union all 将多个select语句的结果组合到一个结果集中 # 效果是左连接和右连接的组合 select * from emp left join dept on emp.dept_id = dept.id union all select * from emp right join dept on emp.dept_id = dept.id; # 效果是先将两张表进行内连接,然后剩下的结果集分别是两张表中没有匹配的行 # 也就是达到了union all 去重的效果 select * from emp left join dept on emp.dept_id = dept.id union select * from emp right join dept on emp.dept_id = dept.id; 备份还原数据库 # 备份单个库 mysqldump -uroot -p --databases 数据库名 > d:\\pms.sql(文件保存位置) # 备份整个库 mysqldump -h 127.0.0.1 -P 3306 -u root --default-character-set=gbk -p --all-databases > d:\\dumpfile.sql # 还原数据库 mysql -uroot -p yuan Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/mysql事务.html":{"url":"mysql/mysql事务.html","title":"mysql事务","keywords":"","body":"事务ACIDmysql事务操作并发访问问题事务隔离级别示例读未提交读已提交可重复读串行化事务 事务指的是逻辑上的一组操作,组成这组操作的各个单元要么全部成功,要么全部失败 事务的作用:保证一个事务在多次操作中,要么全部成功,要么全部失败(当程序出现异常时,可以将所在事务中的数据恢复到原来的状态) mysql中并非所有的存储引擎都支持事务,其中MyISAM和InnoDB是最为常用的两种引擎,而前者不支持事务后者支持,同时这也是mysql将其定为默认的存储引擎的重要原因 ACID InnoDB存储引擎中的事务完全支持ACID特性 原子性(Atomicity):原子性是指事务是一个不可分割的工作单位,事务中的操作,要么全部发生,要么不发生 一致性(consistency):事务前后的数据必须保持一致 隔离性(isolation):指在多个用户访问数据库时,一个用户的事务不能被其他用户的事务所干扰,多个并发事务之间数据要相互 隔离(即该事务提交前对其他事务都不可见,通常使用锁来实现) 持久性(durability):事务一旦提交,其结果就是永久性的,即使数据库发生故障崩溃而需要恢复时,也能保证提交后的数据都不会丢失(保证了应用的高可靠性,而并非高可用) mysql事务操作 sql语句 描述 start transaction; 开启事务 commit; 提交事务 rollback; 回滚事务 创建一张表 CREATE TABLE `account` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `money` double DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 insert into account(id,name,money) values (null,'张三',1000); insert into account(id,name,money) values (null,'李四',1000); mysql中有两种方式进行事务的管理 ​ 自动提交:mysql默认是自动提交的,及执行一条sql语句提交一次事务 ​ 手动提交:先开启事务,再提交 # 查看当前的提交方法,是否自动提交 0=OFF(否) 1=ON(是) show variables like '%commit%'; # 设置自动提交的参数为OFF set autocommit = 0; 假如现在有一个转账的需求 此时设置的是自动提交,将两条sql执行后会持久化到数据库中 update account set money = money - 100 where name = '张三'; update account set money = money + 100 where name = '李四'; 现在把自动提交改为手动提交 show variables like '%commit%'; set autocommit = 0; show variables like '%commit%'; # 开启事务并进行转账 start transaction ; update account set money = money - 100 where name = '张三'; update account set money = money + 100 where name = '李四'; 你会发现在数据库中没有做任何的改变,因为数据库还没有得到指令是提交还是回滚 # 回滚 rollback ; # 提交 commit ; 需要注意的是,在提交后是无法进行回滚的,因为数据已经磁盘中做了持久化,同样的回滚后,再次提交也是没有意义的,可以把这两条语句当做是事务的结束语句 并发访问问题 如果不考虑隔离性,事务存在几种并发访问问题 1.脏读:一个事务读到了另一个事务中未提交的数据。 例如: 当一个事务访问数据库并对数据进行了修改,但该事务还没有结束,也就是还没有完成提交。此时另一个事务中读取到了这条并未提交的数据,这条数据便是脏数据,依据脏数据所做的操作可能是不正确的 2.丢失数据:指在一个事务读取一个数据时,另外一个事务也访问了这个数据,第一个事务中修改了这个数据后,第二个事务也修改了这个数据,这样第一个事务的修改结果就被丢失 例如: 事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 3.不可重复读:一个事务读到了另一个事务已经提交(update)的数据。引发另一个事务中多次查询结果不一致。 例如: 第一个事务读取到了一条数据,但并未结束事务。此时第二个事务对该条数据进行了修改并提交。当第一个事务再次读取这条数据时,发现可能和第一次读取的数据不一致,因此被称为不可重复读。 4.虚读/幻读:一个事务读取到了另一个事务已经提交(insert)的数据。导致另一个事务,在事务中多次查询的结果不一致。 例如: 幻读和不可重复读类似。一个事务读取了几行数据,接着另一个事务插入了一些数据并进行了提交,此时第一个事务会发现多了一些原本不存在的数据,就好像发生了幻觉一样,所以被称为幻读。 不可重复读和幻读的区别: 不可重复读的重点在修改,幻读的重点在与新增或者删除 事务隔离级别 读未提交(READ-UNCOMMITTED):最低的隔离级别,允许读取没有提交的数据,可能会导致脏读,幻读和不可重复读 读已提交(READ-COMMITTED):允许事务读取已经提交的数据,可以防止脏读,但是幻读和不可重复读仍有可能发生 可重复读(REPEATABLE-READ):对同一字段的读取结果是一致的,除非事务本身对数据进行修改,可以阻止脏读和不可重复读,但幻度仍有可能发生。 串行化(SERIALIZABLE):最高的隔离级别,完全服从ACID的隔离级别,所有事务依次执行,可以防止脏读,幻读和不可重复读 安全和性能对比 隔离级别的安全和性能成反比,越安全的性能越低 安全性:serializable > repeatable read > read committed > read uncommitted 性能:serializable MySQL InnoDB存储引擎默认支持的隔离级别是可重复读(repeatable-read) 示例 # 两条命令都适用于查看当前的事务隔离级别 show variables like '%isolation%'; select @@tx_isolation; 可更改设置事务的隔离级别 # 其中session可替换为global session代表当前连接 global代表全局 # 设置全局的隔离级别后需要重新连接 set session transaction isolation level read uncommitted; set session transaction isolation level read committed ; set session transaction isolation level repeatable read ; set session transaction isolation level serializable ; 创建一张表 CREATE TABLE `account` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `money` double DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 # 示例每次演示开始的时候都将数据恢复到这个状态 insert into account(id,name,money) values (null,'张三',1000); insert into account(id,name,money) values (null,'李四',1000); 读未提交 开启两个连接(窗口),将隔离级别设置为读未提交(read uncommitted) 读已提交 将左右的隔离级别都设置为读已提交(read committed) 不可重复度会出现得问题 商场做活动，截至时间内，每个人只能得到一份优惠 1000元以下送纸巾 1000元及以上送食用油 截至时间到后开始计算获奖名单，假如A事务正在算活动名单，先算送纸巾名单，发现用户小明消费了500符合条件添加到了获得纸巾得名单，然后事务A在算食用油名单时出现了卡顿，此时小明又成功消费了500，在计算食用油获奖名单时又出现了小明，此时小明会获得两份奖励。（） 可重复读 左右设置隔离级别为可重复读(repeatable read) 保证了一个事务中读取同一条数据是一致的 前面我们说可重复读可能会出现幻读,也就是一个事务中会读到另一个事务插入并提交的数据,我们从上图看到并未发生这种情况,这不是和串行化没啥区别了吗,上网查资料 https://juejin.im/post/5c9040e95188252d92095a9e 进行验证 总结:从这两张图中的执行结果来看,可重复读其实并未完全解决了幻读的问题,只是解决了读数据情况下的幻读问题,对于修改依旧会出现幻读问题。 串行化 串行化:可以看到串行化对事务的要求很高,即使是进行了一次查询的操作,也必须提交,否则其他事务只能在等待状态,等待其他事务结束后再执行 串行的话，读读事务之间还是可以并行的。只是写写事务 读写事务之间是串行的 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/join.html":{"url":"mysql/join.html","title":"join连接","keywords":"","body":"JOIN笛卡尔乘积on 和 where 条件on 条件结论where 条件结论总结JOIN INNER JOIN（内连接，或等值连接）： 获取两个表中字段匹配关系的记录。INNER JOIN 中的 INNER 可省略。 LEFT JOIN（左连接）： 获取左表所有记录，即使右表没有对应匹配的记录。 RIGHT JOIN（右连接）： 与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。 笛卡尔乘积 举例：假设 A 表有 20 条记录，B 表有 30 条记录，则二者关联后的笛卡尔积共 20 * 30 = 600 条记录。也就是说 A 表中的每条记录都会与 B 表的所有记录关联一次，三种关联方式实际上就是对“笛卡尔积”的处理方式不同。 驱动表与被驱动表 当使用 LEFT JOIN 时，左表是驱动表，右表是被驱动表。 当使用 RIGHT JOIN 时，右表是驱动表，左表是被驱动表。 当使用 INNER JOIN 时，mysql 会选择数据量小的表作为驱动表，大表作为被驱动表。 on 和 where 条件 在探索 on 和 where 条件有何不同时，我们需要先明确几个概念： LEFT JOIN：LEFT JOIN 会返回左表中的所有行，即使在右表中没有匹配的行。 临时表：数据库在通过连接两张或者多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。 无论我们在 on 条件语句中增加 左表的条件 或 右表的条件，还是都不加，最终的结果都是五条数据，即左表的全部数据。 但是右表的数据却不同，具体如下： 左表右表条件都不加：右表返回所有左表关联的数据。 左表增加条件g.goods_name = 'T恤1' ：右表只返回 与满足该条件的左表数据 关联的数据。 右表增加条件c.category_name = 'T恤' ：右表只返回满足该条件，并且和左表有关联的数据。 on 条件结论 on 条件是生成临时表的条件，所以不管 on 中的条件是否为真，都会返回左表中的所有记录。 on 条件会过滤右表数据，无论 on 中的条件是左表的条件还是右表的条件，都是用来过滤右表的数据的。满足条件并且有关联的数据才会写入临时表，不然值为 NULL。 where 条件结论 where 条件是在临时表生成好了之后，再对临时表进行过滤的条件。这时已经没有 LEFT JOIN 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 总结 on 是在生成临时表的时候使用的条件，不管on的条件是否起到作用，都会返回左表所有的行。 where 则是在生成临时表之后使用的条件，此时已经不管是否使用了 LEFT JOIN 了，只要条件不为真的行，全部过滤掉。 (INNER | LEFT | RIGHT) JOIN 会生成临时表，该临时表为左表，所以我们在写 JOIN 语句的时候应该选择数据量较小的表作为驱动表。 参考博客 https://juejin.cn/post/6854573222302597134 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/varchar和char.html":{"url":"mysql/varchar和char.html","title":"varchar和char","keywords":"","body":"varchar最多能存储多少个字符?char和varchar的区别? https://learn.blog.csdn.net/article/details/103341778 https://zhuanlan.zhihu.com/p/86259276 首先char最多可以存储255个字符,而不是255个字节,可以使用例子做验证 首先创建一个表,指定字符长度为256 CREATE TABLE 'luck' ( 'content' char(256) ); # Column length too big for column 'content' (max = 255); use BLOB or TEXT instead # 报错提示最大为255,那我们把它修改为255并可以创建成功 # 接下来验证255是最大字节数还是最大字符数 # 首先测试用的mysql使用的utf-8编码,英文字符占一个字节,中文占3个字节 # 在分别插入255个英文字符和中文字符后都没有报错并添加成功,两者再多添加一个都会报错,可以得知255为char存储的最大字符数 进一步验证 再使用命令查看插入255个字符后所占字节的长度 select content,char_length(content),length(content) from luck; 分别是插入255个中文和255个英文字符所得的字符长度和字节长度,可以看到中文的使用了765个字节,是因为使用的是 utf-8编码,一个中文字符会占用3个字节(gbk中中文占用2个字节) 通过搜索在查看mysql技术内幕一书后看到,在mysql4.1版本开始,char(n)中的n指的是字符的长度,而不是之前版本的字节长度,在不同的字符集下,char类型列内部存储的可能不是定长的数据 也就是说在InnoDB存储引擎下使用多字节的字符编码,会把char类型视为变长字符类型,因此可以认为在多字节字符集的情况下,char和varchar的实际行存储基本是没有区别的 那varchar呢,首先可以根据上图了解到varchar最多存储65535个字节,那在创建一个表测试一下 CREATE TABLE luck ( content varchar(65535) ); # Column length too big for column 'content' (max = 21845); use BLOB or TEXT instead # 他会提示最大可存储21845个字符,可以确定varchar中填写的参数也是字符数量,只不过有总字节数的限制 # 那我们把varchar中的字符数改为varchar(21845) CREATE TABLE luck ( content varchar(21845) ); # Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs 运行后,可以看到还是提示过大,这是为什么,我们先把varchar的字符数调到varchar(21844),会发现表创建成功了,这是为什么 因为varchar类型(可变长字段)在计算可存储的字符数有一些条件 在mysql5.1中默认将记录保存为Compact行格式 # 查看表所用的行格式 SHOW TABLE STATUS LIKE 'luck' 可以看到Compact行格式下,不仅会记录真实的数据,还会记录一些记录的信息 计算可用字符数有一个公式:(最大行大小(65535) - null标识列占用字节数 - 长度标识字节数) / 单个字符所在字符集占用字节数 其中的null标识列便是null值列表,每8个允许为null的列会占用一个字节,每行共享 其中的长度标识则是变长字段长度列表,如果该可变字段允许存储的最大字节数(字符数量x指定编码下一个字符占用的字节数)超过255字节并且真实存储的字节数超过127字节,则使用2个字节,否则使用1个字节 有了这些条件,我们来算一下为什么提示说的21845也不行,首先我们使用的是utf-8编码,也就是一个字符会占用1~3个字节,假如存储的全是中文则为21845x3=65535,正好等于65535,前面我们说到一条记录不止有真实的数据,还有记录的额外信息,如果加上这些额外信息占用的字节65535就不够用了,所以也就超出了范围 有了这些前提条件,我们可以通过公式得知,假设该可变字段存储的最大字节数超过了255,并且真实存储的字节数也超过了127个字节,因为要把情况想到最极端长度标识字节数为2个字节,则得出最大的字符数为 (65535-1(默认创建的列是允许为空的,只有一个字段)-2(长度标识字节数))/3(utf8编码) = 21844 尝试着创建表 CREATE TABLE luck ( content varchar(21844) ); # completed in 12 ms # 创建成功 前面说到的最大行大小65535又是什么意思呢 指的是表中所有的字段加起来的总字节数不能超过65536 CREATE TABLE luck ( content varchar(21843), name varchar(1) ); # Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs 我们使用21844可以成功创建一张表,但现在分开创建却又创建不了表是什么原因呢,再套一下公式 (21843+1)x3+1+2+1=65536 我们来解析一下为什么是这样 首先(21843+1)x3表示了最多可以存储的字节数,这个我们可以从上面使用21844成功创建表得知 +1则表示记录的null列值列表,由于是行字段共享的空间,所以还是占一个字节 +2则表示列varchar(21843)的长度标识字节数 +1则是varchar(1)的长度标识字节数 所以加起来超过了总行大小,我们再来验证一下null值列表记录所占的空间是否共享 把这两个字段都设置为非空 CREATE TABLE luck( content varchar(21843) not null , name varchar(1) not null ); # completed in 16 ms 运行成功 所以我们可以得知null值列表所占的字节数是所有列共享的,同时也就证明了每条记录的长度标识字节数并不共享空间 有遗留的问题(未解决),char如果采⽤变⻓字符集时,该列占⽤的字节数也会被加到变⻓字段⻓度列表,按照这个说法name char(1)应该执行失败 # 运行失败 create table luck( content varchar(21843), name varchar(1) ); # 运行成功 create table luck( content varchar(21843), name char(1) ); 2020.03.20查看官方文档解决 char和varchar的区别 官方资料 https://dev.mysql.com/doc/refman/5.7/en/char.html https://dev.mysql.com/doc/refman/5.7/en/storage-requirements.html https://dev.mysql.com/doc/refman/5.7/en/string-type-syntax.html https://dev.mysql.com/doc/refman/5.7/en/column-count-limit.html 其他非官方资料 https://blog.csdn.net/Gane_Cheng/article/details/52316408 总结: 在mysql5.0版本以上char和varchar中的参数都是字符数,但是char有固定长度限制0-255,在创建表时可以不指定,默认为1,char在存储值时会用空格填充到指定的长度,当char的值被检索到时,删除那些用来填充的空格 而varchar列中的值是可变长度的字符串,创建表时必须指定长度,长度可以指定到0-65535(其中65535字节是最大行大小,所有列共享),其最多可以存储的字符数和其所在编码有关 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/mysql架构/mysql架构.html":{"url":"mysql/jike/mysql架构/mysql架构.html","title":"mysql架构","keywords":"","body":"MySQL架构mysql逻辑架构图连接器查询缓存分析器优化器执行器MySQL架构 mysql逻辑架构图 大体上mysql分为客户端、Server层、存储引擎层。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖mysql的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取，支持InnoDB、MyISAM、Memory等，最常用的是InnoDB，其也是mysql5.5.5版本后默认的存储引擎，如果需要使用别的存储引擎，在create table语句后添加engine=memory来指定表所使用的存储引擎。 连接器 负责跟客户端建立连接、获取权限、维持和管理连接。 mysql -h$host -P$port -u$user -p$password 如果认证成功，连接器回到权限表查出当前用户拥有的权限，之后，这个连接里面的权限判断逻辑，都将依赖与此时读到的权限，这意味着，一个用户建立连接后，即使你用管理员账户对这个用户做了权限的修改，也不会影响之前存在的连接，需要再建立新的连接才会使用新的权限。 连接完成后，如果没有后续的动作，这个连接会处于空闲状态，可以通过命令查询所有连接 show processlist; 客户端如果太长时间没有动静，连接器会自动断开，这个时间是由参数wait_timeout控制，默认8小时，如果再连接被断开时候，客户端再次发送请求，就会收到一条 Lost connnection to MySQL server during query，此时需要重新连接发送请求。 数据库里面的长连接指如果客户端持续有请求，则一直使用同一个连接，短连接是指每次执行完很少的几次查询就断开连接，下次查询新建一个，但是建立连接的过程复杂，尽量减少创建连接的过程，但是全部使用长连接后，有时mysql占用内存涨的特别快，因为mysql在执行时临时使用的内存时管理在链接对象里面的，这些资源再连接断开的时候才释放，长时间积累下来可能导致内存占用太大，被系统强行杀掉，从现象看就是MySQL异常重启了。 解决方案: 定期断开长连接，如果使用的是5.7版本及以上，可以使用mysql_reset_connnection来重新初始化连接，这个过程不会重连和重新做权限认证。 查询缓存 mysql拿到一个查询请求后，会先到查询缓存看看之前是否执行过这条语句，如果存在那么之前的查询结果会被返回，如果不存在，继续执行，执行结果会被缓存，但是查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清除，因此查询缓存的命中率会非常低，mysql8.0版本直接将查询缓存的整块功能删掉了。 分析器 分析器会先对sql语句进行词法分析，比如通过select关键字识别出来这是一个查询语句，把表名、列名等识别出来，做完这些识别后，就要做 语法分析，根据词法分析的结果，语法分析会根据语法规定判断输入的sql语句是否符合mysql语法。 优化器 经过分析器后，mysql就知道你要做什么了，在开始执行之前，还要经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引，或者一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如 mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 执行器 mysql通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行阶段，开始执行语句。 开始执行的时候，要先判断你对所执行的类型或者表是否有权限。 mysql> select * from T where ID=10; ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/一条更新语句是怎么执行的/一条更新语句是怎么执行的.html":{"url":"mysql/jike/一条更新语句是怎么执行的/一条更新语句是怎么执行的.html","title":"一条更新语句是怎么执行的","keywords":"","body":"一条更新语句是怎么执行的不可重复读可能出现的问题一条更新语句是怎么执行的 更新与查询不一样的是，涉及了其他两个重要的日志模块，redo log(重做日志)和binlog(归档日志)， redo log的作用：首先redo log功能是InnoDB存储引擎特有的，其作用是在真正写入数据文件前当一个缓冲，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后更新，整个IO成本、查找成本很高。redo log是物理日志，记录的是在某个数据页上做了什么修改。 这就是mysql经常说到的WAL技术，WAL全程是Write-Ahead-Logging,它的关键点就是先写日志，再写磁盘。 具体来说，当有一条记录需要更新的时候，InnoDB引擎会先把记录写到redo log里面，并更新内存，InnoDB引擎在适当的时候会将操作记录更新到磁盘里面，往往是在系统比较空闲的时候做。 binlog 是Server层实现的日志，每个存储引擎都可以使用，binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给 id=2这一行的c字段加1”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切文件继续写，并不会覆盖之前的日志。 更新的过程 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。 你可能注意到了，最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是\"两阶段提交\"。 redo log的两阶段提交+binlog可以保证mysql遇到问题异常重启后的数据不丢失。 prepare阶段 2.写binlog 3.commit redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 不可重复读可能出现的问题 商场做活动，截至时间内，每个人只能得到一份优惠 1000元以下送纸巾 1000元及以上送食用油 截至时间到后开始计算获奖名单，假如A事务正在算活动名单，先算送纸巾名单，发现用户小明消费了500符合条件添加到了获得纸巾得名单，然后事务A在算食用油名单时出现了卡顿，此时小明又成功消费了500，在计算食用油获奖名单时又出现了小明，此时小明会获得两份奖励。（） Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/suoyin/suoyin.html":{"url":"mysql/jike/suoyin/suoyin.html","title":"索引","keywords":"","body":"MySQL索引索引类型基于主键索引和普通索引的查询有什么区别？回表覆盖索引（针对的是查询的列）最左前缀原则（针对的是where后的查询条件）索引下推（针对的是查询的条件）思考题好问题普通索引和唯一索引如何选择查询方面更新方面总结：MySQL索引 索引类型 索引类型分为主键索引和非主键索引 主键索引的叶子节点存的是整行数据，在InnoDB里，主键索引也被称为聚簇索引。 非主键索引的叶子节点内容是主键的值，在InnoDB里，非主键索引也被称为二级索引。 基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 回表 普通索引的叶子节点存储的是 该索引值 -> ID(主键值)，通过索引找到对应的主键值后会到主键索引（主键索引的叶子节点存储的是页，页里面可以有多行，找到对应的页后进行遍历返回符合条件的行）里面进行回表获取完整数据。 覆盖索引（针对的是查询的列） 如果执行的语句，需要获取的列（select column1,colunm2 ...）在查询时用到了索引，并且查询的列都包含在索引中，此时索引就已经“覆盖”了查询请求，因此可以直接提供查询结果，不需要回表，可以减少树的搜索次数并显著提高查询性能，这就是覆盖索引。 例子：有一张市民表，存储市民信息，一般可能会用身份证号去查市民信息，但是如果有一个高频请求，是通过身份证号查姓名，此时可以将身份证号和姓名建立一个联合索引，如果查该市民的全部信息，可以走该索引然后回表，如果只需要名称，也可以走该索引并且可以直接返回信息，不需要回表。 最左前缀原则（针对的是where后的查询条件） B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。 如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是\"where name like ‘张 %’\"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。 可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 建立联合索引时，如何安排索引内的字段顺序 第一原则是如果通过调整顺序，可以少维护一个索引，那么这个顺序往往是优先考虑的。 如果既有a,b的联合查询，又有a,b的单独查询，此时如果查询条件只有b,是用不了（a,b）的联合索引的，便需要在创建一个b字段的索引，此时要考虑的原则就是空间了，也就是说如果b字段的占用比a字段大，那么可以将联合索引的顺序更换为（b,a）,然后再创建占用较小的a字段对应的索，联合查询时将b字段放在前，a字段放在后，这样联合查询和单独的简单查询都可以有优化的执行效果。 索引下推（针对的是查询的条件） 最左前缀可以用于在索引中定位记录，那么不符合最左前缀部分的会怎么样 已知有（name,age）的联合索引。 mysql> select * from tuser where name like '张 %' and age=10 and ismale=1; 最左前缀原则可以帮我们找到满足条件Id为3的记录，在mysql5.6之前只能从Id3一个个回表，到主键索引上找出数据行，再对比其他字段值，再mysql5.6之后引入了索引下推计划，可以在索引遍历过程中，对索引中包含的字段在做判断，直接过滤掉不满足条件的记录，减少回表次数。 思考题 # 重建索引k alter table T drop index k; alter table T add index(k); # 重建主键索引 alter table T drop primary key; alter table T add primary key(id); 为什么要重建索引。索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 重建索引 k 的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做了。这两个语句，你可以用这个语句代替 ： alter table T engine=InnoDB。 13章见详解 innodb_file_per_table 好问题 1. 一张表两个字段id, uname,id主键，uname普通索引 SELECT * FROM test_like WHERE uname LIKE 'j'/ 'j%' / '%j'/ '%j%' 模糊查询like后面四种写法都可以用到uname的普通索引 添加一个age字段 like后面的'%j'/ '%j%' 这两种情况用不到索引 把select * 改为 select id / select uname / select id,uname like后面'j'/ 'j%' / '%j'/ '%j%' 这四种情况又都可以用到uname普通索引 建立uname,age的联合索引 模糊查询还是 LIKE 'j'/ 'j%' / '%j'/ '%j%'四种情况 其中select id / select uname / select id,uname 会用到uname的普通索引 select * 会用到uname,age的组合索引 因为表中只有id,uname这两个字段，使用uname的普通索引可以获取到全部数据，用到了覆盖索引（%j 和 %j% 用到了索引，但却不能利用索引进行快速定位，需要全盘扫描索引） 添加一个age字段后，select 时 %j 和 %j%用不到索引是因为普通索引uname中的数据不包含age字段，还需要进行回表，索引没有使用到索引，当把select 改为 select [id,uname] 时，可以利用索引覆盖特性上述1 在新增新建一个联合索引（uname,age），当获取的列为 [id，uname]时情况与1相同，当为*时，mysql判断使用（uname,age）的联合索引为最佳并且不需要回表（覆盖索引，上述1）。 2. ​ where c order by b limit 1；获取满足c并且b值最小的行 ​ 如果只有c索引没有b索引，那需要扫描所有满足c条件的行，在进行排序后才能得到b值最小的行。 ​ 如果建立（c,b）的联合索引，那么在建立索引时b已经被排好了序，满足c的第一行自然就是当前条件下b值最小的行。 3. 如果k是索引列，那么between时可以先找到最小范围的，然后再遍历n次，in时mysql不知道参数是否连续的，所以只能挨个搜索。 如果k不是索引列，看到评论的朋友说的很好：in的时候需要比对里面的所有值，而between只需要比对最小值和最大值即可。 普通索引和唯一索引如何选择 普通索引，一把梭 查询方面 因为索引是经过排序的，普通索引在查找时需要找到下一个不是指定的元素，然后结束循环，所以差距是很小的 更新方面 关于元素重复的问题，效率微乎其微，重要的是更新这个操作， 要更新的目标页在内存中 唯一索引：找到元素左右的位置，在判断两个位置中间有没有与元素值冲突 普通索引：找到指定元素的左右的位置，直接插入值 要更新的目标页不在内存中 唯一索引：需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 普通索引：则是将更新记录在 change buffer，语句执行就结束了 -- force index(a) 强制使用a索引 select * from t force index(a) where a between 10000 and 20000;/*Q2*/ 总结： 1.索引的作用：提高数据查询效率 2.常见索引模型：哈希表、有序数组、搜索树 3.哈希表：键 - 值(key - value)。 4.哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置 5.哈希冲突的处理办法：链表 6.哈希表适用场景：只有等值查询的场景 7.有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N)) 8.有序数组查询效率高，更新效率低 9.有序数组的适用场景：静态存储引擎。 10.二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子 11.二叉搜索树：查询时间复杂度O(log(N))，更新时间复杂度O(log(N)) 12.数据库存储大多不适用二叉树，因为树高过高，会适用N叉树 13.InnoDB中的索引模型：B+Tree 14.索引类型：主键索引、非主键索引 主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引) 15.主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表) 16.一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。 17.从性能和存储空间方面考量，自增主键往往是更合理的选择。 思考题： 如果删除，新建主键索引，会同时去修改普通索引对应的主键索引，性能消耗比较大。 删除重建普通索引貌似影响不大，不过要注意在业务低谷期操作，避免影响业务。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/suo/mysql_全局锁_表锁_行锁.html":{"url":"mysql/jike/suo/mysql_全局锁_表锁_行锁.html","title":"锁","keywords":"","body":"MySQL锁全局锁表级锁行锁死锁和死锁检测MySQL锁 全局锁 加锁 Flush tables with read lock; 释放锁 unlock tables; 加了全局锁后该数据库实例变为只读，当前更新时线程会报以下错误 Can't execute the query because you have a conflicting read lock 之后其他线程的以下语句会被阻塞（等待状态）：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。 但是该方法会导致整个系统只读，系统业务失效，官方推荐方式使用 mysqldump配合–single-transaction参数，这样在导数据之前就会启动一个事务，来确保拿到一致性视图，但是引擎必须支持事务（可重复读隔离级别），所以遇到MyISAM引擎时只能使用FTWRL备份了。 set global readonly=true和FTWRL的区别 FTWRL所在的客户端连接异常断开连接后，mysql会自动释放锁，整个库回到正常更新的状态，而前者如果客户端发生异常，则数据库就会一直保持readonly状态。 表级锁 语法 lock tables tableName read/write; unlock tables; 左为上锁的客户端1，右为测试的客户端2 客户端1 另一种表锁是MDL（metadata lock）,其不需要显示使用，在访问一个表的时候会被自动加上。MDL的作用是保证读写的正确性，假如一个线程正在查询遍历一个表中的数据，而执行期间另一个线程对该表结构做了变更，此时拿到的数据与表结构对不上，因此从5.5版本开始引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁（当前线程只读不可写，其他线程可读写阻塞），当对表结构变更操作时加MDL写锁（当前线程可读写，其他线程读写阻塞）。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 行锁 ​ 行锁是在各个引擎层由各个引擎实现的，并不是所有的引擎都支持行锁，MyISAM就不支持行锁，所以MyISAM引擎中一张表任何时刻只能有一个更新执行。InnoDB是支持行锁的，这也是InnoDB替代MyISAM成为默认存储引擎的重要原因之一。行锁就是同一个时刻同一个记录只能有一个事务可以更新，其他事务要等这个事务处理完成才能更新。 ​ 并且如果事务2等待超时会退出并抛出异常 超时时间由参数 innodb_lock_wait_timeout 控制 当 事务1 commit; 后 并且在超时时间内获取到锁后事务2自动执行 如果事务中需要锁多个行，要把最有可能造成锁冲突，最可能影响并发度的锁尽量往后放。 死锁和死锁检测 下图可以看到事务1先更新id为50的记录，获取id为50的行锁，事务2更新id为60的记录获取id为60的行锁，事务1在执行更新id为60的记录时发现有锁阻塞，事务2更新id为50的记录时，死锁检测，发现存在死锁。 死锁检测是由参数 innodb_deadlock_detect 控制的，on代表开启（默认开启），死锁检测会主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。 有A B 两行 Deadlock found when trying to get lock; try restarting transaction A 行 等待50秒 Lock wait timeout exceeded; try restarting transaction 解代码层面决思路：控制并发度。。。，在设计上解决，将在一行上的逻辑改为多行，当然也就带来了维护多行的麻烦。。。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/事务/事务.html":{"url":"mysql/jike/事务/事务.html","title":"事务","keywords":"","body":"MySQL事务MySQL事务 情景1 ​ 在可重复读隔离级别下，开始事务A后即使在事务B里更新了数据提交后，事务A看到的数据仍然和启动时看到的数据一样，但是在事务A中更新时却会影响到事务B中插入的数据。 情景2 ​ 在可重复读隔离级别下，事务A要更新一行，但这一行正在被事务B更新，此时事务A进入阻塞状态，那么等到事务B更新完毕提交之后，事务A会看到什么样的数据。 快照读？ 极客的例子 事务A先开启事务，到最后查询的K值还是1。 事务B开启事务。 事务C开启事务并且更新id为1的K为2，自动提交。 事务B更新id为1的K值，这是的K值已经变为2了，更新后查询则为3。 这里，我们需要注意的是事务的启动时机。 begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 第一种启动方式，一致性视图是在第执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。 在 MySQL 里，有两个“视图”的概念： 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 可重复读情况下 事务A 看不到事务B的更改 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/xn/mysql执行计划.html":{"url":"mysql/jike/xn/mysql执行计划.html","title":"mysql执行计划","keywords":"","body":"mysql执行计划1、执行计划中包含的信息idselect_typetabletypepossible_keyskeykey_lenrefrowsextramysql执行计划 ​ 在企业的应用场景中，为了知道优化SQL语句的执行，需要查看SQL语句的具体执行过程，以加快SQL语句的执行效率。 ​ 可以使用explain+SQL语句来模拟优化器执行SQL查询语句，从而知道mysql是如何处理sql语句的。 ​ 官网地址： https://dev.mysql.com/doc/refman/5.5/en/explain-output.html 1、执行计划中包含的信息 Column Meaning id The SELECT identifier select_type The SELECT type table The table for the output row partitions The matching partitions type The join type possible_keys The possible indexes to choose key The index actually chosen key_len The length of the chosen key ref The columns compared to the index rows Estimate of rows to be examined filtered Percentage of rows filtered by table condition extra Additional information id select查询的序列号，包含一组数字，表示查询中执行select子句或者操作表的顺序 id号分为三种情况： 1、如果id相同，那么执行顺序从上到下 explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal; 2、如果id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 explain select * from emp e where e.deptno in (select d.deptno from dept d where d.dname = 'SALES'); 3、id相同和不同的，同时存在：相同的可以认为是一组，从上往下顺序执行，在所有组中，id值越大，优先级越高，越先执行 explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal where e.deptno in (select d.deptno from dept d where d.dname = 'SALES'); select_type 主要用来分辨查询的类型，是普通查询还是联合查询还是子查询 select_type Value Meaning SIMPLE Simple SELECT (not using UNION or subqueries) PRIMARY Outermost SELECT UNION Second or later SELECT statement in a UNION DEPENDENT UNION Second or later SELECT statement in a UNION, dependent on outer query UNION RESULT Result of a UNION. SUBQUERY First SELECT in subquery DEPENDENT SUBQUERY First SELECT in subquery, dependent on outer query DERIVED Derived table UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) --sample:简单的查询，不包含子查询和union explain select * from emp; --primary:查询中若包含任何复杂的子查询，最外层查询则被标记为Primary explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ; --union:若第二个select出现在union之后，则被标记为union explain select * from emp where deptno = 10 union select * from emp where sal >2000; --dependent union:跟union类似，此处的depentent表示union或union all联合而成的结果会受外部表影响 explain select * from emp e where e.empno in ( select empno from emp where deptno = 10 union select empno from emp where sal >2000) --union result:从union表获取结果的select explain select * from emp where deptno = 10 union select * from emp where sal >2000; --subquery:在select或者where列表中包含子查询 explain select * from emp where sal > (select avg(sal) from emp) ; --dependent subquery:subquery的子查询要受到外部表查询的影响 explain select * from emp e where e.deptno in (select distinct deptno from dept); --DERIVED: from子句中出现的子查询，也叫做派生类， explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ; --UNCACHEABLE SUBQUERY：表示使用子查询的结果不能被缓存 explain select * from emp where empno = (select empno from emp where deptno=@@sort_buffer_size); --uncacheable union:表示union的查询结果不能被缓存：sql语句未验证 table 对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集 1、如果是具体的表名，则表明从实际的物理表中获取数据，当然也可以是表的别名 ​ 2、表名是derivedN的形式，表示使用了id为N的查询产生的衍生表 ​ 3、当有union result的时候，表名是union n1,n2等的形式，n1,n2表示参与union的id type type显示的是访问类型，访问类型表示我是以何种方式去访问我们的数据，最容易想的是全表扫描，直接暴力的遍历一张表去寻找需要的数据，效率非常低下，访问的类型有很多，效率从最好到最坏依次是： system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL 一般情况下，得保证查询至少达到range级别，最好能达到ref --all:全表扫描，一般情况下出现这样的sql语句而且数据量比较大的话那么就需要进行优化。 explain select * from emp; --index：全索引扫描这个比all的效率要好，主要有两种情况，一种是当前的查询时覆盖索引，即我们需要的数据在索引中就可以索取，或者是使用了索引进行排序，这样就避免数据的重排序 explain select empno from emp; --range：表示利用索引查询的时候限制了范围，在指定范围内进行查询，这样避免了index的全索引扫描，适用的操作符： =, <>, >, >=, possible_keys ​ 显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用 explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; key ​ 实际使用的索引，如果为null，则没有使用索引，查询中若使用了覆盖索引，则该索引和查询的select字段重叠。 explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; key_len 表示索引中使用的字节数，可以通过key_len计算查询中使用的索引长度，在不损失精度的情况下长度越短越好。 explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; ref 显示索引的哪一列被使用了，如果可能的话，是一个常数 explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; rows 根据表的统计信息及索引使用情况，大致估算出找出所需记录需要读取的行数，此参数很重要，直接反应的sql找了多少数据，在完成目的的情况下越少越好 explain select * from emp; extra 包含额外的信息。 --using filesort:说明mysql无法利用索引进行排序，只能利用排序算法进行排序，会消耗额外的位置 explain select * from emp order by sal; --using temporary:建立临时表来保存中间结果，查询完成之后把临时表删除 explain select ename,count(*) from emp where deptno = 10 group by ename; --using index:这个表示当前的查询时覆盖索引的，直接从索引中读取数据，而不用访问数据表。如果同时出现using where 表名索引被用来执行索引键值的查找，如果没有，表面索引被用来读取数据，而不是真的查找 explain select deptno,count(*) from emp group by deptno limit 10; --using where:使用where进行条件过滤 explain select * from t_user where id = 1; --using join buffer:使用连接缓存，情况没有模拟出来 --impossible where：where语句的结果总是false explain select * from emp where empno = 7469; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mysql/jike/xn/MYSQL_performance_schema详解.html":{"url":"mysql/jike/xn/MYSQL_performance_schema详解.html","title":"MYSQL_performance_schema详解","keywords":"","body":"MYSQL performance schema详解0、performance_schema的介绍1、performance schema入门2、performance_schema表的分类3、performance_schema的简单配置与使用4、常用配置项的参数说明5、重要配置表的相关说明6、performance_schema实践操作MYSQL performance schema详解 0、performance_schema的介绍 ​ MySQL的performance schema 用于监控MySQL server在一个较低级别的运行过程中的资源消耗、资源等待等情况。 ​ 特点如下： ​ 1、提供了一种在数据库运行时实时检查server的内部执行情况的方法。performance_schema 数据库中的表使用performance_schema存储引擎。该数据库主要关注数据库运行过程中的性能相关的数据，与information_schema不同，information_schema主要关注server运行过程中的元数据信息。 ​ 2、performance_schema通过监视server的事件来实现监视server内部运行情况， “事件”就是server内部活动中所做的任何事情以及对应的时间消耗，利用这些信息来判断server中的相关资源消耗在了哪里？一般来说，事件可以是函数调用、操作系统的等待、SQL语句执行的阶段（如sql语句执行过程中的parsing 或 sorting阶段）或者整个SQL语句与SQL语句集合。事件的采集可以方便的提供server中的相关存储引擎对磁盘文件、表I/O、表锁等资源的同步调用信息。 ​ 3、performance_schema中的事件与写二进制日志中的事件（描述数据修改的events）、事件计划调度程序（这是一种存储程序）的事件不同。performance_schema中的事件记录的是server执行某些活动对某些资源的消耗、耗时、这些活动执行的次数等情况。 ​ 4、performance_schema中的事件只记录在本地server的performance_schema中，其下的这些表中数据发生变化时不会被写入binlog中，也不会通过复制机制被复制到其他server中。 ​ 5、 当前活跃事件、历史事件和事件摘要相关的表中记录的信息。能提供某个事件的执行次数、使用时长。进而可用于分析某个特定线程、特定对象（如mutex或file）相关联的活动。 ​ 6、PERFORMANCE_SCHEMA存储引擎使用server源代码中的“检测点”来实现事件数据的收集。对于performance_schema实现机制本身的代码没有相关的单独线程来检测，这与其他功能（如复制或事件计划程序）不同 ​ 7、收集的事件数据存储在performanceschema数据库的表中。这些表可以使用SELECT语句查询，也可以使用SQL语句更新performance_schema数据库中的表记录（如动态修改performance_schema的setup*开头的几个配置表，但要注意：配置表的更改会立即生效，这会影响数据收集） ​ 8、performance_schema的表中的数据不会持久化存储在磁盘中，而是保存在内存中，一旦服务器重启，这些数据会丢失（包括配置表在内的整个performance_schema下的所有数据） ​ 9、MySQL支持的所有平台中事件监控功能都可用，但不同平台中用于统计事件时间开销的计时器类型可能会有所差异。 1、performance schema入门 ​ 在mysql的5.7版本中，性能模式是默认开启的，如果想要显式的关闭的话需要修改配置文件，不能直接进行修改，会报错Variable 'performance_schema' is a read only variable。 --查看performance_schema的属性 mysql> SHOW VARIABLES LIKE 'performance_schema'; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | performance_schema | ON | +--------------------+-------+ 1 row in set (0.01 sec) --在配置文件中修改performance_schema的属性值，on表示开启，off表示关闭 [mysqld] performance_schema=ON --切换数据库 use performance_schema; --查看当前数据库下的所有表,会看到有很多表存储着相关的信息 show tables; --可以通过show create table tablename来查看创建表的时候的表结构 mysql> show create table setup_consumers; +-----------------+--------------------------------- | Table | Create Table +-----------------+--------------------------------- | setup_consumers | CREATE TABLE `setup_consumers` ( `NAME` varchar(64) NOT NULL, `ENABLED` enum('YES','NO') NOT NULL ) ENGINE=PERFORMANCE_SCHEMA DEFAULT CHARSET=utf8 | +-----------------+--------------------------------- 1 row in set (0.00 sec) ​ 想要搞明白后续的内容，同学们需要理解两个基本概念： ​ instruments: 生产者，用于采集mysql中各种各样的操作产生的事件信息，对应配置表中的配置项我们可以称为监控采集配置项。 ​ consumers:消费者，对应的消费者表用于存储来自instruments采集的数据，对应配置表中的配置项我们可以称为消费存储配置项。 2、performance_schema表的分类 ​ performance_schema库下的表可以按照监视不同的纬度就行分组。 --语句事件记录表，这些表记录了语句事件信息，当前语句事件表events_statements_current、历史语句事件表events_statements_history和长语句历史事件表events_statements_history_long、以及聚合后的摘要表summary，其中，summary表还可以根据帐号(account)，主机(host)，程序(program)，线程(thread)，用户(user)和全局(global)再进行细分) show tables like '%statement%'; --等待事件记录表，与语句事件类型的相关记录表类似： show tables like '%wait%'; --阶段事件记录表，记录语句执行的阶段事件的表 show tables like '%stage%'; --事务事件记录表，记录事务相关的事件的表 show tables like '%transaction%'; --监控文件系统层调用的表 show tables like '%file%'; --监视内存使用的表 show tables like '%memory%'; --动态对performance_schema进行配置的配置表 show tables like '%setup%'; 3、performance_schema的简单配置与使用 ​ 数据库刚刚初始化并启动时，并非所有instruments(事件采集项，在采集项的配置表中每一项都有一个开关字段，或为YES，或为NO)和consumers(与采集项类似，也有一个对应的事件类型保存表配置项，为YES就表示对应的表保存性能数据，为NO就表示对应的表不保存性能数据)都启用了，所以默认不会收集所有的事件，可能你需要检测的事件并没有打开，需要进行设置，可以使用如下两个语句打开对应的instruments和consumers（行计数可能会因MySQL版本而异)。 --打开等待事件的采集器配置项开关，需要修改setup_instruments配置表中对应的采集器配置项 UPDATE setup_instruments SET ENABLED = 'YES', TIMED = 'YES'where name like 'wait%'; --打开等待事件的保存表配置开关，修改setup_consumers配置表中对应的配置项 UPDATE setup_consumers SET ENABLED = 'YES'where name like '%wait%'; --当配置完成之后可以查看当前server正在做什么，可以通过查询events_waits_current表来得知，该表中每个线程只包含一行数据，用于显示每个线程的最新监视事件 select * from events_waits_current\\G *************************** 1. row *************************** THREAD_ID: 11 EVENT_ID: 570 END_EVENT_ID: 570 EVENT_NAME: wait/synch/mutex/innodb/buf_dblwr_mutex SOURCE: TIMER_START: 4508505105239280 TIMER_END: 4508505105270160 TIMER_WAIT: 30880 SPINS: NULL OBJECT_SCHEMA: NULL OBJECT_NAME: NULL INDEX_NAME: NULL OBJECT_TYPE: NULL OBJECT_INSTANCE_BEGIN: 67918392 NESTING_EVENT_ID: NULL NESTING_EVENT_TYPE: NULL OPERATION: lock NUMBER_OF_BYTES: NULL FLAGS: NULL /*该信息表示线程id为11的线程正在等待buf_dblwr_mutex锁，等待事件为30880 属性说明： id:事件来自哪个线程，事件编号是多少 event_name:表示检测到的具体的内容 source:表示这个检测代码在哪个源文件中以及行号 timer_start:表示该事件的开始时间 timer_end:表示该事件的结束时间 timer_wait:表示该事件总的花费时间 注意：_current表中每个线程只保留一条记录，一旦线程完成工作，该表中不会再记录该线程的事件信息 */ /* _history表中记录每个线程应该执行完成的事件信息，但每个线程的事件信息只会记录10条，再多就会被覆盖，*_history_long表中记录所有线程的事件信息，但总记录数量是10000，超过就会被覆盖掉 */ select thread_id,event_id,event_name,timer_wait from events_waits_history order by thread_id limit 21; /* summary表提供所有事件的汇总信息，该组中的表以不同的方式汇总事件数据（如：按用户，按主机，按线程等等）。例如：要查看哪些instruments占用最多的时间，可以通过对events_waits_summary_global_by_event_name表的COUNT_STAR或SUM_TIMER_WAIT列进行查询（这两列是对事件的记录数执行COUNT（*）、事件记录的TIMER_WAIT列执行SUM（TIMER_WAIT）统计而来） */ SELECT EVENT_NAME,COUNT_STAR FROM events_waits_summary_global_by_event_name ORDER BY COUNT_STAR DESC LIMIT 10; /* instance表记录了哪些类型的对象会被检测。这些对象在被server使用时，在该表中将会产生一条事件记录，例如，file_instances表列出了文件I/O操作及其关联文件名 */ select * from file_instances limit 20; 4、常用配置项的参数说明 1、启动选项 performance_schema_consumer_events_statements_current=TRUE 是否在mysql server启动时就开启events_statements_current表的记录功能(该表记录当前的语句事件信息)，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新setup_consumers配置表中的events_statements_current配置项，默认值为TRUE performance_schema_consumer_events_statements_history=TRUE 与performance_schema_consumer_events_statements_current选项类似，但该选项是用于配置是否记录语句事件短历史信息，默认为TRUE performance_schema_consumer_events_stages_history_long=FALSE 与performance_schema_consumer_events_statements_current选项类似，但该选项是用于配置是否记录语句事件长历史信息，默认为FALSE 除了statement(语句)事件之外，还支持：wait(等待)事件、state(阶段)事件、transaction(事务)事件，他们与statement事件一样都有三个启动项分别进行配置，但这些等待事件默认未启用，如果需要在MySQL Server启动时一同启动，则通常需要写进my.cnf配置文件中 performance_schema_consumer_global_instrumentation=TRUE 是否在MySQL Server启动时就开启全局表（如：mutex_instances、rwlock_instances、cond_instances、file_instances、users、hostsaccounts、socket_summary_by_event_name、file_summary_by_instance等大部分的全局对象计数统计和事件汇总统计信息表 ）的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新全局配置项 默认值为TRUE performance_schema_consumer_statements_digest=TRUE 是否在MySQL Server启动时就开启events_statements_summary_by_digest 表的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新digest配置项 默认值为TRUE performance_schema_consumer_thread_instrumentation=TRUE 是否在MySQL Server启动时就开启 events_xxx_summary_by_yyy_by_event_name表的记录功能，启动之后也可以在setup_consumers表中使用UPDATE语句进行动态更新线程配置项 默认值为TRUE performance_schema_instrument[=name] 是否在MySQL Server启动时就启用某些采集器，由于instruments配置项多达数千个，所以该配置项支持key-value模式，还支持%号进行通配等，如下: # [=name]可以指定为具体的Instruments名称（但是这样如果有多个需要指定的时候，就需要使用该选项多次），也可以使用通配符，可以指定instruments相同的前缀+通配符，也可以使用%代表所有的instruments ## 指定开启单个instruments --performance-schema-instrument= 'instrument_name=value' ## 使用通配符指定开启多个instruments --performance-schema-instrument= 'wait/synch/cond/%=COUNTED' ## 开关所有的instruments --performance-schema-instrument= '%=ON' --performance-schema-instrument= '%=OFF' 注意，这些启动选项要生效的前提是，需要设置performance_schema=ON。另外，这些启动选项虽然无法使用show variables语句查看，但我们可以通过setup_instruments和setup_consumers表查询这些选项指定的值。 2、系统变量 show variables like '%performance_schema%'; --重要的属性解释 performance_schema=ON /* 控制performance_schema功能的开关，要使用MySQL的performance_schema，需要在mysqld启动时启用，以启用事件收集功能 该参数在5.7.x之前支持performance_schema的版本中默认关闭，5.7.x版本开始默认开启 注意：如果mysqld在初始化performance_schema时发现无法分配任何相关的内部缓冲区，则performance_schema将自动禁用，并将performance_schema设置为OFF */ performance_schema_digests_size=10000 /* 控制events_statements_summary_by_digest表中的最大行数。如果产生的语句摘要信息超过此最大值，便无法继续存入该表，此时performance_schema会增加状态变量 */ performance_schema_events_statements_history_long_size=10000 /* 控制events_statements_history_long表中的最大行数，该参数控制所有会话在events_statements_history_long表中能够存放的总事件记录数，超过这个限制之后，最早的记录将被覆盖 全局变量，只读变量，整型值，5.6.3版本引入 * 5.6.x版本中，5.6.5及其之前的版本默认为10000，5.6.6及其之后的版本默认值为-1，通常情况下，自动计算的值都是10000 * 5.7.x版本中，默认值为-1，通常情况下，自动计算的值都是10000 */ performance_schema_events_statements_history_size=10 /* 控制events_statements_history表中单个线程（会话）的最大行数，该参数控制单个会话在events_statements_history表中能够存放的事件记录数，超过这个限制之后，单个会话最早的记录将被覆盖 全局变量，只读变量，整型值，5.6.3版本引入 * 5.6.x版本中，5.6.5及其之前的版本默认为10，5.6.6及其之后的版本默认值为-1，通常情况下，自动计算的值都是10 * 5.7.x版本中，默认值为-1，通常情况下，自动计算的值都是10 除了statement(语句)事件之外，wait(等待)事件、state(阶段)事件、transaction(事务)事件，他们与statement事件一样都有三个参数分别进行存储限制配置，有兴趣的同学自行研究，这里不再赘述 */ performance_schema_max_digest_length=1024 /* 用于控制标准化形式的SQL语句文本在存入performance_schema时的限制长度，该变量与max_digest_length变量相关(max_digest_length变量含义请自行查阅相关资料) 全局变量，只读变量，默认值1024字节，整型值，取值范围0~1048576 */ performance_schema_max_sql_text_length=1024 /* 控制存入events_statements_current，events_statements_history和events_statements_history_long语句事件表中的SQL_TEXT列的最大SQL长度字节数。 超出系统变量performance_schema_max_sql_text_length的部分将被丢弃，不会记录，一般情况下不需要调整该参数，除非被截断的部分与其他SQL比起来有很大差异 全局变量，只读变量，整型值，默认值为1024字节，取值范围为0~1048576，5.7.6版本引入 降低系统变量performance_schema_max_sql_text_length值可以减少内存使用，但如果汇总的SQL中，被截断部分有较大差异，会导致没有办法再对这些有较大差异的SQL进行区分。 增加该系统变量值会增加内存使用，但对于汇总SQL来讲可以更精准地区分不同的部分。 */ 5、重要配置表的相关说明 ​ 配置表之间存在相互关联关系，按照配置影响的先后顺序，可添加为 /* performance_timers表中记录了server中有哪些可用的事件计时器 字段解释： timer_name:表示可用计时器名称，CYCLE是基于CPU周期计数器的定时器 timer_frequency:表示每秒钟对应的计时器单位的数量,CYCLE计时器的换算值与CPU的频率相关、 timer_resolution:计时器精度值，表示在每个计时器被调用时额外增加的值 timer_overhead:表示在使用定时器获取事件时开销的最小周期值 */ select * from performance_timers; /* setup_timers表中记录当前使用的事件计时器信息 字段解释： name:计时器类型，对应某个事件类别 timer_name:计时器类型名称 */ select * from setup_timers; /* setup_consumers表中列出了consumers可配置列表项 字段解释： NAME：consumers配置名称 ENABLED：consumers是否启用，有效值为YES或NO，此列可以使用UPDATE语句修改。 */ select * from setup_consumers; /* setup_instruments 表列出了instruments 列表配置项，即代表了哪些事件支持被收集： 字段解释： NAME：instruments名称，instruments名称可能具有多个部分并形成层次结构 ENABLED：instrumetns是否启用，有效值为YES或NO，此列可以使用UPDATE语句修改。如果设置为NO，则这个instruments不会被执行，不会产生任何的事件信息 TIMED：instruments是否收集时间信息，有效值为YES或NO，此列可以使用UPDATE语句修改，如果设置为NO，则这个instruments不会收集时间信息 */ SELECT * FROM setup_instruments; /* setup_actors表的初始内容是匹配任何用户和主机，因此对于所有前台线程，默认情况下启用监视和历史事件收集功能 字段解释： HOST：与grant语句类似的主机名，一个具体的字符串名字，或使用“％”表示“任何主机” USER：一个具体的字符串名称，或使用“％”表示“任何用户” ROLE：当前未使用，MySQL 8.0中才启用角色功能 ENABLED：是否启用与HOST，USER，ROLE匹配的前台线程的监控功能，有效值为：YES或NO HISTORY：是否启用与HOST， USER，ROLE匹配的前台线程的历史事件记录功能，有效值为：YES或NO */ SELECT * FROM setup_actors; /* setup_objects表控制performance_schema是否监视特定对象。默认情况下，此表的最大行数为100行。 字段解释： OBJECT_TYPE：instruments类型，有效值为：“EVENT”（事件调度器事件）、“FUNCTION”（存储函数）、“PROCEDURE”（存储过程）、“TABLE”（基表）、“TRIGGER”（触发器），TABLE对象类型的配置会影响表I/O事件（wait/io/table/sql/handler instrument）和表锁事件（wait/lock/table/sql/handler instrument）的收集 OBJECT_SCHEMA：某个监视类型对象涵盖的数据库名称，一个字符串名称，或“％”(表示“任何数据库”) OBJECT_NAME：某个监视类型对象涵盖的表名，一个字符串名称，或“％”(表示“任何数据库内的对象”) ENABLED：是否开启对某个类型对象的监视功能，有效值为：YES或NO。此列可以修改 TIMED：是否开启对某个类型对象的时间收集功能，有效值为：YES或NO，此列可以修改 */ SELECT * FROM setup_objects; /* threads表对于每个server线程生成一行包含线程相关的信息， 字段解释： THREAD_ID：线程的唯一标识符（ID） NAME：与server中的线程检测代码相关联的名称(注意，这里不是instruments名称) TYPE：线程类型，有效值为：FOREGROUND、BACKGROUND。分别表示前台线程和后台线程 PROCESSLIST_ID：对应INFORMATION_SCHEMA.PROCESSLIST表中的ID列。 PROCESSLIST_USER：与前台线程相关联的用户名，对于后台线程为NULL。 PROCESSLIST_HOST：与前台线程关联的客户端的主机名，对于后台线程为NULL。 PROCESSLIST_DB：线程的默认数据库，如果没有，则为NULL。 PROCESSLIST_COMMAND：对于前台线程，该值代表着当前客户端正在执行的command类型，如果是sleep则表示当前会话处于空闲状态 PROCESSLIST_TIME：当前线程已处于当前线程状态的持续时间（秒） PROCESSLIST_STATE：表示线程正在做什么事情。 PROCESSLIST_INFO：线程正在执行的语句，如果没有执行任何语句，则为NULL。 PARENT_THREAD_ID：如果这个线程是一个子线程（由另一个线程生成），那么该字段显示其父线程ID ROLE：暂未使用 INSTRUMENTED：线程执行的事件是否被检测。有效值：YES、NO HISTORY：是否记录线程的历史事件。有效值：YES、NO * THREAD_OS_ID：由操作系统层定义的线程或任务标识符（ID）： */ select * from threads 注意：在performance_schema库中还包含了很多其他的库和表，能对数据库的性能做完整的监控，大家需要参考官网详细了解。 6、performance_schema实践操作 ​ 基本了解了表的相关信息之后，可以通过这些表进行实际的查询操作来进行实际的分析。 --1、哪类的SQL执行最多？ SELECT DIGEST_TEXT,COUNT_STAR,FIRST_SEEN,LAST_SEEN FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --2、哪类SQL的平均响应时间最多？ SELECT DIGEST_TEXT,AVG_TIMER_WAIT FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --3、哪类SQL排序记录数最多？ SELECT DIGEST_TEXT,SUM_SORT_ROWS FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --4、哪类SQL扫描记录数最多？ SELECT DIGEST_TEXT,SUM_ROWS_EXAMINED FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --5、哪类SQL使用临时表最多？ SELECT DIGEST_TEXT,SUM_CREATED_TMP_TABLES,SUM_CREATED_TMP_DISK_TABLES FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --6、哪类SQL返回结果集最多？ SELECT DIGEST_TEXT,SUM_ROWS_SENT FROM events_statements_summary_by_digest ORDER BY COUNT_STAR DESC --7、哪个表物理IO最多？ SELECT file_name,event_name,SUM_NUMBER_OF_BYTES_READ,SUM_NUMBER_OF_BYTES_WRITE FROM file_summary_by_instance ORDER BY SUM_NUMBER_OF_BYTES_READ + SUM_NUMBER_OF_BYTES_WRITE DESC --8、哪个表逻辑IO最多？ SELECT object_name,COUNT_READ,COUNT_WRITE,COUNT_FETCH,SUM_TIMER_WAIT FROM table_io_waits_summary_by_table ORDER BY sum_timer_wait DESC --9、哪个索引访问最多？ SELECT OBJECT_NAME,INDEX_NAME,COUNT_FETCH,COUNT_INSERT,COUNT_UPDATE,COUNT_DELETE FROM table_io_waits_summary_by_index_usage ORDER BY SUM_TIMER_WAIT DESC --10、哪个索引从来没有用过？ SELECT OBJECT_SCHEMA,OBJECT_NAME,INDEX_NAME FROM table_io_waits_summary_by_index_usage WHERE INDEX_NAME IS NOT NULL AND COUNT_STAR = 0 AND OBJECT_SCHEMA <> 'mysql' ORDER BY OBJECT_SCHEMA,OBJECT_NAME; --11、哪个等待事件消耗时间最多？ SELECT EVENT_NAME,COUNT_STAR,SUM_TIMER_WAIT,AVG_TIMER_WAIT FROM events_waits_summary_global_by_event_name WHERE event_name != 'idle' ORDER BY SUM_TIMER_WAIT DESC --12-1、剖析某条SQL的执行情况，包括statement信息，stege信息，wait信息 SELECT EVENT_ID,sql_text FROM events_statements_history WHERE sql_text LIKE '%count(*)%'; --12-2、查看每个阶段的时间消耗 SELECT event_id,EVENT_NAME,SOURCE,TIMER_END - TIMER_START FROM events_stages_history_long WHERE NESTING_EVENT_ID = 1553; --12-3、查看每个阶段的锁等待情况 SELECT event_id,event_name,source,timer_wait,object_name,index_name,operation,nesting_event_id FROM events_waits_history_longWHERE nesting_event_id = 1553; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"redis/redis.html":{"url":"redis/redis.html","title":"Redis基础","keywords":"","body":"Redis安装常用命令string类型hash类型list类型set类型sorted_set类型通用操作指令Java操作RedisRedis 概述:是使用C语言开发的一个开源的高性能键值对数据库 特征:数据间没有必然的关联关系(mysql外键),内部采用单线程机制进行工作,高性能(官方测试数据为,50个并发执行100000个请求,读的速度是110000次/s,写的速度是81000次每秒),支持持久化 数据类型(key-value,指的是value的数据类型): ​ string:字符串,如果value是一个纯数字可对其进行数字操作,但其任然是一个字符串 ​ hash:底层采用哈希表结构,key对应一片存储空间,存储field value的键值对 ​ list: ​ set: ​ sorted_set: 应用:主要为热点数据加速查询,热点商品,新闻,资讯,访问量信息等;任务队列,如秒杀、抢购、购票排队等;即时信息查询,如各位排行榜在线人数信息 安装 使用docker安装 查看版本 需首先进入容器根目录 redis-server --version # Redis server v=6.0.3 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=839141755a9d16cb 使用命令行需要进入/data文件夹使用命令启动客户端 redis-cli -p 6379 退出 quit exit help # help 空格 接Tab键可以切换查看指定命令组的信息 # 或者直接接某个命令 help Tab | 命令 time # 获取当前系统时间 # 秒 1) \"1593921622\" 2) \"3230\" 常用命令 string类型 基本操作 # 添加/修改 数据 set key value # 获取数据,如果不存在返回 nil get key # 删除数据,会返回Integer数字1代表成功 0代表失败 del key # 添加/修改多个数据 mset key1 value1 key2 value2 ... # 获取多个数据 mget key1 key2 ... # 获取字符串个数 strlen key # 追加信息,返回追加后字符串的长度 append key value 对纯数值value操作 string在redis内部存储默认就是一个字符串,当遇到增减类操作,会自动转成数值型计算 redis所有的操作都是原子性的,采用单线程处理所有业务,命令时一个一个执行,因此无需考虑并发带来的数据影响 数值最大不可操作9223372036854775807(Java中Long类型最大值Long.MAX_VALUE) 基于以上特性,可以应用于控制多数据库时主键生成,保证主键唯一性 # 指定值可以为负数 # 自增1 incr key # 自增指定值 incrby key 指定值 # 可以指定小数值,需要注意的是一旦成为小数,便不可再用incr或decr相关命令 : ERR value is not an integer or out of range incrbyfloat key 指定值 # 自减1 decr key # 自减指定值 decrby key 指定值 设置数据时效性(存活时间) 多应用于控制操作,比如投票,获取验证码等 # 一下操作需注意,如果有多次操作,最后操作会覆盖之前的操作 # 秒级 setex key seconds value # 毫秒级 psetex key milliseconds value 注意事项 数据操作返回值 ​ 1.运行结果: 0 : 失败 1 : 成功 ​ 2.运行后结果值: 比如strlen,incr等 数据未获取到 ​ (nil) : 等同于null 数据最大存储量 : 521MB 数值最大范围: 9223372036854775807(Java中Long类型最大值Long.MAX_VALUE) key的设置约定: 表名:主键名:主键值:字段名:字段值 set user:id:1001:name:tao:age 18 hash类型 可对一系列存储的数据进行编组,方便管理,典型应用存储对象信息;底层使用哈希表结构实现数据存储 hash存储结构优化: 如果field数量较少,存储结构优化为类数组结构,反之采用HashMap结构 也就是一个key对应一片存储空间,存储空间又可以存储多个 field -> value 的键值对 基本操作 # 添加/修改数据 hset key field value # 获取数据(指定字段) hget key field # 获取所有 field和value hgetall key # 使用hgetall命令的返回值,会将字段名和值都打印出来 1) \"name\" 2) \"tao\" 3) \"age\" 4) \"18\" # 删除数据 hdel key # 获取多个字段对应的值 hmget key field1 field2 .. # 添加/修改多个字段 hmset key field1 value1 field2 value2 # 获取哈希表中子段的数量 hlen key # 查看指定字段在哈希表中是否存在 hexists key field 扩展操作 # 获取哈希表中所有字段名或字段值 hkeys key hvals key # 根据指定字段增加指定范围的值 hincrby key field increment hincrbyfloat key field increment # 如果hash表中字段不存在,添加否则不做操作 hsetnx key field value 注意事项 ​ hash类型下的value只能存储字符串,不允许存储其他数据类型,数据不存在返回 (nil) ​ 每个hash可以存储 2的32次方-1 个键值对 ​ hash类型贴近对象的数据存储形式,可以灵活的添加删除属性,但其设计初衷不是为了存储对象,不可滥用 ​ hgetall操作在内部field过多时,整体遍历效率会很低,有可能成为数据访问的瓶颈 list类型 存储多个数据,且通过数据可以体现进入顺序,底层使用双向链表存储 基本操作 其中的l和r分别代表left和right,可以理解为可以分别从左边和右边添加获取并移除数据,而lrange只能从左边开始获取,按照相同方向则是栈操作,不同方向则是对列操作 # 添加/修改数据 # 从左添加数据 lpush key value1 value2 ... # 从右添加数据 rpush key value1 value2 ... # 获取数据 start和stop代表索引 0 -1 可以看到所有数据 -代表倒数第几个 lrange key start stop # 获取指定索引的数据 lindex key index # 获取并移除数据 lpop key rpop key 扩展操作 阻塞获取 类似于消息队列功能 # 设置指定时间(秒)内获取,如果有值立刻返回 blpop key1 key2 ... timeoutr brpop key1 key2 ... timeout # 返回内容 数据所存在的集合 数据值 所用时间 1) \"list2\" 2) \"10\" (15.29s) # 删除指定元素 count: 删除符合value值的元素个数 lrem key count value 注意事项 list中保存的数据都是string类型的,最多2的32次方-1个元素 list具有索引的概念,通常以对列或栈的方式操作 set类型 存储大量数据,在查询方便有更高的效率,与hash结构完全相同,仅存储键,不存储值,其值不允许重复 基本操作 # 添加数据 sadd key member1 member2 ... # 获取全部数据 smembers key # 删除数据 srem key member1 member2 ... # 获取集合元素数量 scard key # 判断集合是否包含指定元素 sismember key member 扩展操作 # 随机获取集合中指定数量的数据,count可选,不选默认为1 srandmember key count # 随机获取集合中某个数据并将其移出集合 spop key 求两个集合的交、并、差集 交集: 两个集合中相同的元素 并集: 两个集合去重后的所有元素 差集: 一个集合在另一个集合中没有的元素 sinter key1 [key2] sunion key1 [key2] sdiff key1 [key2] 求两个集合的交、并、差集并存储到指定集合中 sinterstore destination key1 [key2] sunionstore destination key1 [key2] sdiffstore destination key1 [key2] 将指定数据从源集合移动到目标集合中 smove source destination member 注意事项 set类型不允许数据重复,如果添加的数据在set中已存在,将只保留一份 虽然与hash结构相同,但无法启用hash中存储值的空间 sorted_set类型 基于set的存储结构添加可排序字段(score),可对数据进行整体排序,方便展示 基本操作 # 添加/修改数据 score: 用于排序的数值 zadd key score1 member1 [score2 member2] # 获取全部数据(根据score排序) withscores: 可选,返回的结果是否带score值 zrange key start stop [withscores] # 倒序 zrevrange key start stop [withscores] # 删除数据 zrem key member [member...] # 按条件获取数据 zrangebyscore key min max [withscores] [limit offset count] zrevrangebyscore key max min [withscores] # 条件删除数据 zremrangebyrank key start stop zremrangebyscore key min max 注意 ​ min与max用于限定搜索查询的条件,包含min和max ​ start与stop用于限定查询范围,作用于索引,表示开始和结束索引,包含start和stop ​ offset与count用于限定查询范围,作用域查询结果,表示开始位置和数据总量 # 获取集合数据总量 zcard key zcount key min max # 集合交、并集操作(可以使用help命令查看更多用法) # 求交集和并集会将相同的member的值相加(需注意) zinterstore destination numkeys key [key...] zunionstore destination numkeys key [key...] zinterstore | zunionstore destination numkeys key [key...] [weights num...] [aggregate max | min] 注意 ​ numkeys: 代表参与集合的个数 ​ weights: 代表每个集合中的元素在参与aggregate前的权重值,元素与权重的乘积(默认为1) ​ aggregate: 可选max或min,选出经过元素与权重的乘积后最大或最小的元素(分组) 扩展操作 # 获取数据对应的索引(排名),从0开始 zrank key member zrevrank key member # score值获取与修改 zscore key member zincrby key increment member 注意事项 ​ score数据存储空间为64位,score保存的数据可以是一个双精度的double值,基于双精度浮点数的特征,可能会出现数据精度丢失,sorted_set底层基于set,多个相同的数据添加时会覆盖score值(返回0,表示key虽然没有变,但是对应的score值变成了最后一次添加的score值) 通用操作指令 key通用操作 # 删除指定key del key # 获取key是否存在 exists key # 获取key的类型 type key # 为指定key设置有效期 expire key seconds # 毫秒 pexpire key milliseconds # 时间戳 expireat key timestamp pexpireat key milliseconds-timestamp # 获取key的有效时间 返回值: -1代表持久化 -2代表已过期 如果设置了有效期,但未过期显示剩余存活时间 ttl key pttl key # 切换key从时效性转换为永久性 persist key # 查询key keys pattern 匹配规则 ​ *: 匹配任意数量的任意符号 ​ ?: 匹配一个任意符号 ​ []: 匹配任意一个指定符号 比如 # 所有key keys * # 以zz开头的key keys zz* # 以zz结尾的key keys *zz # 开头以任意三个字符开头,以tao结尾,也就是key总长要在6 keys ???tao # 中间任意匹配其中一个字符,以ao结尾 keys t[aqz]ao # 为key改名 # 如果存在同名的key,覆盖原来的key中的数据 rename key newKey # 如果指定name的key不存在时执行 renamenx key newKey # 对所有key排序,key对应的类型必须是一个list或set sort key db通用操作 redis分为0-15数据库 # 切换数据库 select number # 测试是否与服务器连通,成功返回pong ping # 打印字符串 echo message # 数据移动 move key db # 当前库key数量 dbsize # 数据清除 flushdb flushall Java操作Redis 创建maven项目,导入依赖 redis.clients jedis 2.9.0 创建工具类 public class JedisUtils { private static JedisPool jp = null; private static String host = null; private static int port; private static int maxTotal; private static int maxIdle; static { ResourceBundle rb = ResourceBundle.getBundle(\"redis_zh_CN\"); host = rb.getString(\"redis.host\"); port = Integer.parseInt(rb.getString(\"redis.port\")); maxTotal = Integer.parseInt(rb.getString(\"redis.maxTotal\")); maxIdle = Integer.parseInt(rb.getString(\"redis.maxIdle\")); JedisPoolConfig jpc = new JedisPoolConfig(); jpc.setMaxTotal(maxTotal); jpc.setMaxIdle(maxIdle); jp = new JedisPool(jpc,host,port); } public static Jedis getJedis(){ return jp.getResource(); } public static void main(String[] args){ JedisUtils.getJedis(); } } 在resource文件夹下创建redis_zh_CN.properties配置文件 redis.host=127.0.0.1 redis.port=6379 redis.maxTotal=30 redis.maxIdle=10 public static void main(String[] args) { // 获取连接 Jedis jedis = JedisUtils.getJedis(); // 测试连接 // Jedis操作redis的方法与命令行基本一致 String ping = jedis.ping(); System.out.println(ping); // 关闭连接 jedis.close(); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"redis/redis高级.html":{"url":"redis/redis高级.html","title":"Redis进阶","keywords":"","body":"进阶安全持久化RDBAOFRDB和AOF持久化的比较事务事务锁解决死锁问题删除策略高级数据类型进阶 安全 # 查看是否设置密码 CONFIG get requirepass # 设置密码 CONFIG set requirepass \"password\" # 使用密码登录 AUTH password # 获取所有配置项 CONFIG GET * # 查看当前客户端配置 info 持久化 分为RDB和AOF两种持久化方式 RDB 使用以下两条命令后会在指定的文件夹中生成指定文件名称的文件用于数据持久化 # 立刻执行 save # 后台执行 bgsave 注意事项 save指令的执行会阻塞当前Redis服务器,知道当前RDB完成为止,有可能造成长时间阻塞 AOF 使用日志的方式存储,通过配置开自动开启,记录的是执行的命令,可以通过重写的方式缩小文件大小 RDB和AOF持久化的比较 事务 取消事务 不会针对某个事务,执行命令后会清除存在的所有事务 discard 事务锁 watch必须在事务前执行 解决死锁问题 删除策略 定时: 设置一个定时器,expire(设置了定时元素的存储空间,存储元素的地址值和过期时间)中的元素到达过期时间立刻删除 惰性: 当元素到达过期时间如果服务器压力过大不会立刻删除,等到在获取时再进行删除 定期: 每隔一段时间对内存中的数据进行随机抽查,把抽查到的过期数据删除 高级数据类型 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"kafka/kafka入门.html":{"url":"kafka/kafka入门.html","title":"kafka入门","keywords":"","body":"kafka入门是什么几种常见的消息队列区别消息队列解决了什么问题（好处）异步操作解耦合流量削峰为什么选择kafka推送VS拉取推送(push)拉取(pull)Kafka的解决方案kafka的名词解释小结消息队列两种传递消息的模式，以及优缺点点对点模式发布/订阅模式消费者与消费者组消费者组消费者与消费者组的关系acksack自动提交手动提交kafka java api使用springboot结合Kafkapom.xmlyml创建topicack配置生产者消费者死信+重试kafka入门 是什么 ​ 是由Apache开源，使用scala和java编写的一个分布式的基于发布/订阅模式的消息队列。 ​ kafka使用了长轮询的方式来拉取消息，避免了在消费者端性能瓶颈问题，采用拉取消息的方式可以根据消费者的性能灵活消费消息，但此方式也有缺点，即消费者需要每隔一段时间进行拉取，如果时间过长，会出现消息延迟，如果时间过短进行频繁请求对kafka的压力也会过大。 几种常见的消息队列区别 特性 ActiveMQ RabbitMQ Kafka RocketMQ 所属社区/公司 Apache Mozilla Public License Apache Apache/Ali 成熟度 成熟 成熟 成熟 比较成熟 生产者-消费者模式 支持 支持 支持 支持 发布-订阅 支持 支持 支持 支持 API完备性 高 高 高 低（静态配置） 多语言支持 支持JAVA优先 语言无关 支持，JAVA优先 支持 单机呑吐量 万级（最差） 万级 十万级 十万级（最高） 消息延迟 - 微秒级 毫秒级 - 可用性 高（主从） 高（主从） 非常高（分布式） 高 消息丢失 - 低 理论上不会丢失 - 消息重复 - 可控制 理论上会有重复 - 事务 支持 不支持 支持 支持 文档的完备性 高 高 高 中 首次部署难度 - 低 中 高 消息队列解决了什么问题（好处） 异步操作 ​ 假如一个服务调用另一个服务时间比较长时，可以使用消息队列进行异步处理，将请求参数发送至消息队列，再由服务获取消息队列中的数据进行慢慢消化。（一些没有强制关联的操作，也可以使用消息队列来加快响应速度，比如注册时的发送邮箱确认，注册后可以立即给出响应，发邮件的操作可以交给消息队列去做）。 解耦合 ​ 一个模块的处理需要强依赖另一个模块时，假如被依赖的模块出现故障，自然也会影响调用模块的运行，引入消息队列后，可以先将请求参数保存至消息队列中，被依赖模块从消息队列中获取请求参数。 流量削峰 ​ 比如秒杀下订单时会有大量用户在同一时间请求服务，可能会导致服务因此宕机，可以引入消息队列，对请求进行排队和限制，然后慢慢消化。 为什么选择kafka ​ 一些大型网站需要通过日志分析用户的使用行为，日志数据通常都是大量的，而kafka的优点则是高吞吐量、高性能并且支持分布式，可以从我们G01系统中看到有许多统计的地方，这些数据也大多是用户日常使用的行为日志，通过使用kafka结合es来对这些日志数据进行分析统计，可以更直观的感受用户的使用习惯和系统的功能性。 推送VS拉取 推送(push) ​ push模式下消息的实时性更高，对于消费者使用来说更简单，反正有消息来了就会推过来。 但push模式很难适应消费速率不同的消费者，因为消息发送速率是由服务端决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成消费者来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。 拉取(pull) ​ pull模式主动权就在消费者身上了，消费者可以根据自身的情况来发起拉取消息的请求。假设当前消费者觉得自己消费不过来了，它可以根据一定的策略停止拉取，或者间隔拉取都行。 消息延迟，因为是消费者是主动拉取消息，它并不知道消息的存在与否，于是只能不停的去拉取，但又不能很频繁，太频繁对服务端的压力也很大。 Kafka的解决方案 ​ 首先Kafka采用了pull拉取消息的模式，消费者采用长轮询的方式去服务端拉取消息时定义了一个超时时间，如果有马上返回消息，没有的话消费者等着直到超时，然后再次发起拉消息的请求。 kafka的名词解释 producer: 生产者，生产者生产消息数据发送至broker。 consumer: 消费者，消费者从broker中读取数据做处理。 broker: 已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker)，反言之每一个Broker就是一个Kafka的实例。 topic: 是kafka下消息的类别，每一类消息都可以称为一个topic，是逻辑上的概念，用来区分、隔离不同的消息，开发时大多只需要关注消息存到了哪个topic，需要从哪个topic中取数据即可。 partition: 分区，为了负载均衡，将同一个Topic分为多个分区分别存储在集群中不同的broker上，每一个Topic可以有多个分区，但一个分区只属于一个主题，同一个主题中不同分区之间的数据不一样。这样做的好处是当存储不足时可以通过增加分区实现横向扩展。分区数也不是越多越好，配置为broker节点的整数倍即可。 offset: 写入topic的消息会被平均分配到每一个分区，到达分区后会落实到一个文件中，消息在文件的位置就是offset(偏移量)，是消息在分区中的唯一标识。（开发时可以灵活的使用该属性进行消息消费，可以指定消费者从指定的offset开始消费） replicas：分区的副本，保障了分区的高可用， 每个分区在每个broker上最多只有一个副本。 Leader： 分区中的一个角色， Producer 和 Consumer 只与 Leader 交互； Follower： 分区中的一个角色，从 Leader 中复制数据，作为它的副本，同时一旦某 Leader 挂掉，便会从它的所有 Follower 中选举出一个新的 Leader 继续提供服务； Zookeeper在Kafka中的作用： Kafka集群之间并不直接联系，而是将各自节点信息注册到Zookeeper，由ZK进行统一管理。 小结 ​ 多个broker组成一个Kafka集群，broker之间通过Zookeeper进行通讯，一个broker中可以有多个topic，生产者可以将消息发送至topic的不同分区中，同一个topic的不同分区内的消息不一样，每个分区可以有最多N（取决与broker的数量）个副本保存在不同的broker中。 ​ 消费者从订阅的topic中拉取消息，可以通过增加消费者形成消费者组来提高消费者的消费速度。 ​ 每个分区中有两个角色，类似主从，生产者和消费者只与leader节点交互，leader节点出故障后，会从所有的follower节点中选举出一个新的leader节点继续提供服务。 消息队列两种传递消息的模式，以及优缺点 点对点模式 ​ 消息生产者将消息发送到消息队列中，消息消费者从消息队列中取出并且消费数据，消息被消费后，消息队列中不在存储，所有消息消费者不可能接受到已经被消费的消息。 ​ 消费者在接收消息后需要向队列应答成功，以便消息队列删除该消息。 ​ 类似客服，一个客户打客服电话，同时只能有一个客服人员进行解答。 发布/订阅模式 ​ 消息生产者将消息发布到topic中，可以有多个消费者订阅该topic进行消费，达到一对多的效果。 ​ 类似报纸，出版社发行报纸后可以有很多人去购买阅读 消费者与消费者组 消费者组 ​ 当生产者向 Topic 写入消息的速度超过了消费者（consumer）的处理速度，导致大量的消息在 Kafka 中淤积，此时需要对消费者进行横向伸缩，用多个消费者从同一个主题读取消息，对消息进行分流。 消费者与消费者组的关系 ​ Kafka 的消费者都属于消费者组（consumer group）。一个组中的 consumer 订阅同样的 topic，每个 consumer 接收 topic 一些分区（partition）中的消息。同一个分区不能被一个组中的多个 consumer 消费。换句话说：每一个分区内的消息，只能被同消费组中的一个消费者消费。 消费者组(consumer group) ： ​ 当生产者向 Topic 写入消息的速度超过了消费者（consumer）的处理速度，导致大量的消息在 Kafka 中淤积，此时需要对消费者进行横向伸缩，用多个消费者从同一个主题读取消息，对消息进行分流。 ​ Kafka 的消费者都属于消费者组（consumer group）。一个组中的 consumer 订阅同样的 topic，每个 consumer 接收 topic 一些分区（partition）中的消息。同一个分区不能被一个组中的多个 consumer 消费。 一个topic有四个分区，随着消费者组中的消费者数量增加时，该topic分区中数据被读取的情况 acks 这里的acks指的是producer的消息发送确认机制。 ack有三个值可以选择 0，1，-1（all） ack = 0，就是kafka生产端发送消息之后，不管broker的副本有没有成功收到消息，在producer端都会认为是发送成功了，这种情况提供了最小的延迟，和最弱的持久性，如果在发送途中leader异常，就会造成数据丢失。 ack = 1，是kafka默认的消息发送确认机制，此机制是在producer发送数据成功，并且leader接收成功并确认后就算消息发送成功，但是这种情况如果leader接收成功了，但是follwer未同步时leader异常，就会造成上位的follwer丢失数据，提供了较好的持久性和较低的延迟性。 ack =-1,也可以设置成all,此机制是producer发送成功数据，并且leader接收成功，并且follwer也同步成功之后，producer才会发送下一条数据。 ack 自动提交 ​ 最简单的方式是消费者自动提交偏移量。如果 enable.auto.commit 设为 true，那 么每过一定时间间隔，消费者会自动把从 poll() 方法接收到的最大偏移量提交上去。提交时间间隔由 auto.commit.interval.ms 控制，默认是5s。 ​ 缺点： 假如在提交时间间隔内发生了分区再均衡（比如topic添加分区，会将消息进行重新分配），会发生消息重新被消费的情况，所以只能通过调小提交时间间隔来更频繁的提交偏移量，但也无法完全避免。 手动提交 同步提交 ​ 把 auto.commit.offset 设为false，使用commitSync()方法提交偏移量最简单也最可靠，该方法会提交由 poll()方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。需在poll()的所有数据处理完成后再调用。只要没有发生不可恢复的错误，commitSync() 会一直尝试直至提交成功。如果提交 失败会抛出 CommitFailedException 异常。 异步提交 ​ 手动提交有一个不足之处，在 broker 对提交请求作出回应之前，应用程序会阻塞，这会影响应用程序的吞吐量。可以使用异步提交的方式，不等待 broker 的响应。 ​ 异步提交是不能重试的，因为重试的时候，待提交的位移很可能是一个过期的位移（也就是偏移量值大的有可能会比偏移量值小的先提交）。对于失败或者成功后续的处理，可以在定义的回调函数中进行处理。 ​ 方法： consumer.commitAsync() kafka java api使用 pom.xml org.apache.kafka kafka-clients 2.4.1 org.apache.commons commons-io 1.3.2 org.slf4j slf4j-log4j12 1.7.6 log4j log4j 1.2.16 生产者 package producer.one; import org.apache.kafka.clients.admin.AdminClient; import org.apache.kafka.clients.admin.KafkaAdminClient; import org.apache.kafka.clients.admin.NewTopic; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import java.util.Arrays; import java.util.Properties; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; public class KafkaProducerOne { private static final String TOPIC_TWO = \"topic_two\"; public static void main(String[] args) { // 1. 创建用于连接Kafka的Properties配置 Properties props = new Properties(); // 指定Kafka集群的地址，可以设置一个或多个，多个使用英文逗号隔开 props.put(\"bootstrap.servers\", \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\"); // 提供了一个常量类，其中有kafka生产者所需要的配置以及说明 // props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\"); // 0: 如果设置为零，则生产者根本不会等待来自服务器的任何确认。该记录将立即添加到套接字缓冲区并被视为已发送。这种情况下不能保证服务端已经收到记录,retries 配置也不会生效（因为客户端一般不会知道任何失败） // 1: 这意味着领导者会将记录写入其本地日志，但会在不等待所有追随者的完全确认的情况下做出响应。在这种情况下，如果领导者在确认记录后但在追随者复制它之前立即失败，那么记录将丢失。 // 2: 这意味着领导者将等待完整的同步副本集来确认记录。这保证了只要至少一个同步副本保持活动状态，记录就不会丢失。这是最有力的保证。这相当于 acks=-1 设置。 props.put(\"acks\", \"all\"); // 与生产者客户端 KafkaProducer 中的 key.serializer 和 value.serializer 参数对应。消费者从 broker 端获取的消息格式都是字节数组（byte[]）类型，所以需要执行相应的反序列化操作才能还原成原有的对象格式。 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 手动创建Topic，如果没有手动创建也会自动创建 AdminClient create = KafkaAdminClient.create(props); // 创建一个拥有两个分区，一个副本的Topic create.createTopics(Arrays.asList(new NewTopic(TOPIC_TWO, 2, (short) 1))); create.close(); // 2. 创建一个生产者对象KafkaProducer KafkaProducer producer = new KafkaProducer(props); // 3. 调用send发送1-10消息到指定Topic for (int i = 0; i future = producer.send(new ProducerRecord(TOPIC_TWO, null, i + \"\")); // 调用一个Future.get()方法等待响应 RecordMetadata record = future.get(); System.out.printf(\"topic = %s ,partition = %d,offset = %d%n\", record.topic(), record.partition(), record.offset()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } // 5. 关闭生产者 producer.close(); } } 消费者 package consumer; import constans.Constans; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.time.Duration; import java.util.Collections; import java.util.Properties; public class KafkaConsumerOne { private static final String TOPIC_TWO = \"topic_two\"; public static void main(String[] args) { // 1. 创建用于连接Kafka的Properties配置 Properties props = new Properties(); // 同时提供了一个Kafka消费者的配置常量类 // props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\"); props.setProperty(\"bootstrap.servers\", \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\"); // 消费者隶属的消费组的名称 props.setProperty(\"group.id\", \"topic_two-group_one\"); // 是否自动提交 true:是 false:否 props.setProperty(\"enable.auto.commit\", \"true\"); // 定时提交，自动提交时间间隔 props.setProperty(\"auto.commit.interval.ms\", \"1000\"); props.setProperty(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.setProperty(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // 2. 创建一个消费者对象KafkaConsumer KafkaConsumer consumer = new KafkaConsumer<>(props); // 3. 订阅指定的Topic,如果参数为空代表取消订阅 consumer.subscribe(Collections.singletonList(TOPIC_TWO)); while(true) { // 4. 参数毫秒，每多少毫秒从服务器开始拉取数据， ConsumerRecords records = consumer.poll(Duration.ofMillis(100)); records.forEach(record -> { System.out.printf(\"topic = %s ,partition = %d,offset = %d, key = %s, value = %s%n\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); }); } } } 控制台输出 可以看到该topic中有两个分区，并且不同分区之间数据不一样 topic = topic_two ,partition = 1,offset = 105, key = null, value = 0 topic = topic_two ,partition = 0,offset = 105, key = null, value = 1 topic = topic_two ,partition = 1,offset = 106, key = null, value = 2 topic = topic_two ,partition = 1,offset = 107, key = null, value = 4 topic = topic_two ,partition = 1,offset = 108, key = null, value = 6 topic = topic_two ,partition = 0,offset = 106, key = null, value = 3 topic = topic_two ,partition = 0,offset = 107, key = null, value = 5 topic = topic_two ,partition = 1,offset = 109, key = null, value = 8 topic = topic_two ,partition = 0,offset = 108, key = null, value = 7 topic = topic_two ,partition = 0,offset = 109, key = null, value = 9 在 topic: topic_two 消费者组中再添加一个消费者后打印 两个消费者设置相同的group.id即可 消费者1 topic = topic_two ,partition = 0,offset = 110, key = null, value = 0 topic = topic_two ,partition = 0,offset = 111, key = null, value = 2 topic = topic_two ,partition = 0,offset = 112, key = null, value = 4 topic = topic_two ,partition = 0,offset = 113, key = null, value = 6 topic = topic_two ,partition = 0,offset = 114, key = null, value = 8 消费者2 topic = topic_two ,partition = 1,offset = 110, key = null, value = 1 topic = topic_two ,partition = 1,offset = 111, key = null, value = 3 topic = topic_two ,partition = 1,offset = 112, key = null, value = 5 topic = topic_two ,partition = 1,offset = 113, key = null, value = 7 topic = topic_two ,partition = 1,offset = 114, key = null, value = 9 可以看到分片中的数据被均匀的分配到了两个消费者中 手动提交 // 同步提交：方法本身是阻塞的，如果没消费一条进行一次位移提交，势必会拉低整体性能。 consumer.commitSync(); // 异步提交：异步提交是不能重试的，因为重试的时候，待提交的位移很可能是一个过期的位移。 consumer.commitAsync(); // 异步提交：回调 consumer.commitAsync(new OffsetCommitCallback() { // 提交完成时回回调此函数 public void onComplete(Map offsets, Exception e) { for (Map.Entry entry : offsets.entrySet()) { TopicPartition key = entry.getKey(); OffsetAndMetadata value = entry.getValue(); System.out.println(\"topic:\"+ key.topic()+\" partition:\"+key.partition() +\" offset:\"+value.offset() +\" metadata:\"+value.metadata()); } if (e != null) System.out.println(\"Commit failed for offsets \"+ offsets); } }); springboot结合Kafka pom.xml org.springframework.kafka spring-kafka org.projectlombok lombok cn.hutool hutool-all 5.7.13 yml server: port: 8088 spring: kafka: # kafka集群地址 bootstrap-servers: '192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092' producer: acks: all # 键序列化方式 key-serializer: org.apache.kafka.common.serialization.StringSerializer # # 值序列化方式 value-serializer: org.apache.kafka.common.serialization.StringSerializer # # 消息重试次数 # retries: 0 consumer: # 消费者组id # group-id: key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer # 新增一个消费者组时会从头（第一条消息）开始消费，无论之前是否partition提交过offset # auto-offset-reset: earliest # 从最新的消息开始消费 auto-offset-reset: latest 创建topic package top.taoqz.config; import org.apache.kafka.clients.admin.NewTopic; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.kafka.config.TopicBuilder; @Configuration public class KafkaConfig { @Bean(\"bootTopic_one\") public NewTopic bootTopicOne() { return TopicBuilder.name(\"bootTopic_one\") .partitions(10) .replicas(1) .compact() .build(); } @Bean(\"bootTopic_two\") public NewTopic bootTopicTwo() { return TopicBuilder.name(\"bootTopic_two\") .partitions(3) .replicas(1) .compact() .build(); } } ack配置 package top.taoqz.config; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.common.serialization.StringDeserializer; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory; import org.springframework.kafka.core.DefaultKafkaConsumerFactory; import org.springframework.kafka.listener.ContainerProperties; import org.springframework.stereotype.Component; import java.util.HashMap; import java.util.Map; @Component public class KafkaAckConfig { private static final Logger log = LoggerFactory.getLogger(KafkaAckConfig.class); @Value(\"${spring.kafka.bootstrap-servers}\") private String servers; private Map consumerProps() { Map props = new HashMap<>(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\"); props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, \"15000\"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); return props; } @Bean(\"ackContainerFactory\") public ConcurrentKafkaListenerContainerFactory ackContainerFactory() { ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory(consumerProps())); factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE); factory.setConsumerFactory(new DefaultKafkaConsumerFactory(consumerProps())); return factory; } } 生产者 ListenableFuture send = kafkaTemplate.send(\"bootTopic_one\", partition, \"myKey\", message); 消费者 package top.taoqz.consumer; import lombok.extern.slf4j.Slf4j; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; import org.springframework.context.annotation.Bean; import org.springframework.kafka.annotation.KafkaListener; import org.springframework.kafka.annotation.TopicPartition; import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory; import org.springframework.kafka.core.ConsumerFactory; import org.springframework.kafka.core.KafkaTemplate; import org.springframework.kafka.listener.DeadLetterPublishingRecoverer; import org.springframework.kafka.listener.SeekToCurrentErrorHandler; import org.springframework.kafka.support.Acknowledgment; import org.springframework.stereotype.Component; import org.springframework.util.backoff.BackOff; import org.springframework.util.backoff.FixedBackOff; @Slf4j @Component public class KafkaConsumer { /** * 默认自动提交 * * @param record */ @KafkaListener(groupId = \"hello1\", topicPartitions = @TopicPartition( topic = \"bootTopic_one\", partitions = {\"1\", \"3\", \"5\", \"7\", \"9\"} )) public void processMessage(ConsumerRecord record) { System.out.println(\"consumer1:\" + record); } /** * 不会自动提交，需手动调用 ack.acknowledge() 方法 * * @param record * @param ack */ @KafkaListener(topics = \"bootTopic_one\", groupId = \"hello2\", containerFactory = \"ackContainerFactory\", topicPartitions = @TopicPartition( topic = \"bootTopic_one\", partitions = {\"0\", \"2\", \"4\", \"6\", \"8\"} // topic = \"bootTopic_one\", partitions = {\"5\"} )) public void processMessage2(ConsumerRecord record, Acknowledgment ack) { System.out.println(\"consumer2:\" + record); ack.acknowledge(); // ack.nack(60000); } // @Autowired // private KafkaTemplate template; @Bean public ConcurrentKafkaListenerContainerFactory kafkaListenerContainerFactory( ConcurrentKafkaListenerContainerFactoryConfigurer configurer, ConsumerFactory kafkaConsumerFactory, KafkaTemplate template) { ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory<>(); configurer.configure(factory, kafkaConsumerFactory); // 创建 FixedBackOff 对象 设置重试间隔 10秒 次数为 3次 BackOff backOff = new FixedBackOff(5 * 1000L, 3L); factory.setErrorHandler(new SeekToCurrentErrorHandler(new DeadLetterPublishingRecoverer(template), backOff)); return factory; } /** * 方法抛出异常,经过重试后仍不能消费时进入死信队列 * * @param record */ // @KafkaListener(topics = {\"bootTopic_two\"}, groupId = \"bootTopic_two\") public void processMessageBootTopicTwo(ConsumerRecord record) { System.out.println(\"重试。。。\"); throw new RuntimeException(); } /** * 死信队列 * 死信队列的命名方式： 队列名称 = \".DTL\" * * @param record */ // @KafkaListener(topics = {\"bootTopic_two.DLT\"}, groupId = \"bootTopic_two.DLT\") public void bootTopicTwoDLT(ConsumerRecord record) { System.out.println(\"死信队列接收消息:\" + record); } } 死信+重试 package top.taoqz.consumer; import lombok.extern.slf4j.Slf4j; import org.springframework.kafka.annotation.DltHandler; import org.springframework.kafka.annotation.KafkaListener; import org.springframework.kafka.annotation.RetryableTopic; import org.springframework.kafka.support.KafkaHeaders; import org.springframework.messaging.handler.annotation.Header; import org.springframework.retry.annotation.Backoff; import org.springframework.stereotype.Component; @Slf4j @Component public class KafkaConsumerDeadLetter { /** * attempts 最多重试次数（成功+失败的总次数） * * @param in * @param topic * @param offset */ @RetryableTopic(attempts = \"3\", backoff = @Backoff(delay = 2_000, maxDelay = 10_000, multiplier = 2)) @KafkaListener(id = \"bootTopic_two_group\", topics = \"bootTopic_two\") public void listen(String in, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic, @Header(KafkaHeaders.OFFSET) long offset) { log.error(\"Received: {} from {} @ {}\", in, topic, offset); throw new RuntimeException(\"failed.......\"); } @DltHandler public void listenDlt(String in, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic, @Header(KafkaHeaders.OFFSET) long offset) { log.error(\"DLT Received: {} from {} @ {}\", in, topic, offset); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"kafka/kafka搭建集群.html":{"url":"kafka/kafka搭建集群.html","title":"kafka搭建集群","keywords":"","body":"kafka搭建集群环境kafka下载地址解压安装包目录结构安装配置创建日志文件夹创建zookeeper数据文件夹在zookeeper数据文件夹中创建文件修改zookeeper配置文件修改kafka配置文件启动启动zookeeper启动kafka修改后完整配置文件zookeeper.propertiesserver.properties可视化工具防火墙基本命令查看kafka topic列表创建topic查看topic向topic中发送消息（生产消息）从topic中获取消息（消费消息）kafka搭建集群 环境 CentOS Linux release 7.3.1611 (Core) java version \"1.8.0_321\" kafka_2.12-2.7.0(使用了内置的zookeeper) kafka下载地址 https://kafka.apache.org/downloads 解压安装包 tar zxvf kafka_2.12-2.7.0.tgz 目录结构 目录名称 说明 bin kafka所有的执行脚本 config kafak所有的配置文件(包括zookeeper) logs kafka所有日志文件 libs kafka所依赖的所有jar包 site-docs 使用说明文档 安装配置 创建日志文件夹 目的：因为日志默认是存在/tmp 下，重新启动后会消失 kafka日志文件：/home/app/kafka/kafka_2.12-2.7.0/log/kafka zookeeper日志文件：/home/app/kafka/kafka_2.12-2.7.0/log/zookeeper 创建zookeeper数据文件夹 /home/app/kafka/kafka_2.12-2.7.0/zkData 在zookeeper数据文件夹中创建文件 # 其中 1 则是每个服务器的id,不可重复 cd /home/app/kafka/kafka_2.12-2.7.0/zkData echo 1 > myid 修改zookeeper配置文件 zookeeper.properties 修改或添加项 # 数据目录，对应新创建的数据目录 dataDir=/home/app/kafka/kafka_2.12-2.7.0/zkData # 日志目录，对应新创建的目录 dataLogDir=/home/app/kafka/kafka_2.12-2.7.0/log/zookeeper # 注释该行 # maxClientCnxns=0 # 设置连接参数，添加如下配置　　　　 # 为zk的基本时间单元，毫秒 tickTime=2000 # Leader-Follower初始通信时限 tickTime*10 initLimit=10 # Leader-Follower同步通信时限 tickTime*5 syncLimit=5 # 设置broker Id的服务地址（修改对应服务器ip即可） # 其中server.0 server.1 server.2 后的 0 1 2 也是对应zookeeper数据目录zkData下myid文件中的内容 server.0=192.168.92.128:2888:3888 server.1=192.168.92.129:2888:3888 server.2=192.168.92.130:2888:3888 修改kafka配置文件 server.properties 修改或添加项 # broker 的全局唯一编号，不能重复，对应 zkData中myid的值 broker.id=0 # 配置监听，修改位本机ip listeners=PLAINTEXT://192.168.92.128:9092 advertised.listeners=PLAINTEXT://192.168.92.128:9092 # 修改输出日志文件位置 log.dirs=/home/app/kafka/kafka_2.12-2.7.0/log/kafka # 配置三台zookeeper地址 zookeeper.connect=192.168.92.128:2181,192.168.92.129:2181,192.168.92.130:2181 启动 需要先启动zookeeper,等zookeeper启动完成后启动kafka 启动zookeeper # 当前命令行启动 ./bin/zookeeper-server-start.sh ./config/zookeeper.properties & # 后台启动 nohup ./bin/zookeeper-server-start.sh ./config/zookeeper.properties > ./log/zookeeper/zookeeper.log 2>1 & 启动kafka # 当前命令行启动 ./bin/kafka-server-start.sh ./config/server.properties & # 后台启动 nohup ./bin/kafka-server-start.sh ./config/server.properties > ./log/kafka/kafka.log 2>1 & 修改后完整配置文件 zookeeper.properties # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # the directory where the snapshot is stored. dataDir=/home/app/kafka/kafka_2.12-2.7.0/zkData dataLogDir=/home/app/kafka/kafka_2.12-2.7.0/log/zookeeper # the port at which the clients will connect clientPort=2181 # disable the per-ip limit on the number of connections since this is a non-production config # maxClientCnxns=0 # Disable the adminserver by default to avoid port conflicts. # Set the port to something non-conflicting if choosing to enable this admin.enableServer=false # admin.serverPort=8080 # 设置连接参数，添加如下配置　　　　 # 为zk的基本时间单元，毫秒 tickTime=2000 # Leader-Follower初始通信时限 tickTime*10 initLimit=10 # Leader-Follower同步通信时限 tickTime*5 syncLimit=5 # 设置broker Id的服务地址 server.0=192.168.92.128:2888:3888 server.1=192.168.92.129:2888:3888 server.2=192.168.92.130:2888:3888 server.properties # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 #listeners=PLAINTEXT://:9092 listeners=PLAINTEXT://192.168.92.128:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for \"listeners\" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # 配置监听，修改位本机ip advertised.listeners=PLAINTEXT://192.168.92.128:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files log.dirs=/home/app/kafka/kafka_2.12-2.7.0/log/kafka # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics \"__consumer_offsets\" and \"__transaction_state\" # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log unless the remaining # segments drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 配置三台zookeeper地址 zookeeper.connect=192.168.92.128:2181,192.168.92.129:2181,192.168.92.130:2181 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=18000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 可视化工具 zookeeper https://github.com/vran-dev/PrettyZoo/releases kafka kafkaTool 防火墙 解决虚拟机之间连接不上的问题 # 查看防火墙状态 firewall-cmd --state # 停止firewall systemctl stop firewalld.service # 禁止firewall开机启动 systemctl disable firewalld.service 基本命令 查看kafka topic列表 ./bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --list 创建topic ./bin/kafka-topics.sh --create --topic myTopicOne --bootstrap-server 192.168.92.128:9092 查看topic ./bin/kafka-topics.sh --describe --topic consoleTopic --bootstrap-server 192.168.92.128:9092 向topic中发送消息（生产消息） ./bin/kafka-console-producer.sh --topic myTopicOne --bootstrap-server 192.168.92.128:9092 从topic中获取消息（消费消息） ./bin/kafka-console-consumer.sh --topic myTopicOne --from-beginning --bootstrap-server 192.168.92.129:9092 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"logstash/logstash技术分享.html":{"url":"logstash/logstash技术分享.html","title":"Logstash","keywords":"","body":"Logstash技术分享介绍数据处理流程安装部署下载地址Elastic Stack组件版本对应表文件目录Hello World启动命令Es-To-Eselasticsearch-output-plugin actionKafka-To-Eskafka-to-es.confkafka-to-es-template.jsonscript 脚本更新多配置文件pipelines.yml性能优化及扩展其他问题@timestamp 少8小时问题Logstash技术分享 介绍 ​ Elasticsearch是当前主流的分布式大数据存储和搜索引擎，可以为用户提供强大的全文本检索能力，广泛用于日志检索、全站搜索等领域。Logstash作为Elasticsearch常用的实时数据采集引擎，可以采集来自不同数据源的数据，并对数据进行处理后输出到多种输出源，是Elastic Stack的重要组成部分。 ​ logstash支持的input和output插件有很多，该分享主要针对Es-To-Es 及 Kafka-To-Es 两种情况。 数据处理流程 ​ Logstash的数据处理过程主要包括：Inputs、Filters、Outputs，另外在Inputs和Outputs中可以使用Codecs对数据格式进行处理，这四个部分均已插件的形式存在。 Inputs：输入源，也就是数据源，常见的插件如Kafka、File、Redis等。 Filter: 过滤，对数据源的数据进行过滤清洗，如格式转换、数据派生等，常见的插件如grok、mutate、drop等。 Outputs：用于数据输出，将数据输出到指定的组件中比如Elasticsearch、Mysql等，主要是作为ElasticSearch数据的输入源。 Codecs：Codecs不是一个单独的流程，而是在输入和输出等插件中用户数据转换的模块，用于对数据进行编码处理，常见的插件如json、multiline。 安装部署 下载地址 https://www.elastic.co/cn/downloads/logstash Elastic Stack组件版本对应表 https://www.elastic.co/cn/support/matrix#matrix_compatibility 注意：Logstash的运行环境依赖JDK，Logstash中自带了这部分，如果有需要也支持灵活使用其他JDK。 文件目录 主要目录 bin：bin目录，执行命令 config：配置目录 logs：日志目录 data：数据目录 主要文件 都在config目录下 logstash.yml：用于控制logstash的执行过程[参考链接] pipelines.yml: 如果有多个pipeline时使用该配置来配置多pipeline执行[参考链接] jvm.options：jvm的配置 log4j2.properties:log4j 2的配置，用于记录logstash运行日志[参考链接] startup.options: 仅适用于Lniux系统，用于设置系统启动项目！ Hello World 在logstash根目录下创建conf目录，新建hello-world.conf，内容如下 input { stdin {} # 标准输入 } output { stdout {} # 标准输出 } 可以看到输出的数据会自动添加 host、hostname、@timestamp、@version等数据。 启动命令 验证配置文件 ./bin/logstash -f ./conf/es-to-es.conf --config.test_and_exit 启动 ./bin/logstash -f ./conf/es-to-es.conf 配置文件热加载（默认3s重新加载一次） ./bin/logstash -f ./conf/es-to-es.conf --config.reload.automatic Es-To-Es 以下配置意为将在线的机器信息输出到另一个索引中 input { elasticsearch { hosts => [\"http://192.168.10.65:9200\"] # Elasticsearch集群主机列表 index => \"zc_machine\" # 索引 query => '{ \"query\":{ \"term\":{ \"onlineStatus\":{ \"value\":1 } } } }' # query查询过滤数据 size => 500 # 每个滚动返回的最大命中数 scroll => '5m' # 控制滚动请求的保活时间（单位：秒） docinfo => true # 设置为true时，则在事件中包含Elasticseach文档信息，例如索引、类型和id docinfo_target => \"[@metadata][doc]\" # 如果docinfo选项请求文档元数据存储，则此选项将存储元数据字段的字段命名为子字段。 } } output { elasticsearch { hosts => [\"http://192.168.10.65:9200\"] index => \"tao_dest_event\" action => 'index' # 操作 document_type => \"%{[@metadata][doc][_type]}\" # 由于ElasticSearch从7.x后废弃该字段，所以无关紧要 document_id => \"%{[@metadata][doc][_id]}\" # 如果有更新操作，则作用与覆盖原文档 } } # 一个conf文件中可以存在多个input和output output { stdout { codec => rubydebug } } 运行后可以看到数据已经被插入到了新的索引中 elasticsearch-output-plugin action action是 logstash elasticsearch-output-plugin中的重要参数，代表数据入库elasticsearch时的操作。 index： 没有 document_id：将数据入库到Elasticsearch中，由Elasticsearch生成该文档的Id，因为没有指定document_id，重复的数据会生 成多条内容相同但 id 不同的文档。 有 document_id： ​ 如果 document_id 在Elasticsearch中不存在，创建一条新的文档，使用 document_id作为该文档的 id。 ​ 如果 document_id 在Elasticsearch中存在，直接更新文档。 create： 没有 document_id : 不指定，直接返回失败 ​ 有 document_id： ​ 如果 document_id 在Elasticsearch中不存在，创建一条新的文档，使用 document_id作为该文档的 id。 ​ 如果 document_id 在Elasticsearch中存在，直接返回失败，提示原因为该 id 的文档已存在。 delete： 没有 document_id：必须设置 document_Id,，否则将抛出错误 update： 没有 document_id：必须设置document_id，否则将排除错误 有 document_id： 如果 document_id 在Elasticsearch中不存在，创建一条新的文档，使用 document_id 作为该文档的id。 ​ doc_as_upsert 为 true 时，使用event的值作为文档的值。 ​ scripted_upsert 为 true 时， 使用script作为文档的值。 ​ 其余情况，使用upsert作为文档的值。 如果 document_id 在Elasticsearch中存在，直接更新该文档。 注意：es-to-es时，一条数据只会执行一次，源索引发生更新时，并不会通过logstash更新目标索引的数据。 Elasticsearch Input Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html Elasticsearch Output Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html Kafka-To-Es 使用Kafka作为数据源，将数据写入Elasticsearch。 kafka-to-es.conf input { # 配置kafka数据源 kafka { # kafka服务器地址，多个地址使用逗号分隔 bootstrap_servers => \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\" # sasl验证 #security_protocol => \"SASL_PLAINTEXT\" #sasl_mechanism => \"PLAIN\" #sasl_jaas_config => \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"g011234\";\" # 订阅的主题,支持订阅多个主题 topics => [\"logstash-kafka-to-es\"] # 消费者线程数 consumer_threads => 3 # 消费组Id group_id => \"logstash-kafka-to-es\" # 此属性会将当前topic、offset、group、partition等信息也带到message中 decorate_events => true # 添加json插件 codec => json { charset => \"UTF-8\" } } } filter { # 解决@timestamp 少8小时问题 ruby { code => \"event.set('index_date', (event.get('@timestamp').time.localtime).strftime('%Y-%m-%d'))\" } mutate { convert => [\"index_date\", \"string\"] } } filter{ mutate{ # 删除字段event: 原始信息会存一份到event remove_field => [\"event\"] # 添加字段 add_field => { \"kafka-metadata\" => \"%{[@metadata][kafka][topic]}-%{[@metadata][kafka][partition]}-%{[@metadata][kafka][offset]}\" } } } output { elasticsearch { hosts => [\"http://192.168.10.63:9200\", \"http://192.168.10.64:9200\", \"http://192.168.10.65:9200\"] # user => \"elastic\" # password => \"123qweasdZXC\" index => \"tao_index-%{index_date}\" document_id => \"%{[id]}\" # 是否覆盖ES中的template，默认是false template_overwrite => true # 模板名称 默认 文件名 template_name => \"kafka-to-es-template\" # 指定映射的模板文件，可以设置setting、mapping、aliases等等 template => \"/home/app/logstash/logstash-8.3.1/conf/template/kafka-to-es-template.json\" sniffing => true action => \"update\" doc_as_upsert => true } } output { stdout { codec => rubydebug } } kafka-to-es-template.json 可以设置 settings、mappings、aliases，比如我们想传输到另一个索引中时想修改某些字段的mapping，也可以选择在这边进行处理 { \"template\":\"tao_index-*\", \"settings\":{ \"index\":{ \"refresh_interval\":\"5s\" } }, \"mappings\":{ }, \"aliases\":{ \"{index}-alias\":{ } } } 发送消息到kafka 增量更新 script 脚本更新 在开发过程中也有很多是通过script脚本来进行更新的场景 kafka-to-es-script-upsert.conf （重点关注在 output elasticsearch 中的 script 配置） input { kafka { bootstrap_servers => \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\" topics => [\"logstash-kafka-to-es\"] consumer_threads => 3 group_id => \"logstash-kafka-to-es\" decorate_events => true codec => json { charset => \"UTF-8\" } } } filter { ruby { code => \"event.set('index_date', (event.get('@timestamp').time.localtime).strftime('%Y-%m-%d'))\" } mutate { convert => [\"index_date\", \"string\"] } } filter{ mutate{ remove_field => [\"event\"] add_field => { \"kafka-metadata\" => \"%{[@metadata][kafka][topic]}-%{[@metadata][kafka][partition]}-%{[@metadata][kafka][offset]}\" } } } output { elasticsearch { hosts => [\"http://192.168.10.63:9200\", \"http://192.168.10.64:9200\", \"http://192.168.10.65:9200\"] index => \"tao_index-%{index_date}\" document_id => \"%{[id]}\" action => \"update\" doc_as_upsert => true script_lang => \"painless\" script_type => \"inline\" script => 'ctx._source.count += params.event.get(\"count\"); ' } } output { stdout { codec => rubydebug } } 插入数据 增量更新 logstash打印消息 查看修改后的数据 多配置文件 ​ 在使用过程中我们可能会有要起多个配置文件的需要，这里不再演示使用 指定配置文件（多配置文件的方式），可以使用 pipelines 配置文件 文件位置 /home/app/logstash/logstash-8.3.1/config/pipelines.yml 配置完成后直接启动 ./bin/logstash 创建两个配置文件，内容如下 input { kafka { bootstrap_servers => \"192.168.181.128:9092,192.168.181.129:9092,192.168.181.130:9092\" topics => [\"logstash-kafka-to-es\"] consumer_threads => 3 group_id => \"logstash-kafka-to-es\" decorate_events => true codec => json { charset => \"UTF-8\" } } } filter { ruby { code => \"event.set('index_date', (event.get('@timestamp').time.localtime).strftime('%Y-%m-%d'))\" } mutate { convert => [\"index_date\", \"string\"] } } filter{ mutate{ remove_field => [\"event\"] add_field => { \"kafka-metadata\" => \"%{[@metadata][kafka][topic]}-%{[@metadata][kafka][partition]}-%{[@metadata][kafka][offset]}\" } } } output { elasticsearch { hosts => [\"http://192.168.10.63:9200\", \"http://192.168.10.64:9200\", \"http://192.168.10.65:9200\"] index => \"tao_index-%{index_date}\" document_id => \"%{[id]}\" action => \"update\" doc_as_upsert => true } } output { stdout { codec => rubydebug } } pipelines.yml 配置 - pipeline.id: muti1 pipeline.workers: 1 pipeline.batch.size: 2 pipeline.batch.delay: 5000 path.config: \"/home/app/logstash/logstash-8.3.1/conf/muti_conf/muti1.conf\" - pipeline.id: muti2 pipeline.workers: 1 pipeline.batch.size: 1 path.config: \"/home/app/logstash/logstash-8.3.1/conf/muti_conf/muti2.conf\" 这个topic我们设置了6个分区，因为每个配置文件配置了3个消费者线程，启动后可以看到共有6个消费者线程 pipelines.yml - pipeline.id: muti1 # pipeline线程数，官方建议是等于CPU内核数 pipeline.workers: 1 # 每次发送的事件数 默认 125 可能需要在jvm.options配置文件中增加堆内存 pipeline.batch.size: 20000 # 发送延时 毫秒 默认 50 pipeline.batch.delay: 5000 path.config: \"/home/app/logstash/logstash-8.3.1/conf/muti_conf/muti1.conf\" ​ 以上面的配置为例 Logstash 会努力攒到 20000 条数据一次性发送出去，但是如果 5 秒钟内也没攒够 20000 条，Logstash 还是会以当前攒到的数据量发一次。也就是两个条件满足其一即可。 ​ 经过我的实际测试 当 pipeline.batch.delay 设置为 5000 时会等待 10s ，设置为 10000 时会等待20s，也就是两倍。 性能优化及扩展 ​ 当单个Logstash无法满足性能需求时，可以采用横向扩展的方式来提高Logstash的处理能力。横向扩展的多个Logstash相互独立，采用相同的pipeline配置，另外可以在这多个Logstash前增加一个LoadBalance，以实现多个Logstash的负载均衡。 ​ 在 kafka-to-es 的场景下，我们可以利用kafka有分区和消费者组的天然优势做负载均衡。 ​ 由于多个Logstash之间是相互的独立的，所以在使用相同的配置文件时，还需要注意数据的处理，比如插入es时，如果希望相同的数据只存在一份，那我们必须在输入时指定document_id，同时在输出时声明出来（这里的指定不代表非要自定义es的document_id，而是输入源的数据要有document_id的属性值），输出时选择create、update或upsert。 ​ 其他问题 @timestamp 少8小时问题 filter { # # 解决timestamp少8小时问题 # # 1.添加字段timestamp，将原始字段@timestamp添加8小时赋值到timestamp # ruby { # code => \"event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)\" # } # # 2.重新设置@timestamp值 # ruby { # code => \"event.set('@timestamp',event.get('timestamp'))\" # } # # 3.删除临时字段timestamp # mutate { # remove_field => [\"timestamp\"] # } # # # 格式化@timestamp字段赋值到新字段index_date上 # ruby { # code => \"event.set('index_date', (event.get('@timestamp').time.localtime - 8*60*60).strftime('%Y-%m-%d'))\" # } # # 将index_date字段转换为string类型 # mutate { # convert => [\"index_date\",\"string\"] # } ruby { code => \"event.set('index_date_time', (event.get('@timestamp').time.localtime).strftime('%Y-%m-%d - %H:%M:%S'))\" } ruby { code => \"event.set('timestamp_origin', (event.get('@timestamp').time.localtime - 8*60*60).strftime('%Y-%m-%d - %H:%M:%S'))\" } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"linux/Linux环境下安装jdk和maven.html":{"url":"linux/Linux环境下安装jdk和maven.html","title":"安装jdk及maven","keywords":"","body":"环境: Ubuntu18.04 jdk1.8 maven3.6.0 将jdk和maven的linux版本的压缩包上传或下载到服务器中 首先进入root: sudo -i 解压: tar zxvf ****.tar.gz 如果需要移动到另一个文件夹中: mv 需要移动的文件文件名 要移动到的文件夹地址 解压后删除原压缩包: rm -rf 文件名 配置环境变量: vi /etc/profile 输入i进入编辑模式即insert添加以下代码 export JAVA_HOME=/usr/local/java/jdk1.8.0_141 为具体的安装位置 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH export M2_HOME=/usr/local/maven/apache-maven-3.6.0 为具体的安装位置 export PATH=${M2_HOME}/bin:$PATH 编辑完成后 按 esc 退出编辑模式再输入 :wq 保存后退出 使修改后的配置文件生效 source /etc/profile 检验是否安装成功 java -version 查看java版本 mvn -v 查看maven版本 可能出现的问题: vi ll 等命令不能使用可能是环境变量编辑有问题,输入以下命令解决 export PATH=/usr/bin:/usr/sbin:/bin:/sbin:/usr/X11R6/bin Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"linux/Linux远程管理.html":{"url":"linux/Linux远程管理.html","title":"Linux远程管理","keywords":"","body":"使用SSH连接工具XShll连接服务器 ​ 环境:谷歌云服务器,CentOS 7或者Ubuntu Server 18.04 LTS ​ 使用账号密码的方式 ​ 步骤: 1.切换到root角色 sudo -i 2.修改SSH配置文件 vi /etc/ssh/sshd_config 3.给root用户设置密码 passwd root 4.重启SSH服务 service sshd restart ​ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"spring/spring.html":{"url":"spring/spring.html","title":"Spring","keywords":"","body":"SpringXML方式注解方式@ComponentScan主要属性自定义一个扫描过滤类TypeFilter@ComponentScans@Scope作用域@Lazy懒加载@Conditional条件注册Bean@Import注册Bean回顾注册Bean的几种常用的方式ImportSelector自定义注册BeanImportBeanDefinitionRegistrar手动注册BeanFactoryBeanBean的声明周期自定义初始化销毁方法BeanPostProcessor接口@Value 赋值关于自动装配@Autowired@Resource@Primary@Inject注入顺序AOP相关术语:相关注解:切入点表达式:基本使用步骤:注意事项:切面类示例代码执行顺序什么是声明式事务?spring使用事务自定义配置Spring ​ context包提供了IOC容器功能 ​ IOC就是一个大的Map集合,key就是name,value就是对象 XML方式 public class XmlTest { @Test public void demo(){ ClassPathXmlApplicationContext classPathXmlApplicationContext = new ClassPathXmlApplicationContext(\"beans.xml\"); Object user = classPathXmlApplicationContext.getBean(\"user\"); System.out.println(user); } } 注解方式 // 使用注解方式的配置类 @Configuration public class SpringConfig { // 可以自定义name,默认是方法名 @Bean public User getUser(){ return new User(); } } public class AnnoTest { @Test public void demo(){ AnnotationConfigApplicationContext app = new AnnotationConfigApplicationContext(SpringConfig.class); User user = (User) app.getBean(\"getUser\"); System.out.println(user); // 根据类获取该类在容器中的name String[] beanNamesForType = app.getBeanNamesForType(User.class); // 获取容器中所有的name String[] names = app.getBeanDefinitionNames(); } } @ComponentScan ​ 该注解写在配置类用于扫描其他其他包中的组件@Controller @Service @Repository 这三个组件最顶层是@Component ​ 默认的name是类名首字母小写 ​ 通过该注解扫描得到的Bean和在配置类中使用@Bean注解得到的Bean是一个级别的 // 扫描时进行包含过滤,过滤类型默认为注解,classes的值则是最后包含的组件 // 需要将默认的过滤方式禁用 false // FilterType 常用类型还有自定义 也就是指定哪些类 ASSIGNABLE_TYPE // useDefaultFilters的作用是 是否开启扫描其他组件默认为true 包含时设置为false,排除某个类型时为true @Configuration @ComponentScan(value = \"xyz.taoqz.two\",includeFilters = { @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = {Controller.class}) },useDefaultFilters = false) // public class SpringConfig { } 主要属性 value：指定要扫描的package； includeFilters=Filter[]：指定只包含的组件 excludeFilters=Filter[]：指定需要排除的组件； useDefaultFilters=true/false：指定是否需要使用Spring默认的扫描规则：被@Component, @Repository, @Service, @Controller或者已经声明过@Component自定义注解标记的组件； 在过滤规则Filter中： FilterType：指定过滤规则，支持的过滤规则有 ANNOTATION：按照注解规则，过滤被指定注解标记的类； ASSIGNABLE_TYPE：按照给定的类型； ASPECTJ：按照ASPECTJ表达式； REGEX：按照正则表达式 CUSTOM：自定义规则； value：指定在该规则下过滤的表达式； 自定义一个扫描过滤类TypeFilter 过滤类: public class CustomTypeFilter implements TypeFilter { /** * 读取到当前正在扫描类的信息 * @param metadataReader * 可以获取到其他任何类的信息 * @param metadataReaderFactory * @return * @throws IOException */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException { // 获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); // 获取当前正在扫描的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); // 获取当前类路径 Resource resource = metadataReader.getResource(); System.out.println(classMetadata.getClassName()); // 可以在此处进行判断,根据类信息进行过滤 return false; } } 配置类: @Configuration @ComponentScan(value = \"xyz.taoqz.two\",includeFilters = { // 将过滤类型改为CUSTOM自定义,classes的值为自定义的过滤类 @ComponentScan.Filter(type = FilterType.CUSTOM,classes ={CustomTypeFilter.class}) },useDefaultFilters = false) public class SpringConfig { } ​ @ComponentScans // (可以声明多个@ComponentScan) @ComponentScans({@ComponentScan(\"xyz.taoqz\"), @ComponentScan(value = \"xyz.taoqz.two\",includeFilters = { @ComponentScan.Filter(type = FilterType.CUSTOM,classes = {CustomTypeFilter.class}) },useDefaultFilters = false) }) public class SpringConfig { } @Scope作用域 @Configuration public class SpringConfig { // 无论是使用@Bean还是其他组件的注解,默认都是单例的 @Bean /** * 更改作用域 * singleton: 单实例,IOC容器启动时会调用方法创建对象并放到IOC容器中,以后获取时拿的是同一个对象 * prototype: 多实例,IOC容器启动时并不会调用方法创建对象,而是每次使用时用方法创建对象 * request: 一个请求一个实例 * session: 同一个会话一个实例 */ @Scope(\"PROTOTYPE\") public User getUser(){ return new User(); } } @Lazy懒加载 ​ 懒加载:主要针对单例Bean在容器启动时创建对象,设置懒加载后,容器启动后不再创建对象,但是可以在容器中获取其beanname,在第一次使用时才会创建对象初始化 ​ 如何使用懒加载? ​ 在@Bean注解或者其他组件上添加@Lazy注解 ​ 代码示例: @Configuration public class SpringConfig { @Bean @Lazy public User getUser(){ System.out.println(\"向容器中添加对象\"); return new User(); } } public class LazyTest { @Test public void fun(){ AnnotationConfigApplicationContext app = new AnnotationConfigApplicationContext(SpringConfig.class); System.out.println(\"容器创建完成\"); app.getBean(\"getUser\"); } } console: 容器创建完成 向容器中添加对象 使用懒加载后,会先创建容器,然后在获取Bean时才会向容器中添加对象并获得 @Conditional条件注册Bean ​ @Conditional(Class [] value()) ​ 写在类上表示该类或者该配置类下所有Bean必须符合条件才会注册,和@Bean一起写在方法上表示该方法返回值能否注册 ​ 该注解的参数是一个Condition的实现类,实现其方法,可以判断Bean满足需求时进行注册 public class MyCondition implements Condition { /** * 判断条件可以使用的上下文(环境0 * @param context * 注解的信息 * @param metadata * @return */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { // 可以在此处进行逻辑判断 返回true时Bean实例才会注入到容器中 return false; } } @Import注册Bean ​ @Import(Class [] value()) : 参数时要注册的Bean的class数组 回顾注册Bean的几种常用的方式 ​ 1.在配置中使用@Bean的方式将方法的返回值注册到容器中,通常是使用第三方类库时使用使用组件加包扫描的方式,一般用于自定义 ​ 2.组件注解:@Component @Controller @Service @Repository ​ 包扫描:@ComponentScan(组件注解所在的包) ​ 此方法优先级高于Import方式使用@Import注解 ​ 3.使用@Import方式注册的Bean其name是类全路径(包名+类名) ​ 回顾使用其他方式时的name(没有指定名称的情况下) ​ @Bean : 方法名 ​ 组件 : 类首字母小写 ​ 4.使用FactoryBean 下方有解释 ImportSelector自定义注册Bean // 在配置类中添加 @Import(value = {MyImportSelector.class}) public class MyImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { // 返回需要注册到容器中的Bean的全类名数组 return new String[0]; } } ImportBeanDefinitionRegistrar手动注册Bean // 在配置类中添加 // @Import(value = {MyImportBeanDefinitionRegistrar.class}) public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { /** * 当前类的注解信息 * @param importingClassMetadata * BeanDefinition注册类 * 把所有需要添加到容器中的Bean加入 * @param registry */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { // 该方法可以判断容器中是否有指定name的Bean registry.containsBeanDefinition(\"\"); // 使用该方式注册Bean时,需要手动给出Bean的信息 Spring提供了RootBeanDefinition类 // 调用注册Bean的方法手动将Bean的name 和 信息注册到容器中 // IDEA中双击Shift键 搜索DefaultListableBeanFactory 类 会调用该类中的registerBeanDefinition将Bean注册到容器中 RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Animal.class); registry.registerBeanDefinition(\"animal\",rootBeanDefinition); } } FactoryBean ​ 如果想创建一个工厂类,用来灵活的创建对象并且想要交由Spring管理,可以使用该类 ​ 需要注意的是getBean时获取的对象并不是Factory本身,而是其getObject()方法的返回值返回的具体类型 ​ 想要获取该类对象在getBean()中的beanname参数前加&,底层在获取对象时会先判断beanname是否以&开头 ​ 使用方式:实现改接口,并且重写方法,将该类注入到容器中 Object getObject() : 该方法则为实际返回的对象 Class getObjectType() : 对象的类型 boolean isSingleton() : 表示对象注入容器时是否单例 ​ Bean的声明周期 ​ 创建 : 构造方法 ​ 初始化 : 指定的初始化方法 ​ 销毁 : 指定的销毁方法 ​ 会在容器关闭或移除对象时执行 ​ 创建和初始化会在容器创建完成前 自定义初始化销毁方法 ​ xml方式 ​ 注解方式1 @Bean(initMethod = \"初始化方法名\",destroyMethod = \"销毁方法名\") public Person person(){ return new Person(); } ​ 注解方式2 ​ 属于JDK的注解 ​ @PostConstruct : 初始化方法 ​ @PreDestroy : 销毁方法 @Component public class Person { public Person() { System.out.println(\"创建Person\"); } // 对象创建并赋值后调用 @PostConstruct public void init(){ System.out.println(\"Person 初始化\"); } // 容器移除对象调用 @PreDestroy public void destory(){ System.out.println(\"Person销毁\"); } } ​ spring 提供的接口 ​ InitializingBean, DisposableBean : 实现该接口完成init和destroy ​ 注意:只有单实例在容器创建时才会执行Bean的创建和初始化,并添加到容器中 ​ 多实例只有在获取Bean时才会执行 BeanPostProcessor接口 ​ 实现该接口可以在对象创建完成后的初始化方法前后进行增强 ​ spring底层有很多实现该接口的类(处理器),功能包括bean的赋值,注入其他组件,生命周期注解功能等 @Value 赋值 ​ 1.普通赋值,直接在组件的成员变量上声明该注解并且赋值(字符串),同时支持springEL表达式完成计算 ​ 例如 @Value(\"#{1*3}\") 在获取时便可得到该表达式的结果 ​ 2.使用.properties文件+@PropertySource(文件名数组)注解 ​ @Value(\"${属性名在文件中的key}\") ​ 在配置类上声明@PropertySource(value = {\"文件名\"}) AnnotationConfigApplicationContext app = new AnnotationConfigApplicationContext(SpringConfig.class); // 获取环境 ConfigurableEnvironment environment = app.getEnvironment(); // 获取所有配置文件 MutablePropertySources propertySources = environment.getPropertySources(); for (PropertySource propertySource : propertySources) { System.out.println(\"获取每一个配置文件名称\"+propertySource.getName()); System.out.println(\"获取每一个文件中的所有的属性及属性值\"+propertySource.getSource()); System.out.println(\"获取指定属性名称的属性值 没有则为null\"+propertySource.getProperty(\"name\")); } 关于自动装配 @Autowired ​ @Autowired默认根据类型匹配,@Autowired本身不能根据beanname指定获取容器中的实例对象,需要配合使用@Qualifier(beanname) ​ required属性 ​ 属性值为true表示注入的时候，容器中该bean必须存在，否则就会注入失败。 ​ 属性值为false：表示忽略当前要注入的bean，如果有直接注入，没有跳过，不会报错 @Resource ​ jdk提供的@Resource(name = beanname)注解可以指定名称,默认根据变量名作为beanname获取,如果没有则根据类型查找,不支持required,不支持优先级 // 获取指定环境的上下文对象(注解方式) ApplicationContext helloController = new AnnotationConfigApplicationContext(MyApplication.class); // 获取容器中所有的bean的name 默认是类首字母小写 也就是beanname String[] beanDefinitionNames = helloController.getBeanDefinitionNames(); for (String beanDefinitionName : beanDefinitionNames) { System.out.println(beanDefinitionName); } // 通过beanname获取指定的bean System.out.println(helloController.getBean(\"userController\")); // 通过指定的类型获取bean System.out.println(helloController.getBean(UserController.class)); @Primary ​ 设置获取bean时的优先级,可以在类上添加,也可以配合@Bean使用 ​ 使用@Autowired注解进行依赖注入时便会使用声明该注解的实例,@Resource不会起作用 ​ 比如一个接口有两个实现类,都会被扫描进容器,但是在使用接口进行注入时会报错,如果给 其中一个实现类添加优先级,会解决该问题并且优先使用该实现类 ​ @Inject ​ 是Java JSR330的规范,和@Autowired功能类似,支持@Primary优先级,区别是没有required属性 javax.inject javax.inject 1 ​ 如果@Bean的方法名或者name相同于类首字母小写或者注解中的beanname会优先使用 @Bean产生的实例此效果三种注解相同 如果同时声明了@Qualifier(beanname1)+@Autowired和@Primary+@Bean(beanname2):会使用beanname1 注入顺序 **@Qualifier > 优先级 > 变量名(同名@Bean优先) > 类型** AOP ​ 面向切面编程(底层使用动态代理) ​ 在程序运行期间动态,在不修改源码的基础上，对已有方法进行增强。 ​ 使用aop需要的依赖 org.springframework spring-aspects 5.1.8.RELEASE ​ 相关术语: ​ Target(目标对象):被增强方法的所属对象 ​ Joinpoint(连接点):目标方法可增强的位置 ​ Pointcut(切入点):实际被增强的方法 ​ Advice(通知/增强):对切入点增强的位置及方法 ​ Aspect(切面):切入点的通知的结合() ​ Proxy(代理):一个类被AOP织入增强后,会产生一个结果代理类 相关注解: 注解 描述 @Aspect 把当前类声明成切面类 @Before 把当前方法看成是前置通知 @AfterReturning 把当前方法看成是返回通知,需要方法正常返回。 @AfterThrowing 把当前方法看成是异常通知,方法参数可以拿到 @After 把当前方法看成是后置(最终)通知,,无论是否出现异常都会执行 @Around 把当前方法看成是环绕通知 @Pointcut 定义方法声明该注解添加切入点表达式,可以直接引用该方法作为表达式,相当于提取 切入点表达式: //切入点表达式：execution后小括号中寻找要增强方法的表达式 // @Before(\"execution(public void com.czxy.demo03.*.save())\") //某个包下的所有类用* // @Before(\"execution(void com.czxy.demo03.*.save())\") //访问权限可以省略 // @Before(\"execution(* com.czxy.demo03.*.save())\") //任意返回值 // @Before(\"execution(* *.*.*.*.save())\") //任意包 // @Before(\"execution(* com..*.save())\") //当前包下的任意包 // @Before(\"execution(* com..*.*())\") //任意方法名称 // @Before(\"execution(* com..*.*(*))\") //任意方法参数必须有 // @Before(\"execution(* com..*.*(..))\") //任意方法任意参数均可 // @Before(\"execution(* *..*.*(..))\") //项目中所有方法 ​ 基本使用步骤: ​ 1.导入spring aop 的相关依赖 ​ 2.声明一个切面类 添加@Aspect @Component 注解 ​ 3.在切面类中编写增强方法,并添加相关通知注解以及所需切入点 ​ 4.在配置类中添加@EnableAspectJAutoProxy 注解开启切面 注意事项: ​ 关于@Around 注解的使用 ​ 声明该注解后方法会默认有一个JoinPoint/ProceedingJoinPoint形参 ​ 将joinpoint强转为proceedingJoinPoint,并调用其pjp.proceed();方法 ​ 在该方法调用前执行的就是前置通知,之后则是后置通知 ​ 可以使用ty{}catch()finally{}做出异常通知和最终通知的效果 @Around(\"execution(public void com.czxy.demo03.OrderDao.save())\") public void myAround(JoinPoint joinPoint,ProceedingJoinPoint proceedingJoinPoint) { //获取目标对象 Object target = joinPoint.getTarget(); System.out.println(target); //获取切点方法 Signature signature = joinPoint.getSignature(); System.out.println(signature); //将joinpoint强转为proceedingJoinPoint ProceedingJoinPoint pjp = (ProceedingJoinPoint)joinPoint; try { System.out.println(\"之前？\"); pjp.proceed(); System.out.println(\"之后？\"); } catch (Throwable throwable) { System.out.println(\"异常出现啦！\"); throwable.printStackTrace(); } finally { System.out.println(\"最终执行的！\"); } } ​ 关于@Pointcut的使用 //设置公共切点，可以方便其他增强使用，这里bf方法与ar方法使用了该公共切点 @Pointcut(需要提取的切入点的表达式) private void myPointcut(){ } @Before(\"切面类.myPointcut()\") // @AfterReturning(\"myPointcut()\") public void bf() { System.out.println(\"增强\"); } 切面类示例代码 //日志切面类 @Aspect public class LogAspects { @Pointcut(\"execution(public int com.enjoy.cap10.aop.Calculator.*(..))\") public void pointCut(){}; //@before代表在目标方法执行前切入, 并指定在哪个方法前切入 @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint){ System.out.println(joinPoint.getSignature().getName()+\"除法运行....参数列表是:{\"+ Arrays.asList(joinPoint.getArgs())+\"}\"); } @After(\"pointCut()\") public void logEnd(JoinPoint joinPoint){ System.out.println(joinPoint.getSignature().getName()+\"除法结束......\"); } @AfterReturning(value=\"pointCut()\",returning=\"result\") public void logReturn(Object result){ System.out.println(\"除法正常返回......运行结果是:{\"+result+\"}\"); } // @AfterThrowing(value=\"pointCut()\",throwing=\"exception\") public void logException(Exception exception){ System.out.println(\"运行异常......异常信息是:{\"+exception+\"}\"); } @Around(\"pointCut()\") public Object Around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable{ System.out.println(\"@Arount:执行目标方法之前...\"); Object obj = proceedingJoinPoint.proceed();//相当于开始调div地 System.out.println(\"@Arount:执行目标方法之后...\"); return obj; } } 执行顺序 ​ @Around之前 > @Before > 切入点(目标方法) > @Around之后(如抛出异常不执行) > @After > @AfterThrowing(程序抛出异常) > @AfterReturning(成功返回,如抛出异常不执行) ​ 什么是声明式事务? ​ 以方法为单位,进行事务控制,抛出异常,事务回滚 ​ 最小的执行单位为方法,决定执行成败通过是否抛出异常来判断,抛出异常即执行失败 spring使用事务 ​ 导入相关依赖: mysql mysql-connector-java 5.1.47 com.alibaba druid 1.1.10 org.springframework spring-jdbc 5.1.8.RELEASE ​ 配置类 package xyz.taoqz.config; import com.alibaba.druid.pool.DruidDataSource; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.datasource.DataSourceTransactionManager; import org.springframework.transaction.annotation.EnableTransactionManagement; import javax.sql.DataSource; /** * @author :almostTao * @date :Created in 2019/9/7 15:13 */ @Configuration @ComponentScan(\"xyz.taoqz\") // 开启事务 配合@Transactional注解以及事务管理器使用 @EnableTransactionManagement public class SpringConfig { /** * 配置数据源 * @return */ @Bean public DataSource dataSource(){ DruidDataSource druidDataSource = new DruidDataSource(); druidDataSource.setUsername(\"root\"); druidDataSource.setPassword(\"123\"); druidDataSource.setUrl(\"jdbc:mysql:///spring_day08_shiwu\"); druidDataSource.setDriverClassName(\"com.mysql.jdbc.Driver\"); return druidDataSource; } /** * 实现简单的增删改查 * @param dataSource * @return */ @Bean public JdbcTemplate jdbcTemplate(DataSource dataSource){ return new JdbcTemplate(dataSource); } /** * 事务管理器 * @param dataSource * @return */ @Bean public DataSourceTransactionManager txManager(DataSource dataSource){ return new DataSourceTransactionManager(dataSource); } } ​ dao依赖注入 @Autowired private JdbcTemplate jdbcTemplate; ​ @Transactional ​ 添加在方法上,表示该方法有事务效果 ​ 添加在类上,表示该类中所有方法都有事务 自定义配置 应该实现WebMvcConfigurer接口还是继承WebMvcConfigurerAdapter类,该类是接口的一个实现类.为了解决需要单独配置其 中一项而需要实现接口中的所有方法而产生,但在spring5之后该接口中的方法定义为default,可以选择重写方法,因此推荐使用实 现WebMvcConfigurer接口+@EnableWebMvc的方式 https://docs.spring.io/spring/docs/5.2.1.RELEASE/spring-framework-reference/web.html#mvc-config-enable ​ FactoryBean和BeanFactory的区别 ​ FactoryBean:将java实例注入到容器中 ​ BeanFactroy:从容器中获取实例化后的Bean Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"spring/SpringMVC.html":{"url":"spring/SpringMVC.html","title":"SpringMVC","keywords":"","body":"1.SpringMVC是什么:2.视图解析器:3.参数绑定:3.1.注意事项(重要)3.2.@RequestParam3.3.数组绑定3.4.集合类型:4.多路径映射/请求方法限定/窄化路径:4.1多路径映射:4.2请求方法限定(也是该注解的一个参数)4.3窄化路径:5. @PathVaraible注解的使用:6.Controller方法返回值以及跳转方式:7.异常处理器:8.图片上传:9.JSON数据交互:10.拦截器:11.序列化12.@RequestBody注意事项1.SpringMVC是什么: ​ Spring web mvc是表现层的框架,它是Spring框架的一部分 ​ 执行流程:客户端发送请求,springMVC的前端控制器DispatcherServlet接收并分配请求到对应的Controller执行对应的 方法,方法会返回视图,DispatherServlet解析视图,返回客户端进行渲染 2.视图解析器: ​ 视图:简单理解为浏览器向用户展示的界面 ​ 为什么使用? ​ 浏览器想要展示页面需要有一个请求的地址,拿到路径,但路径通常会在一个文件夹中,而文件夹可能会有多层,所 以SpringMVC中的视图解析器,可以帮我们减少一些重复的代码,把绝对路径可以写成我们的逻辑路径 ​ 需要在配置类中添加:用户在返回视图时,会经过该解析器,帮我们拼接字符串 ​ 配置类继承WebMvcConfigurerAdapter,注意spring5后一律推荐使用实现WebMvcConfigurer接口 @Bean public InternalResourceViewResolver viewResolver(){ InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); // 这是视图的前缀 viewResolver.setPrefix(\"/\"); // 这是视图的后缀 viewResolver.setSuffix(\".jsp\"); return viewResolver; } // properties 方式配置 spring.mvc.view.prefix= spring.mvc.view.suffix= 3.参数绑定: 3.1.注意事项(重要) ​ 其中前台的name必须和属性值名称一致,使用set方法赋值 ​ 其中方法的参数类型如果是基本数据类型推荐使用包装类型,因为基本数据类型不能为null,如果请求中没有改参数,会抛异常 3.2.@RequestParam ​ 作用:当请求参数的名称与方法参数名称不一致时,可以使用该注解标记对应关系 ​ 该注解中的参数: ​ name = \"对应的在前台的name名称\":与方法中的参数名对应 ​ required:表示是否必须有该参数有两个值,,默认为false ​ defaultValue:表示如果前台没有传递该参数时的默认值 ​ 引用数据类型的映射(POJO): ​ 在方法参数声明一个对象变量,请求时如果name和对象中的属性值一致,会自动注入到该对象中 ​ 简单说就是在对象中又有其他对象: ​ 在前台传递参数时,参数名称为该对象的名称再点属性 ​ 例子: 两个javaBean @Data public class CartItem { private Item item; private Integer number; private Double price; } @Data public class Item { private String name; private Double price; } 前台要提交的表单 Controller层处理,及控制台输出 3.3.数组绑定 ​ 前台表单传递一个相同name的一组数据,在方法的参数中写一个数组类型进行赋值,其中数组的名称 也要和前台name的名称相同 ​ 示例: // 数据:[\"1\",\"2\"] @PostMapping(\"/deleteMore\") public ResponseEntity deleteMore(@RequestBody String[] ids){} ​ // 请求路径 http://localhost:8080/emp/demo?arr=1&arr=2 @GetMapping(\"/demo\") public void fun(String[] arr){System.out.println(Arrays.toString(arr));} 3.4.集合类型: ​ 如果在方法参数中直接写集合类型是赋值不了的,需要使用数组或者在Bean中添加一个集合属性进行赋 值,如果需要给指定索引赋值,在name中指定索引即可 ​ 例子:前台传递表单方式 在上面的基础之上CartItem类中添加了一个集合属性 ​ ​ 前台数据传递的方式以及后台打印结果 4.多路径映射/请求方法限定/窄化路径: 4.1多路径映射: ​ @RequestMapping:请求路径映射 ​ 常用的参数: ​ value:\"指定访问的路径\"可以多个{\"路径1\",\"路径2\"} ​ headers :指定请求头中的参数 **params** :你可以让多个处理方法处理到同一个URL 的请求, 而这些请求的参数是不一样的。也就是 请求时的不同参数,调用的方法也会不同 @RequestMapping(value = \"/hello.action\",params = {\"id=10\"}) public String fun(String id){ System.out.println(id); return \"index\"; } @RequestMapping(value = \"/hello.action\",params = {\"id=20\"}) public String fun2(String id){ System.out.println(id); return \"index\"; } 4.2请求方法限定(也是该注解的一个参数) ​ method:指定请求的类型,也可以是数组 ​ method = {RequestMethod.GET,RequestMethod.POST} 4.3窄化路径: ​ 在类上添加相当于设置请求的前缀 ​ 在方法上添加,设置方法对应的请求路径 5. @PathVaraible注解的使用: ​ @RequestMapping 注解可以同 @PathVaraible 注解一起使用，用来处理动态的 URI ​ 将 @PathVaraible注解写到方法的参数前,对应方法中的参数,可以将URL中的对应的位置的值赋值到变量中,还可以使用正则表达式 @RequestMapping(value = \"/fetch/{id}.action\", method = RequestMethod.GET) public String getDynamicUriValue(HttpServletRequest request, @PathVariable(\"id\") String id) { System.out.println(\"ID is \" + id); return \"index\"; } ​ 如果设置参数不是必须的,需要有对应的路径映射 // required 设置参数为非必须项 @GetMapping({\"/path/{id}\",\"/path\"}) public void fun(@PathVariable(required = false) Integer id){ System.out.println(id); } 6.Controller方法返回值以及跳转方式: ​ 返回值为ModelAndView:创建ModelAndView对象,或者在方法形参中给出,可以在构造方法中直接给(返回的页面,存在域中的名称,存在域中的数据),或者分别使用addObject(name,value),setViewName(地址),会经过视图解析器 ​ 返回值为String:在方法的形参上添加Model,使用addAttribute()存储数据,返回值写地址 ​ 还可以使用 return \"redirect:tao.jsp\"; ​ return \"forward:tao.jsp\"; ​ 这两种方法存储的域都是request(想要存储到别的域中,可以使用原生) ​ 返回值为void使用方式和BaseServlet时一样,并且不会经过SpringMVC中的视图解析器 ​ 想要重定向或请求转发,在字符串中分别拼接redirect:和forward:,MVC会帮我们解析返回值,并在底层调用不同的方法 7.异常处理器: ​ SpringMVC异常处理机制: ​ 系统的dao、service、controller出现异常都通过throws Exception向上抛出，最后由springmvc前端控制器交由 异常处理器进行异常处理。springmvc提供全局异常处理器（一个系统只有一个异常处理器）进行统一异常处理。 具体代码实现: ​ 创建一个自己的异常类,继承Exception ​ 定义统一异常处理器类:实现HandlerExceptionResolver接口并交由spring管理,重写方法 ​ @Component public class CustomExceptionResolver implements HandlerExceptionResolver { @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception ex) { //统一处理异常 CustomException customException = null; // 如果是自定义异常直接类型转换赋值,并显示错误信息 // 如果不是创建自定义异常,提示未知错误 if(ex instanceof CustomException){ customException = (CustomException) ex; } else { customException = new CustomException(\"系统错误，请与系统管理 员联系！\"); } //设置数据 ModelAndView modelAndView = new ModelAndView(); modelAndView.addObject(\"message\", customException.getMessage()); modelAndView.setViewName(\"WEB-INF/error\"); return modelAndView; } } 8.图片上传: ​ 在写具体代码时,先了解一下表单的一个属性 ​ enctype:该属性有三个属性值 ​ application/x-www-form-urlencoded:默认值,请求方式为get时,会将表单中的数据以键值对的方式加到url后面中间用?分隔,为post时,浏览器把form数据封装到http body中，然后发送到服务器 ​ text/plain：表单以纯文本形式进行编码 ​ multipart/form-data：当上传的含有非文本数据时,设置为该属性值,将表单数据变成二进制数据进行上传,这时使用request是获取不到表单响应的值的 ​ 在了解这点后,看具体上传图片的代码: ​ 在配置类中添加文件上传解析器: ​ 需要导入commons-io和commons-fileupload依赖 ​ @Bean public CommonsMultipartResolver multipartResolver[1] (){ CommonsMultipartResolver multipartResolver = new CommonsMultipartResolver(); // 设置所有的上传文件的总大小 10M multipartResolver.setMaxInMemorySize(10*1024*1024); // 设置单个文件上传的大小 4M multipartResolver.setMaxUploadSize(4*1024*1024); multipartResolver.setDefaultEncoding(\"utf-8\"); return multipartResolver; } 注解方式: spring.servlet.multipart.max-file-size= 文件大小 spring.servlet.multipart.max-request-size= 请求大小 Controller中写方法接受表单数据,需要设置一个参数MultipartFile表示文件上传对象,其他参数可以正常获取 ​ 该类常用方法: 方法名 描述 String getOriginalFilename() 获得原始上传文件名 transferTo(File file) 将上传文件转换到一个指定的文件中 String getContentType() 获取文件MIME类型，如image/pjpeg、text/plain等 String getName() 获取表单中文件组件的名字 ​ 直接写一个数组(没试过) ​ 如果是多文件上传,可以写一个VO类,里面添加一个成员变量List MultipartFiles 9.JSON数据交互: ​ 需要导入的依赖: com.fasterxml.jackson.core jackson-databind 2.9.7 ​ 前端发送json格式的数据,如果对应的Controller方法中的对象或者变量名相同可以直接封装赋值 ​ 后台想要把数据返回到前台页面显示,需要在配置类上添加@EnableWebMvc开启配置 ​ 有两种方式: ​ 1.在类上添加@Controller在方法返回值前加@ResponseBody ​ 2.直接在类上添加@RestController,因为该注解包含上面两个注解 10.拦截器: ​ 自定义拦截器需要实现HandlerInterceptor接口重写其方法 preHandle方法是controller方法执行前拦截的方法 可以使用request或者response跳转到指定的页面 return true放行，执行下一个拦截器，如果没有拦截器，执行controller中的方法。 return false不放行，不会执行controller中的方法。 postHandle是controller方法执行后执行的方法，在JSP视图执行前。 可以使用request或者response跳转到指定的页面 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 afterCompletion方法是在JSP执行后执行 request或者response不能再跳转页面了 使用配置类配置拦截器: ​ 配置类继承WebMvcConfigurerAdapter ​ 重写方法: @Override public void addInterceptors(InterceptorRegistry registry) { // 添加拦截器 InterceptorRegistration interceptorRegistration1 = registry.addInterceptor(myInterceptor1); InterceptorRegistration interceptorRegistration2 = registry.addInterceptor(myInterceptor2); interceptorRegistration1.addPathPatterns(\"/**\"); interceptorRegistration2.addPathPatterns(\"/**\"); } ​ 多个拦截器执行流程: ​ 会先按顺序执行preHandle方法,在Controller方法执行完成并且在jsp视图执行前反顺序执行postHandle方法,然后 在返回视图,在返回视图完成后在反顺序执行afterCompletion方法 11.序列化 @JsonInclude注解的使用 有时前台获取数据时,数据中有为null或者\"\"空字符串的字段,为了解决不必要的麻烦,可以在实体类的字段上添加该注解 @JsonInclude(JsonInclude.Include.NON_NULL) // 字段的值为空时跳过序列化 @JsonInclude(JsonInclude.Include.NON_EMPTY)// 包含上面注解,同时字段值为空字符时跳过序列化 12.@RequestBody 配合post请求使用 该注解主要作用用于解析请求体中的数据 如果不添加该注解,那么获取形参的数据为null // 不添加注解 @PostMapping public void fun(String student){ System.out.println(student); } // 请求体中的数据 {\"sname\":\"taoqz\"} // 控制台输出 console : null // 添加注解 @PostMapping public void fun(@RequestBody String student){ System.out.println(student); } // 请求体中的数据 {\"sname\":\"taoqz\"} // 控制台输出 会将数据当做字符串解析赋值 console : {\"sname\":\"taoqz\"} ​ 传递对象 ​ 只需将参数设置为需要的对象,mvc会将请求中的数据自动封装到对象中,前提是请求体中的字段与实体类中的属性名称一致 @PostMapping public void fun(@RequestBody Student student){ System.out.println(student); } // 请求体中数据 { \"sname\":\"taoqz\", \"address\":\"北京\" } // 控制台输出 Student(sid=null, sname=taoqz, address=北京, teachers=null) ​ 结合@RequestParam使用 ​ @RequestBody注解只能使用一次,如果有特殊需要可以结合@RequestParam使用 @PostMapping(\"/test\") public void add(@RequestBody Student student, @RequestParam(\"arr\") String[] arr){ System.out.println(student); System.out.println(Arrays.toString(arr)); } // 请求路径 mvc会将路径中相同名称的参数封装到数组中 http://localhost:8080/test/zz?arr=1&arr=2 // 请求体 自动封装到对象中 { \"sname\": \"zs\", \"address\": \"江苏省南京市\" } // 控制台输出 student: Student(sid=null, sname=zs, address=江苏省南京市, teachers=null) arr : [1, 2] 注意事项 ​ 请求: http://localhost:8090/user?ids=1&ids=2 ​ 接收数组参数 @DeleteMapping public void fun(Integer[] ids){ System.out.println(Arrays.toString(ids)); } ​ 将参数转为集合 // 也可以使用请求: http://localhost:8090/user?ids=1,2 @DeleteMapping public void fun(@RequestParam List ids){ ids.forEach(System.out::println); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"spring/spring循环依赖.html":{"url":"spring/spring循环依赖.html","title":"Spring循环依赖","keywords":"","body":"Spring循环依赖Spring创建bean过程个人理解为什么要三级缓存？获取bean经过三级缓存过程源码资料链接Spring循环依赖 Spring创建bean过程 主bean：依赖链中最先被spring加载的bean。 属性注入、setter注入如何解决循环依赖。 使用了三级缓存，会先从一级缓存找然后二级，三级。 singletonObjects：完成初始化的单例对象的 cache，这里的 bean 经历过 实例化->属性填充->初始化 以及各种后置处理（一级缓存） earlySingletonObjects：存放原始的 bean 对象（完成实例化但是尚未填充属性和初始化），仅仅能作为指针提前曝光，被其他 bean 所引用，用于解决循环依赖的 （二级缓存） singletonFactories：在 bean 实例化完之后，属性填充以及初始化之前，如果允许提前曝光，Spring 会将实例化后的 bean 提前曝光，也就是把该 bean 转换成 beanFactory 并加入到 singletonFactories（三级缓存） Spring 为了解决单例的循环依赖问题，使用了三级缓存。其中一级缓存为单例池（singletonObjects），二级缓存为提前曝光对象（earlySingletonObjects），三级缓存为提前曝光对象工厂（singletonFactories）。 个人理解 一级缓存：完成初始化的单例对象的 cache，这里的 bean 经历过 实例化->属性填充->初始化 以及各种后置处理。 二级缓存：和三级缓存配合解决循环依赖问题，是只存储进行实例化后提前曝光的对象。 三级缓存：是一个bean的对象工厂，创建bean实例化时会将刚实例化后的对象转成对应的bean对象工厂添加到三级缓存中，获取bean时经过三级缓存，三级缓存获取到后，会判断是否需要代理对象，如果需要代理对象三级缓存会创建代理对象，并提供原始对象给代理对象，如果不需要直接返回原始对象，最后会将对象移动至二级缓存，从三级缓存中移除。 为什么要三级缓存？ 当然如果只解决循环依赖二级缓存确实够用了，但是如果此时需要获取代理对象怎么办，只能在之前将bean统统完成AOP代理，没有必要也不太合适。 构造注入无法解决循环依赖的问题是因为其原理。 比如A依赖B，B依赖A，A使用构造参数注入B。 反射获取调用有参构造，此时必须传入一个B的对象，但是B对象同时依赖了A对象，A对象由于连实例化都没有完成无法获取，需要创建，此时便陷入了循环。 如果主bean也就是A使用setter注入，B使用构造注入，那么循环依赖也可以解决，因为A可以通过反射使用无参构造进行实例化，并将未进行初始化（属性注入）的A添加到对象缓存中，B在使用有参构造是便可以获取到依赖A的引用。 获取bean经过三级缓存过程源码 DefaultSingletonBeanRegistry.java /** Cache of singleton objects: bean name --> bean instance */ private final Map singletonObjects = new ConcurrentHashMap<>(256); /** Cache of singleton factories: bean name --> ObjectFactory */ private final Map> singletonFactories = new HashMap<>(16); /** Cache of early singleton objects: bean name --> bean instance */ private final Map earlySingletonObjects = new HashMap<>(16); protected Object getSingleton(String beanName, boolean allowEarlyReference) { // 从 singletonObjects 获取实例，singletonObjects 中的实例都是准备好的 bean 实例，可以直接使用 Object singletonObject = this.singletonObjects.get(beanName); //isSingletonCurrentlyInCreation() 判断当前单例bean是否正在创建中 if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) { synchronized (this.singletonObjects) { // 一级缓存没有，就去二级缓存找 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { // 二级缓存也没有，就去三级缓存找 ObjectFactory singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { // 三级缓存有的话，就把他移动到二级缓存,.getObject() 后续会讲到 singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject; } AbstractAutowireCapableBeanFactory doCreateBean() // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isTraceEnabled()) { logger.trace(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } // 将实例化后的bean对应的ObjectFactory对象添加到三级缓存中 addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } // Initialize the bean instance. Object exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); exposedObject = initializeBean(beanName, exposedObject, mbd); } protected void addSingletonFactory(String beanName, ObjectFactory singletonFactory) { Assert.notNull(singletonFactory, \"Singleton factory must not be null\"); synchronized (this.singletonObjects) { if (!this.singletonObjects.containsKey(beanName)) { this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); } } } protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) { Object exposedObject = bean; if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { // 判断是否有后置方法，如果有后置方法创建代理对象，否则直接返回原对象 if (bp instanceof SmartInstantiationAwareBeanPostProcessor) { SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); } } } return exposedObject; } AbstractAutoProxyCreator.java public Object getEarlyBeanReference(Object bean, String beanName) { Object cacheKey = this.getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return this.wrapIfNecessary(bean, beanName, cacheKey); } protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { if (StringUtils.hasLength(beanName) && this.targetSourcedBeans.contains(beanName)) { return bean; } else if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) { return bean; } else if (!this.isInfrastructureClass(bean.getClass()) && !this.shouldSkip(bean.getClass(), beanName)) { Object[] specificInterceptors = this.getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, (TargetSource)null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); // 为给定的 bean 创建一个 AOP 代理。 // 参数： // beanClass – bean 的类 // beanName – bean 的名称 // specificInterceptors – 特定于此 bean 的拦截器集（可能为空，但不为空） // targetSource – 代理的 TargetSource，已经预先配置为访问 bean // 返回： // bean 的 AOP 代理 Object proxy = this.createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } else { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } } else { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } } 资料链接 https://jishuin.proginn.com/p/763bfbd2c640 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"spring/SpringBoot.html":{"url":"spring/SpringBoot.html","title":"springboot入门","keywords":"","body":"什么是SpringBoot?项目结构:创建webapp目录,以及使用jsp热部署webjars读取配置文件数据日期格式转换静态文件mysql驱动包 8.x 修改时区使用hikari连接池设置虚拟路径什么是SpringBoot? ​ 简化了Spring项目的搭建和开发过程,可以整合其他框架,简单,快速,快变 项目结构: ​ springboot项目启动后会自动扫描与其在同一目录下的文件夹 创建webapp目录,以及使用jsp ​ 需要添加依赖 javax.servlet jstl org.apache.tomcat.embed tomcat-embed-jasper provided ​ 创建webapp目录应该与Java和resources目录同级 ​ 标记为web目录:Modules --> 点击项目下的Web,将webapp路径添加到下图 热部署 ​ 需要的依赖: org.springframework.boot spring-boot-devtools ​ properties: #开启热部署 spring.devtools.restart.enabled=true #设置重启文件目录 spring.devtools.restart.additional-paths=src/main/java #页面不加载缓存，修改即时生效 spring.thymeleaf.cache=false ​ idea需要的配置: webjars ​ WebJars可以让大家 以Jar 包的形式来使用前端的各种框架、组件。 ​ WebJars 是将客户端（浏览器）资源（JavaScript，Css等）打成 Jar 包文件，以对资源进行统一依赖管理。WebJars 的 Jar 包部署在 Maven 中央仓库上,可以让我们对前端资源也以jar包的方式进行依赖管理 ​ 通过查看mvc的自动配置类可以看到前端资源配置的虚拟路径和映射路径 ​ 引入时,在src创建与pom文件中组件名称相同的空文件夹 webjars/artifactId/version ​ 例如 : src/webjars/jquery/3.3.1 ​ 官网:https://www.webjars.org/ ​ 所需依赖: org.webjars webjars-locator-core org.webjars jquery 3.3.1 ​ 测试访问路径:http://localhost:8080/webjars/jquery/3.3.1/jquery.js ​ 对应在jsp中引用: 读取配置文件数据 ​ @propertySource : 指定配置文件 ​ @ConfigurationProperties : 指定配置文件中属性的前缀 ​ @Value(\"${配置文件中的属性}\") 配置文件 application.properties server.port=8080 user.username=zzz @Component @ConfigurationProperties(prefix = \"user\") @PropertySource(value = \"classpath:application.properties\") public class User { private String username; public String getUsername() { return username; } public void setUsername(String username) { System.out.println(\"通过set方法赋值\"); this.username = username; } @RestController @RequestMapping(\"/hello\") public class HelloController { @Autowired private User user; @Value(\"user.username\") private String username; @GetMapping(\"/user\") public User getUser(){ return user; } @GetMapping(\"/username\") public String getUserName(){ return user.getUsername(); } } 日期格式转换 ​ 前台传递后台按照指定格式接收: ​ 方式一:在bean属性上添加注解 ​ @DateTimeFormat(pattern = \"\") ​ 方式二:在application.properties文件中添加 ​ spring.mvc.date-format=yyyy-MM-dd HH:mm:ss ​ 后台返回给前台json时的data自定义格式字符串: ​ 方式一:在bean属性上添加注解 ​ @JsonFormat(pattern=\"yyyy-MM-dd HH:mm:ss\",timezone=\"GMT+8\") ​ 方式二:在application.properties文件中添加 ​ spring.jackson.time-zone=GMT+8 ​ spring.jackson.date-format=yyyy-MM-dd HH:mm:ss 静态文件 ​ 阅读源码可以看到WebMvcAutoConfiguration 类中addResourceHandlers方法 去读ResourceProperties类中的属性 其中将默认的值赋值给了它,共有四个默认值,其优先级跟下图声明顺序一样 这几个文件夹都是在resources文件夹下的 classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public/ mvc添加该路径方便我们创建webapp目录 还可以添加webapp,在idea中添加该目录需要在project settings -- > models 选中项目的web 有两栏添加第一个提那家web.xml路径,第二个添加webapp路径,如果jsp访问不到 如下配置,别忘记添加依赖 自定义方式: ​ 使用配置文件: ​ 配置文件第一行代表添加一个静态资源文件夹路径,如果没有把默认的配置加上会覆盖默认的设置 ​ 同时使用自定义和默认的,用英文逗号隔开即可,前面的优先级高 ​ 使用java配置类: ​ 实现WebMvcConfigurer接口重写addResourceHandlers,可以达到相同效果 ​ 映射resources下的文件夹或文件以classpath:开头 classpath:/ 代表resources根目录 ​ 映射磁盘下的文件以file:开头 ,其中盘符写绝对路径,盘符后的 冒号 可写可不写 ​ 例如 : file:D:/temp/upload/ ​ file:D/temp/upload/ ​ 使用webapp目录,把js文件放在其根目录下,在webapp根目录和WEB-INF的jsp都可以从所在包直接引用 ​ 注意事项:这两种方式都会覆盖其默认配置,也就是说只配置了自己想要的路径,其默认的便会失效 ​ 关于前端引入可以在webjars中查看 ​ 参考博客 ​ https://www.jianshu.com/p/d40ee98b84b5 ​ https://www.cnblogs.com/sxdcgaq8080/p/7833400.html ​ 江南一点雨 mysql驱动包 8.x 修改时区 spring.datasource.url=jdbc:mysql://localhost:3306/study?useUnicode=true&characterEncoding=utf8&serverTimezone=UTC 使用hikari连接池 导入该启动器,如果不配置其他连接池默认使用hikariCP作为连接池 org.springframework.boot spring-boot-starter-jdbc application.properties 关于该连接池的配置 #数据源配置 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/数据库?useUnicode=true&characterEncoding=utf8&serverTimezone=UTC spring.datasource.username=账号 spring.datasource.password=密码 #连接池配置 #最小空闲连接数 spring.datasource.hikari.minimum-idle=5 #池中最大连接数,包括闲置和使用的连接 spring.datasource.hikari.maximum-pool-size=15 #自动提交从池中返回的连接 spring.datasource.hikari.auto-commit=true #连接允许在池中闲置的最长时间 spring.datasource.hikari.idle-timeout=30000 #连接池的用户定义名称，主要出现在日志记录 spring.datasource.hikari.pool-name=hikariCP #此属性控制池中连接的最长生命周期，值0表示无限生命周期，默认1800000即30分钟 spring.datasource.hikari.max-lifetime=1800000 # 数据库连接超时时间,默认30秒，即30000 spring.datasource.hikari.connection-timeout=30000 spring.datasource.hikari.connection-test-query=SELECT 1 #打印sql日志 #mybatis.mapper-locations=classpath:mapper/*.xml #将数据转换为指定格式+时区返回 #spring.jackson.time-zone=GMT+8 #spring.jackson.date-format=yyyy-MM-dd #将接收的数据以指定格式存储 #spring.mvc.date-format=yyyy-MM-dd #开启热部署 spring.devtools.restart.enabled=true #设置重启文件目录 spring.devtools.restart.additional-paths=src/main/java #页面热部署 #spring.thymeleaf.cache=false 设置虚拟路径 添加配置类 @Configuration public class WebMvcConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { // 前面是访问路径 后面是文件实际存在的路径 registry.addResourceHandler(\"/z/**\").addResourceLocations(\"file:F://code/\"); } } 或者在配置文件中添加 #url访问的请求路径 spring.mvc.static-path-pattern=/zz/** #真实路径 spring.resources.static-locations=file:F://code/ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"boot_integration/javamail.html":{"url":"boot_integration/javamail.html","title":"javamail","keywords":"","body":"JavaMail使用依赖创建javamail.yml创建配置类创建工具类测试JavaMail 在用户登录注册或者进行其他操作时,除了手机号码还可以提供另一种处理方式,就是使用JavaMail,并且该方式也是完全免费的。 需要在不同邮箱提供商的设置POP3/SMTP/IMAP中开启IMAP/SMTP和POP3/SMTP服务 使用 结合springboot项目使用 依赖 javax.mail mail 1.4.7 org.projectlombok lombok 创建javamail.yml javamail: smtp_host: smtp.163.com username: 具体邮箱@163.com password: 授权码,并不是邮箱密码,在邮箱设置处获取 # 和username使用同一个邮箱地址即可 from: 具体邮箱@163.com 创建配置类 import lombok.Data; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.PropertySource; /** * @author :almostTao * @date :Created in 2020/4/9 16:31 */ // 简化getters setters @Data // 指定配置文件名称 @PropertySource(value = \"classpath:javamail.yml\") // 默认使用application.properties 文件 prefix指定前缀 @ConfigurationProperties(prefix = \"javamail\") // 表名该类是一个配置类 @Configuration public class JavaMailConfig { public String smtp_host ; public String username ; public String password ; public String from ; } 创建工具类 /** * @author :almostTao * @date :Created in 2020/4/9 16:19 */ import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import xyz.taoqz.config.JavaMailConfig; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import javax.mail.internet.MimeMessage.RecipientType; import java.util.Properties; // 启动时可以被spring扫描并添加到容器中 @Component public class MailUtil { // 注入配置类 @Autowired private JavaMailConfig javaMailConfig; // 发送邮件主要方法 public void sendMail(String to, String subject, String text) throws Exception { // 1 准备发送邮件需要的参数 Properties props = new Properties(); // 设置主机地址 smtp.qq.com smtp.126.com smtp.163.com props.put(\"mail.smtp.host\",javaMailConfig.smtp_host); // 是否打开验证:只能设置true，必须打开 props.put(\"mail.smtp.auth\", true); // 2 连接邮件服务器 Session session = Session.getDefaultInstance(props); // 3 创建邮件信息 MimeMessage message = new MimeMessage(session); // 4 设置发送者 InternetAddress fromAddress = new InternetAddress(javaMailConfig.from); message.setFrom(fromAddress); // 5 设置接收者 InternetAddress toAddress = new InternetAddress(to); // to:直接接收者 cc：抄送 bcc暗送 message.setRecipient(RecipientType.TO, toAddress); // 6 设置主题 message.setSubject(subject); // 7 设置正文 message.setText(text); // 设置HTML方式发送 //message.setContent(text, \"text/html;charset=utf-8\"); // 8 发送:坐火箭 Transport transport = session.getTransport(\"smtp\");// 参数不能少，表示的是发送协议 // 登录邮箱,此处的密码是授权码 transport.connect(javaMailConfig.username,javaMailConfig.password); transport.sendMessage(message, message.getAllRecipients()); transport.close(); System.out.println(\"ok\"); } } 测试 @SpringBootTest class JavamailDemoApplicationTests { @Test void contextLoads() { } @Autowired private MailUtil mailUtil; @Test public void demo(){ try { mailUtil.sendMail(\"xxx@qq.com\", \"激活测试\", \"test\"); } catch (Exception e) { e.printStackTrace(); } } @Autowired private JavaMailConfig javaMailConfig; @Test public void demo2(){ System.out.println(javaMailConfig); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"boot_integration/springtask.html":{"url":"boot_integration/springtask.html","title":"springtask","keywords":"","body":"SpringTaskSpringTask springboot集成了springtask,不用导依赖可以直接开启定时任务 需要在启动类上添加注解开启定时任务 @EnableScheduling 示例 // 项目启动后可以被spring扫描到 @Component public class QuickStart { private int count = 0; // 每一秒钟执行一次 // @Scheduled(cron = \"*/1 * * * * ?\") // 上一次开始执行时间点之后2秒再执行,与上面corn参数形式一致 // @Scheduled(fixedRate = 2000) // 上一次执行完毕时间点之后2秒再执行 // @Scheduled(fixedDelay = 2000) // 第一次延迟6秒后执行,以后每1秒执行一次 @Scheduled(initialDelay=6000, fixedRate=1000) private void process() { System.out.println(\"定时任务1：\" + (count++)+\"当前时间:\"+ LocalDateTime.now()); } } CronTrigger 的使用 也就是cron参数值应该怎么写 这个定时任务表达式代表： 一些符号的用法： 调度工作, CronTriggerBean调度器介绍 一个cron表达式有至少6个（也可能是7个）由空格分隔的时间元素。从左至右，这些元素的定义如下 参数1：秒 0 - 59 参数2：分钟 0-59 参数3：小时 0-23 参数4：月份中的日期 0-30 参数5：月份 0-11或JAN-DEC 参数6：星期中的日期 1-7或SUN-SAT 参数7：年份1970-2099 每一个元素都可以显式地规定一个值(如6),一个区间(如9-12)，一个列表(如9，11，13)或一个通配符（如*）。“月份中的日期”和“星期中的 日期”这两个元素是互斥的，因此应该通过设置一个问号(?)来表明你不想设置的那个字段 【案例】 0 0 10,14,16 * * ? 每天上午10点，下午2点，4点 0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时，从0分开始每隔30分钟发送一次 0 0 12 ? * WED 表示每个星期三中午12点 \"0 0 12 * * ?\" 每天中午12点触发 \"0 15 10 ? * *\" 每天上午10:15触发 \"0 15 10 * * ?\" 每天上午10:15触发 \"0 15 10 * * ? *\" 每天上午10:15触发 \"0 15 10 * * ? 2005\" 2005年的每天上午10:15触发 \"0 * 14 * * ?\" 在每天下午2点到下午2:59期间的每1分钟触发 \"0 0/55 14 * * ?\" 在每天下午2点到下午2:55期间的每5分钟触发 \"0 0/55 14,18 * * ?\" 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发 \"0 0-5 14 * * ?\" 在每天下午2点到下午2:05期间的每1分钟触发 \"0 10,44 14 ? 3 WED\" 每年三月的星期三的下午2:10和2:44触发 \"0 15 10 ? * MON-FRI\" 周一至周五的上午10:15触发 \"0 15 10 15 * ?\" 每月15日上午10:15触发 \"0 15 10 L * ?\" 每月最后一日的上午10:15触发 \"0 15 10 ? * 6L\" 每月的最后一个星期五上午10:15触发 \"0 15 10 ? * 6L 2002-2005\" 2002年至2005年的每月的最后一个星期五上午10:15触发 \"0 15 10 ? * 6#3\" 每月的第三个星期五上午10:15触发 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"spring/transactional/@Transactional.html":{"url":"spring/transactional/@Transactional.html","title":"@Transactional","keywords":"","body":"@Transactional作用域事务的传播行为spring中七种事务传播行为常用的传播行为以及使用注意事项REQUIRED、REQUIRES_NEW和NESTED 的异同注解失效场景使用注意事项@Transactional 本人理解:其实单个或多个方法的事务,主要是看他们的数据库连接是否为同一个,也就是把事务方法里的mysql语句集合当作原子操作 作用域 类、接口、方法 作用在类上时表示该类下的所有public的方法都会有事务 作用在接口上时不管是注入接口或者接口的实现类,调用实现类的方法都会有回滚的效果(无论是否为重写的接口的方法) 注解添加在public的方法上才会生效 事务的传播行为 事务的传播行为用来描述一个事务方法被嵌套入另一个事务方法时事务的行为 spring中七种事务传播行为 常用的传播行为以及使用注意事项 Propagation.REQUIRED : 外围方法没有开启事务时,会开启自己的事务,且开启的事务相互独立 外围方法开启事务时,内部方法会加入到外部方法的事务中,其中只要有一个方法回滚,整个事务均回滚 Propagation.REQUIRES_NEW : 外围方法未开启事务,会在内部开启新的事务,且相互独立,外围方法开启事务,被修饰的内部方法依然开启新的事务,且与内外部事务方法相互独立 Propagation.NESTED : 外围方法未开启事务时,内部方法开启新的事务,且相互之间独立(外围方法没有事务时与REQUIRED作用一致),外围方法开启事务时,内部方法的事务是外部事务的子事务,即外围事务回滚,子事务一定回滚,而内部子事务可以单独回滚而不影响外围事务和其他子事务 REQUIRED、REQUIRES_NEW和NESTED 的异同 REQUIRED和NESTED修饰的内部方法都属于外围方法事务,只要外围事务方法抛出异常,这两种传播机制的事务方法也会回滚。但是区别在于 REQUIRED事务抛出异常外围事务也会回滚,而NESTED是外围事务的子事务,内部方法抛出异常不会影响外围事务 NESTED和REQUIRES_NEW修饰的方法都可以做到内部事务方法抛出异常不会影响外部事务方法,但是区别在于NESTED是外部事务的子事务,会受到外部事务的影响而回滚,而REQUIRES_NEW不会受到外部事务的影响 注解失效场景 应用在非public方法上 同一类中方法调用(this) 异常被try catch,下面这段代码,mybatis()和jpa()方法都没有添加@Transactional(跟事务传播行为没有关系),mybatis()方法内部会抛出异常,但是被捕捉后没有影响整体事务的提交(需要结合) @Transactional public void demo() { // jpa和mybatis上都没有加@Transactional注解 jpaService.jpa(); try { // 该方法内部会抛出异常 // 如果该方法添加@Transactional注解,还会抛出如下异常 // org.springframework.transaction.UnexpectedRollbackException: Transaction silently rolled back because it has been marked as rollback-only // 事务以静默方式回滚，因为它已被标记为“仅回滚” // 因为@Transactional注解的propagation的默认值为Propagation.REQUIRED // 表示内部方法和外部方法是同一个事务,内部方法报错需要回滚,虽然外部方法捕捉异常,但此时的事务已经被标记为回滚了 mybatisService.mybatis(); } catch (Exception e) { e.printStackTrace(); } } 数据库引擎不支持事务:针对mysql来说,默认呢引擎为innodb支持事务,切换为myisam则从底层就不支持事务了 使用注意事项 该注解是通过aop来保证事务的,所以只有是通过代理的对象调用的方法,才会管用(this无效) 常用的两个属性 rollbackFor: 表示被该注解修饰的方法在遇到什么类型的异常时回滚,如果是非RuntimeException，是要显示用rollbackFor指定的,不然不会回滚 propagation: 表示该注解修饰的事务方法在遇到另一个事务方法时的传播行为,属性值默认为Propagation.REQUIRED 当事务的传播行为是REQUIRED时,方法内部抛出异常但是在外围事务中try catch还是会被感知导致外围和内部事务都会回滚 // springboot还提供了回滚当前事务的接口方法 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); 学习链接 https://mp.weixin.qq.com/s/IglQITCkmx7Lpz60QOW7HA https://mp.weixin.qq.com/s/XTPlEIDfak_1Ocyn9c5-IA Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"rabbitmq/rabbitmq.html":{"url":"rabbitmq/rabbitmq.html","title":"RabbitMQ","keywords":"","body":"Rabbitmq应用场景流量削峰应用解耦异步处理核心术语支持模式helloworld（代码示例）workqueue消息应答机制（ack）自动应答手动应答批量应答（同样不推荐使用）消息自动重新入队（代码示例）持久化队列持久化消息持久化不公平分发预取值发布确认单个确认发布批量确认发布异步确认发布交换机ExchangesExchanges的类型无名exchange临时队列绑定（bingdings）Fanout（扇出）Direct（直接）主题（topic）死信队列延迟队列通过插件实现延迟队列发布确认高级vue使用rabbitmqRabbitmq 应用场景 流量削峰 比如秒杀下订单时会有大量用户在同一时间请求服务，可能会导致服务因此宕机，可以引入消息队列，对请求进行排队和限制，然后慢慢消化 应用解耦 一个模块的处理需要强依赖另一个模块时，假如被依赖的模块出现故障，自然也会影响调用模块的运行，引入消息队列后，可将先将请求参数保存至消息队列中，被依赖模块从消息队列中获取请求参数 异步处理 假如一个服务调用另一个服务时间较久时，可以使用消息队列进行异步处理，将请求参数发送至消息队列，再由服务获取消息队列中的数据进行慢慢消化（还比如一些不需要强制关联的操作，注册和发送邮箱确认，注册后可以立即给出响应，发邮件的操作可以依靠消息队列去做） 核心术语 生产者 ：产生消息发送至消息队列 交换机 : 接收消息，发送至绑定的指定队列 队列 : 存储接收来自交换机分配的消息 消费者：消费消息，对消息做处理 连接：生产者和消费者与消息队列的TCP连接 信道：为了解决频繁创建连接消耗资源的问题，一个连接中可有多个信道发送消息 broker: exchage（交换机） + queue（队列） vhost（多租户）: 多个用户可以通过vhost标识权限区分消息队列，也就是可以有多个broker 支持模式 helloworld（代码示例） public static final String queue_name = \"hello\"; public static void main(String[] args) throws IOException, TimeoutException { // 创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); // 创建连接 final Connection connection = factory.newConnection(); // 获取信道 final Channel channel = connection.createChannel(); // 创建队列 /** * 队列名称 * 是否持久化 默认false * 队列是否进行消息共享（默认只有一个消费者可以消费） * 是否自动删除，最后一个消费者断开连接以后，该队列是否自动删除 * 其他参数 */ channel.queueDeclare(queue_name, false, false, false, null); String message = \"HelloWorld！\"; /** * 交换机 * 路由key值，本次为队列名称 * 其他参数 * 消息体 */ channel.basicPublish(\"\", queue_name, null, message.getBytes(StandardCharsets.UTF_8)); } public static String queue_name = \"hello\"; public static void main(String[] args) throws IOException, TimeoutException { final ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); final Connection connection = factory.newConnection(); final Channel channel = connection.createChannel(); DeliverCallback deliverCallback = (consumerTag, message) -> { System.out.println(new String(message.getBody())); }; CancelCallback cancelCallback = consumerTag -> { System.out.println(\"消息被中断\"); }; /** * 消费哪个队列 * 消费成功是否自动应答 true 自动应答 * 消费者未成功消费的回调 * 消费者取消消费回调 */ channel.basicConsume(queue_name, true, deliverCallback, cancelCallback); } workqueue 会遇到生产者大量生产消息的场景，消费者可以转为工作线程，工作线程之间是竞争关系，消息被其中一个工作线程（队列）消费后，不允许被其他工作线程消费，也就是不能重复消费（rabbitmq默认会轮询消费） 消息应答机制（ack） 为了保证消息不丢失，rabbitmq引入了消息应答机制，消费者在接收到消息并且处理该消息之后，告诉rabbitmq它已经处理了，rabbitmq可以把该消息删除了 自动应答 需要在高吞吐量和数据传输安全性方面做权衡（需要良好的情况），也就是消费者接收后就自动应答了，此时会有消息在处理的过程中出现问题的情况 手动应答 手动应答的好处是可以批量应答并且减少网络拥堵 批量应答（同样不推荐使用） 消费者从信道中获取消息，信道中有多少条消息取决与队列向信道里放了多少条消息，如果有多条消息时选择使用批量应答，假如在第一条消息处理时就选择了应答，但是后几条消息还没有被处理，不确定是否会成功，所以会有消息丢失的可能 消息自动重新入队（代码示例） 如果消费者由于某些原因失去连接（通道关闭，连接关闭或者TCP断开连接），导致消息未发送ack确认，rabbitmq将了解到消息未完全处理，并将对其重新排队。如果此时其他消费者可以处理，它将很快将其重新分发给另一个消费者。这样即使某个消费者偶尔死亡，也可以确保消息不回丢失。 可以创建两个消费者，生产两条消息，手动停止一个消费者，检测消息是否会重新入队会被另一个消费者所消费 public static final String queue_name = \"ack_queue\"; public static void main(String[] args) throws IOException, TimeoutException { final Channel channel = RabbitMQUtil.getChannel(); System.out.println(\"消费者1开始接收消息。。。\"); DeliverCallback deliverCallback = (consumerTag, message) -> { System.out.println(\"process...\"); System.out.println(new String(message.getBody())); // 消息唯一Id 是否批量应答 channel.basicAck(message.getEnvelope().getDeliveryTag(), false); System.out.println(\"end\"); }; // 手动应答 boolean autoAck = false; channel.basicConsume(queue_name, autoAck, deliverCallback, (consumerTag) -> { System.out.println(\"消息取消发送\"); }); } 持久化 队列持久化 服务重启后，队列还存在，但消息已经不在了 // 如果队列已经在指定的vhost消息队列中存在，就会出现此异常，需要删掉重新建队列 // Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for queue 'hello' in vhost '/': received 'true' but current is 'false', class-id=50, method-id=10) public static final String queue_name = \"hello\"; public static final String durable_queue_name = \"durable_queue\"; public static void main(String[] args) throws IOException, TimeoutException { final Channel channel = RabbitMQUtil.getChannel(); boolean durable = true; channel.queueDeclare(durable_queue_name, durable, false, false, null); String msg = \"durable_msg\"; channel.basicPublish(\"\", durable_queue_name, null, msg.getBytes(StandardCharsets.UTF_8)); } 消息持久化 服务重启后，队列和消息都存在 public static final String durable_queue_name = \"durable_queue\"; public static void main(String[] args) throws IOException, TimeoutException { final Channel channel = RabbitMQUtil.getChannel(); boolean durable = true; channel.queueDeclare(durable_queue_name, durable, false, false, null); String msg = \"durable_msg——PERSISTENT_TEXT_PLAIN\"; // MessageProperties.PERSISTENT_TEXT_PLAIN 要求消息持久化 channel.basicPublish(\"\", durable_queue_name, MessageProperties.PERSISTENT_TEXT_PLAIN, msg.getBytes(StandardCharsets.UTF_8)); } 不公平分发 如果使用轮询，对消费能力强的消费端是一种浪费，当然对于消费能力弱的消费端来说就是压力较大 通过在消费端设置信道改为不公平分发（经测试如果有两个消费端，虽然只在运行较慢一端设置也可生效，只在运行较快一端设置不生效，最好都设置），默认0轮询 // 在每个消费端设置不公平分发 channel.basicQos(1); 预取值 相当于消费者的权重 // 预取值 int prefetchCount = 5; channel.basicQos(prefetchCount); 发布确认 保证消息不丢失的重要功能，前面的持久化（队列、消息持久化）虽然也能保证消息不丢失，但是在消息发送的过程宕机了，消息有可能丢失。 这里的发布确认指生产者发送消息至rabbitmq服务端时是否发送成功。 单个确认发布 生产者端设置 final Channel channel = RabbitMQUtil.getChannel(); // 开启消息发布确认 channel.confirmSelect(); final String queueName = UUID.randomUUID().toString(); // channel.queueDeclare(queueName, false, false, false, null); final StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i 发布速度慢 批量确认发布 发布速度快，但追踪不到发送失败的具体消息 异步确认发布 在发送消息之前声明监听器 /** * 消息发布成功回调 * 消息发布失败回调 * deliveryTag： 消息的标记 * multiple： 是否未批量确认 */ channel.addConfirmListener( (deliveryTag, multiple) -> { System.out.println(\"消息发送成功：\" + deliveryTag + \" \" + multiple); }, (deliveryTag, multiple) -> { System.out.println(\"消息发送失败：\" + deliveryTag + \" \" + multiple); } ); 如何处理未确认的消息 因为发布消息和确认消息是两个线程，可以使用并发类存储所有的消息，将成功发布的消息删除，剩余的就是未处理的消息。 交换机Exchanges ​ RabbitMQ消息传递模型的核心思想是：生产者生产的消息从不会直接发送到队列。实际上，通常生产者甚至都不知道这些消息传递到了哪些队列中。 ​ 相反，生产者只能将消息发送到交换机，交换机工作的内容非常简单，一方面它接收来自生产者的消息，另一方面将它们推入队列。交换机必须确切知道如何处理收到的消息。是应该把这些消息放到特定队列还是说把它们放到许多队列中还是应该丢弃他们。这就由交换机的类型来决定。 Exchanges的类型 扇出（fanout），直接（direct），主题（topic） 无名exchange /** * 交换机名称（空字符串表示默认或无名称交换机(RabbitMQ会有一个默认的交换机)） * 路由key值（routingKey），本次为队列名称（消息能路由发送到队列中是由routingKey(bindingKey)绑定的key指定的） * 其他参数 * 消息体 */ channel.basicPublish(\"\", queue_name, null, message.getBytes(StandardCharsets.UTF_8)); 临时队列 // 创建临时(一个具有随即名称的)队列，一旦消费者断开连接后队列会自动删除 String queueName = channel.queueDeclare().getQueue(); 绑定（bingdings） banding其实是exchange和queue之间的桥梁。 Fanout（扇出） fanout模式下会将所有消息发送至所有绑定到该交换机的所有队列（跟routingKey无关） RabbitMQUtil public static Channel getChannel() throws IOException, TimeoutException { final ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); final Connection connection = factory.newConnection(); final Channel channel = connection.createChannel(); return channel; } public static final String exchange_name = \"fanout_exchange\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); // fanout(扇出)：将所有发送到交换机的消息路由到所有与它绑定的queue中 channel.exchangeDeclare(exchange_name, BuiltinExchangeType.FANOUT); final Scanner scanner = new Scanner(System.in); while (true) { final String msg = scanner.nextLine(); channel.basicPublish(exchange_name, \"\", null, msg.getBytes(StandardCharsets.UTF_8)); } } public static final String exchange_name = \"fanout_exchange\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); channel.exchangeDeclare(exchange_name, BuiltinExchangeType.FANOUT); // 创建临时队列，消费者断开连接后队列自动删除 final String queueName = channel.queueDeclare().getQueue(); // 将队列绑定至交换机 channel.queueBind(queueName,exchange_name,\"\"); channel.basicConsume(queueName,true,(consumerTag,msg) -> { System.out.println(\"消费者1接收到消息：\"+new String(msg.getBody())); },(consumerTag)-> {}); } Direct（直接） 直接模式下会将消息发送至与交换机绑定后并且routingKey完全匹配的队列中 public static final String exchange_name = \"direct_exchange\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); channel.exchangeDeclare(exchange_name, BuiltinExchangeType.DIRECT); final Scanner scanner = new Scanner(System.in); while (true) { System.out.println(\"请输入routingkey\"); final String routingKey = scanner.nextLine(); System.out.println(\"请输入msg\"); final String msg = scanner.nextLine(); channel.basicPublish(exchange_name, routingKey, null, msg.getBytes(StandardCharsets.UTF_8)); } } public static final String exchange_name = \"direct_exchange\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); channel.exchangeDeclare(exchange_name, BuiltinExchangeType.DIRECT); final String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, exchange_name, \"c1\"); System.out.println(queueName); // 多个队列绑定同一个routingKey final String queueName2 = channel.queueDeclare().getQueue(); channel.queueBind(queueName2, exchange_name, \"c1\"); System.out.println(queueName2); // 一个队列绑定多个routingKey // channel.queueBind(queueName, exchange_name, \"c3\"); channel.basicConsume(queueName, true, ((consumerTag, message) -> { System.out.println(\"消费者1接收到消息：\" + new String(message.getBody())); }), (consumerTag, sig) -> { }); channel.basicConsume(queueName2, true, ((consumerTag, message) -> { System.out.println(\"消费者1接收到消息_：\" + queueName2 + new String(message.getBody())); }), (consumerTag, sig) -> { }); } public static final String exchange_name = \"direct_exchange\"; public static void main(String[] args) throws Exception{ final Channel channel = RabbitMQUtil.getChannel(); channel.exchangeDeclare(exchange_name, BuiltinExchangeType.DIRECT); final String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName,exchange_name,\"c2\"); channel.basicConsume(queueName,true,((consumerTag, message) -> { System.out.println(\"消费者2接收到消息：\"+new String(message.getBody())); }),(consumerTag, sig) -> {}); } 主题（topic） ​ topic模式下的routingKey匹配会更加灵活，但也必须满足一定的要求，它必须是一个单词列表，以点号分隔开，这些单词可以是任意单词，比如\"abc.aaa.ccc\",\"s.asf.vdf\",这个单词列表最多不能超过255个字节。 ​ 规则：*（星号）可以代替一个单词；#（＃号）可以代替零个或多个单词。 ​ 注意：当一个队列绑定键是#，那么这个队列将接收所有数据，就有点像fanout了；如果队列绑定键当中没有#和*出现，那么该队列绑定类型就是direct了。 消费者1 public static final String excange_name = \"topic_exchange\"; public static final String routingKey = \"*.orange.*\"; public static final String queue_name = \"Q1\"; public static void main(String[] args) throws Exception { // 获取信道 final Channel channel = RabbitMQUtil.getChannel(); // 声明并创建队列 channel.queueDeclare(queue_name, false, false, false, null); // 声明并创建交换机 channel.exchangeDeclare(excange_name, BuiltinExchangeType.TOPIC); // 将队列绑定至交换机 channel.queueBind(queue_name, excange_name, routingKey); // 接收消息 channel.basicConsume(queue_name, true, (consumerTag, message) -> { System.out.println(\"队列Q1接收到消息：\" + new String(message.getBody()) + \"所属routingKey：\" + message.getEnvelope().getRoutingKey()); }, (consumerTag) -> { }); } 消费者2 public static final String excange_name = \"topic_exchange\"; public static final String routingKeyOne = \"*.*.rabbit\"; public static final String routingKeyTwo = \"lazy.#\"; public static final String queue_name = \"Q2\"; public static void main(String[] args) throws Exception { // 获取信道 final Channel channel = RabbitMQUtil.getChannel(); // 声明并创建队列 channel.queueDeclare(queue_name, false, false, false, null); // 声明并创建交换机 // channel.exchangeDeclare(excange_name, BuiltinExchangeType.TOPIC); // 将队列绑定至交换机 channel.queueBind(queue_name, excange_name, routingKeyOne); channel.queueBind(queue_name, excange_name, routingKeyTwo); // 接收消息 channel.basicConsume(queue_name, true, (consumerTag, message) -> { System.out.println(\"队列Q2接收到消息：\" + new String(message.getBody()) + \"所属routingKey：\" + message.getEnvelope().getRoutingKey()); }, (consumerTag) -> { }); } 生产者 public static final String exchange_name = \"topic_exchange\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); // channel.exchangeDeclare(exchange_name, BuiltinExchangeType.TOPIC); final Map map = new HashMap<>(); map.put(\"quick.orange.rabbit\", \"被队列Q1Q2接收\"); map.put(\"lazy.orange.elephant\", \"被队列Q1Q2接收\"); map.put(\"quick.orange.fox\", \"被队列Q1接收\"); map.put(\"lazy.brown.fox\", \"被队列Q2接收\"); map.put(\"lazy.pink.rabbit\", \"虽然满足两个绑定但只被Q2队列（两个routingKey绑定到了同一个队列上）接收一次\"); map.put(\"quick.brown.fox\", \"不匹配任何绑定不会被任何队列接收会被丢弃\"); map.put(\"quick.orange.male.rabbit\", \"是四个单词不会匹配任何绑定会被丢弃\"); map.put(\"lazy.orange.male.rabbit\", \"是四个单词会匹配到队列Q2\"); for (Map.Entry entry : map.entrySet()) { final String routingKey = entry.getKey(); final String message = entry.getValue(); channel.basicPublish(exchange_name, routingKey, null, message.getBytes(StandardCharsets.UTF_8)); } } console1 队列Q1接收到消息：被队列Q1Q2接收所属routingKey：lazy.orange.elephant 队列Q1接收到消息：被队列Q1Q2接收所属routingKey：quick.orange.rabbit 队列Q1接收到消息：被队列Q1接收所属routingKey：quick.orange.fox console2 队列Q2接收到消息：被队列Q1Q2接收所属routingKey：lazy.orange.elephant 队列Q2接收到消息：被队列Q2接收所属routingKey：lazy.brown.fox 队列Q2接收到消息：被队列Q1Q2接收所属routingKey：quick.orange.rabbit 队列Q2接收到消息：虽然满足两个绑定但只被Q2队列（两个routingKey绑定到了同一个队列上）接收一次所属routingKey：lazy.pink.rabbit 队列Q2接收到消息：是四个单词会匹配到队列Q2所属routingKey：lazy.orange.male.rabbit 死信队列 死信队列本质和正常队列没有区别，用来保存队列因为各种原因无法被消费的消息 应用场景：为了保证订单业务信息数据不丢失；用户下单成功并点击支付后指定时间未支付时自动失效 消息TTL（过期时间）过期 public static final String normal_exchange = \"normal_exchange\"; public static final String normal_routingKey = \"normal_routingKey\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); // 设置消息的TTL时间 10s build的单位为ms final AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().expiration(\"10000\").build(); for (int i = 0; i // 普通交换机和队列 public static final String normal_exchange = \"normal_exchange\"; public static final String normal_routingKey = \"normal_routingKey\"; public static final String normal_queue_name = \"normal_queue\"; // 接收死信的交换机和队列 public static final String dead_exchange = \"dead_exchange\"; public static final String dead_routingKey = \"dead_routingKey\"; public static final String dead_queue_name = \"dead_queue\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); // 声明并创建普通和死信交换机 channel.exchangeDeclare(normal_exchange, BuiltinExchangeType.DIRECT); channel.exchangeDeclare(dead_exchange, BuiltinExchangeType.DIRECT); // 声明并创建普通和死信队列 channel.queueDeclare(dead_queue_name, false, false, false, null); final Map arguments = new HashMap<>(); // 设置消息过期时间 单位ms,该参数建议设置在生产者，扩展性高 // arguments.put(\"x-message-ttl\", 10000); // 为正常队列设置死信交换机 arguments.put(\"x-dead-letter-exchange\", dead_exchange); // 为正常队列设置死信routingKey arguments.put(\"x-dead-letter-routing-key\", dead_routingKey); channel.queueDeclare(normal_queue_name, false, false, false, arguments); // 绑定队列 channel.queueBind(dead_queue_name, dead_exchange, dead_routingKey); channel.queueBind(normal_queue_name, normal_exchange, normal_routingKey); channel.basicConsume(normal_queue_name, true, normal_routingKey, (consumerTag, message) -> { System.out.println(\"消费者接收消息：\" + new String(message.getBody())); }, (consumer) -> { }); } 重要代码 生产者 // 设置消息的TTL时间 10s build的单位为ms final AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().expiration(\"10000\").build(); // 发送消息 channel.basicPublish(normal_exchange, normal_routingKey, properties, message.getBytes(StandardCharsets.UTF_8)); 或者在消费者设置 final Map arguments = new HashMap<>(); // 设置消息过期时间 单位ms,该参数建议设置在生产者，扩展性高 // arguments.put(\"x-message-ttl\", 10000); // 为正常队列设置死信交换机 arguments.put(\"x-dead-letter-exchange\", dead_exchange); // 为正常队列设置死信routingKey arguments.put(\"x-dead-letter-routing-key\", dead_routingKey); channel.queueDeclare(normal_queue_name, false, false, false, arguments); 启动消费者创建交换机和队列后关闭，再开启生产者 过期后进死信队列 队列达到最大长度 // 普通交换机和队列 public static final String normal_exchange = \"normal_exchange\"; public static final String normal_routingKey = \"normal_routingKey\"; public static final String normal_queue_name = \"normal_queue\"; // 接收死信的交换机和队列 public static final String dead_exchange = \"dead_exchange\"; public static final String dead_routingKey = \"dead_routingKey\"; public static final String dead_queue_name = \"dead_queue\"; public static void main(String[] args) throws Exception{ final Channel channel = RabbitMQUtil.getChannel(); // 声明并创建普通和死信交换机 channel.exchangeDeclare(normal_exchange, BuiltinExchangeType.DIRECT); // channel.exchangeDeclare(dead_exchange, BuiltinExchangeType.DIRECT); // 声明并创建普通和死信队列 // channel.queueDeclare(dead_queue_name, false, false, false, null); // 由于参数改变需要先删除原来的队列，再新建 // channel.queueDelete(normal_queue_name); final Map arguments = new HashMap<>(); // 为正常队列设置死信交换机 arguments.put(\"x-dead-letter-exchange\", dead_exchange); // 为正常队列设置死信routingKey arguments.put(\"x-dead-letter-routing-key\", dead_routingKey); // 为正常队列设置queue最大长度 一定要是数字类型 5 。。。 arguments.put(\"x-max-length\", 5); channel.queueDeclare(normal_queue_name, false, false, false, arguments); // 绑定队列 // channel.queueBind(dead_queue_name, dead_exchange, dead_routingKey); channel.queueBind(normal_queue_name, normal_exchange, normal_routingKey); channel.basicConsume(normal_queue_name, true, normal_routingKey, (consumerTag, message) -> { System.out.println(\"消费者接收消息：\" + new String(message.getBody())); }, (consumer) -> { }); } 重要代码 消费者设置参数 final Map arguments = new HashMap<>(); // 为正常队列设置死信交换机 arguments.put(\"x-dead-letter-exchange\", dead_exchange); // 为正常队列设置死信routingKey arguments.put(\"x-dead-letter-routing-key\", dead_routingKey); // 为正常队列设置queue最大长度 一定要是数字类型 5 。。。 arguments.put(\"x-max-length\", 5); channel.queueDeclare(normal_queue_name, false, false, false, arguments); 消息被拒绝 // 普通交换机和队列 public static final String normal_exchange = \"normal_exchange\"; public static final String normal_routingKey = \"normal_routingKey\"; public static final String normal_queue_name = \"normal_queue\"; // 接收死信的交换机和队列 public static final String dead_exchange = \"dead_exchange\"; public static final String dead_routingKey = \"dead_routingKey\"; public static final String dead_queue_name = \"dead_queue\"; public static void main(String[] args) throws Exception { final Channel channel = RabbitMQUtil.getChannel(); // 声明并创建普通和死信交换机 channel.exchangeDeclare(normal_exchange, BuiltinExchangeType.DIRECT); // 由于参数改变需要先删除原来的队列，再新建 channel.queueDelete(normal_queue_name); final Map arguments = new HashMap<>(); // 为正常队列设置死信交换机 arguments.put(\"x-dead-letter-exchange\", dead_exchange); // 为正常队列设置死信routingKey arguments.put(\"x-dead-letter-routing-key\", dead_routingKey); channel.queueDeclare(normal_queue_name, false, false, false, arguments); // 绑定队列 channel.queueBind(normal_queue_name, normal_exchange, normal_routingKey); boolean autoAck = false; channel.basicConsume(normal_queue_name, autoAck, normal_routingKey, (consumerTag, message) -> { final String msg = new String(message.getBody()); if (msg.equals(\"info4\")) { System.out.println(\"消费者接收消息：\" + msg + \"并拒绝接收消息\"); // requeue 是否重新放回正常队列，选择否 channel.basicReject(message.getEnvelope().getDeliveryTag(), false); } else { System.out.println(\"消费者接收消息：\" + msg); channel.basicAck(message.getEnvelope().getDeliveryTag(), false); } }, (consumer) -> { }); 重要代码 消费者 boolean autoAck = false; channel.basicConsume(normal_queue_name, autoAck, normal_routingKey, (consumerTag, message) -> { final String msg = new String(message.getBody()); if (msg.equals(\"info4\")) { System.out.println(\"消费者接收消息：\" + msg + \"并拒绝接收消息\"); // requeue 是否重新放回正常队列，选择否 channel.basicReject(message.getEnvelope().getDeliveryTag(), false); } else { System.out.println(\"消费者接收消息：\" + msg); channel.basicAck(message.getEnvelope().getDeliveryTag(), false); } }, (consumer) -> { }); 延迟队列 ​ 延迟队列最重要的特性就体现再它的延时属性上，延时队列中的元素是希望在指定时间到了以后或之前取出和处理，简单来说，延时队列就是用来存放需要在指定时间被处理的元素的队列。 使用场景 订单在10分钟之内未支付则自动取消。 用户注册成功后，如果三天内没有登录则进行短信提醒。 用户发起退款，如果三天内没有得到处理则通知相关运营人员。 TTL ​ TTL是RabbitMQ中一个消息或队列的属性，表明一条消息或者该队列中的所有消息的最大存活时间，单位为毫秒，如果消息在设置的指定时间内没有被消费，则会成为死信，如果同时配置了队列的TTL和消息的TTL，会使用较小的TTL值。 消息设置TTL rabbitTemplate.convertAndSend(x_exchange, xc_routingKey, \"消息来自ttl为10的队列QA：\" + msg, message -> { message.getMessageProperties().setExpiration(ttl); return message; }); 队列设置TTL @Bean(\"QB\") public Queue QB() { // 或者使用 map参数 x-message-ttl return QueueBuilder.durable(qb_queue) .deadLetterExchange(dead_y_exchange) .deadLetterRoutingKey(dead_yd_routingKey) .ttl(30000) .build(); } 区别 ​ 如果设置了队列的TTL时间，消息一旦过期，就会被丢弃（如果配置了死信队列，会被丢到死信队列中），如果配置了消息的TTL时间，当投递到消费者队列中时，如果消费者队列由消息积压的情况，消息可能还会存活较长时间，如果不设置TTL，表示消息永远不会过期，如果设置为0，表示除非可以直接投递消息到消费者，否则该消息将会被丢弃。 整合springboot pom.xml org.projectlombok lombok org.springframework.boot spring-boot-starter-web org.springframework.boot spring-boot-starter-amqp application.yml spring: rabbitmq: host: 127.0.0.1 port: 5672 username: admin password: admin 配置类 根据上图声明并创建交换机、队列，并进行绑定。 package xyz.taoqz.bootlearn.config; import org.springframework.amqp.core.*; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * RabbitMQConfig * * @author taoqz * @date 2021/6/27 19:37 */ @Configuration public class RabbitMQConfig { private final String x_exchange = \"x.exchange\"; private final String qa_queue = \"QA\"; private final String qb_queue = \"QB\"; private final String qc_queue = \"QC\"; private final String xa_routingKey = \"XA\"; private final String xb_routingKey = \"XB\"; private final String xc_routingKey = \"XC\"; private final String dead_y_exchange = \"dead.y.exchange\"; private final String dead_qd_queue = \"QD\"; private final String dead_yd_routingKey = \"YD\"; @Bean(\"xExchange\") public DirectExchange xExchange() { return new DirectExchange(x_exchange); } @Bean(\"QA\") public Queue QA() { return QueueBuilder.durable(qa_queue) .deadLetterExchange(dead_y_exchange) .deadLetterRoutingKey(dead_yd_routingKey) .ttl(10000) .build(); } @Bean(\"QB\") public Queue QB() { return QueueBuilder.durable(qb_queue) .deadLetterExchange(dead_y_exchange) .deadLetterRoutingKey(dead_yd_routingKey) .ttl(30000) .build(); } @Bean(\"QC\") public Queue QC() { return QueueBuilder.durable(qc_queue) .deadLetterExchange(dead_y_exchange) .deadLetterRoutingKey(dead_yd_routingKey) .build(); } @Bean public Binding QABindingXExchange(@Qualifier(\"QA\") Queue qa, @Qualifier(\"xExchange\") DirectExchange xExchange){ return BindingBuilder.bind(qa).to(xExchange).with(xa_routingKey); } @Bean public Binding QBBindingXExchange(@Qualifier(\"QB\") Queue qb, @Qualifier(\"xExchange\") DirectExchange xExchange){ return BindingBuilder.bind(qb).to(xExchange).with(xb_routingKey); } @Bean public Binding QCBindingXExchange(@Qualifier(\"QC\") Queue qc, @Qualifier(\"xExchange\") DirectExchange xExchange){ return BindingBuilder.bind(qc).to(xExchange).with(xc_routingKey); } @Bean(\"yExchange\") public DirectExchange yExchange() { return new DirectExchange(dead_y_exchange); } @Bean(\"QD\") public Queue QD() { return QueueBuilder.durable(dead_qd_queue) .build(); } @Bean public Binding QDBindingXExchange(@Qualifier(\"QD\") Queue qd, @Qualifier(\"yExchange\") DirectExchange yExchange){ return BindingBuilder.bind(qd).to(yExchange).with(dead_yd_routingKey); } } 生产者 @Slf4j @RestController @RequestMapping(\"/rabbitmq\") public class RabbitMQController { private final String x_exchange = \"x.exchange\"; private final String xa_routingKey = \"XA\"; private final String xb_routingKey = \"XB\"; private final String xc_routingKey = \"XC\"; private final RabbitTemplate rabbitTemplate; public RabbitMQController(RabbitTemplate rabbitTemplate) { this.rabbitTemplate = rabbitTemplate; } @GetMapping(\"/sendTtlMsg/{msg}\") public String sendTtlMsg(@PathVariable String msg) { rabbitTemplate.convertAndSend(x_exchange, xa_routingKey, \"消息来自ttl为10的队列QA：\" + msg); rabbitTemplate.convertAndSend(x_exchange, xb_routingKey, \"消息来自ttl为30的队列QA：\" + msg); log.info(\"发送消息，时间：{}\", new Date().toString()); return \"sendTtl OK!\"; } @GetMapping(\"/sendExpirationMsg/{msg}/{ttl}\") public String sendExpirationMsg(@PathVariable String msg, @PathVariable String ttl) { rabbitTemplate.convertAndSend(x_exchange, xc_routingKey, \"消息来自ttl为10的队列QA：\" + msg, message -> { message.getMessageProperties().setExpiration(ttl); return message; }); log.info(\"发送消息，ttl:{}，当前时间：{}\", ttl, new Date().toString()); return \"sendExpirationMsg OK!\"; } } 消费者 package xyz.taoqz.bootlearn.rabbitmqlistener; import com.rabbitmq.client.Channel; import lombok.extern.slf4j.Slf4j; import org.springframework.amqp.core.Message; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.stereotype.Component; import java.util.Date; /** * TtlListener * * @author taoqz * @date 2021/6/27 20:04 */ @Slf4j @Component public class TtlListener { @RabbitListener(queues = \"QD\") public void receiveD(Message message, Channel channel) { final String msg = new String(message.getBody()); log.info(\"当前时间：{}，接收到消息：{}\", new Date().toString(), msg); } } sendTtlMsg 两个队列分别配置10s和30s的消息ttl时间 sendExpirationMsg 消息分别设置不同的ttl时间，先发送20s后发送10s，可以看到没有达到想要的效果，10s的消息没有被先消费，而是等待20s的消息消费后在消费，这是设置单纯消息过期时间的问题。 因为RabbitMQ只会检查第一个消息是否过期，如果过期则丢进死信队列，如果第一个消息的延时时间比第二个消息的延时时间还长，第二个消息不回被优先执行。 通过插件实现延迟队列 下载 https://www.rabbitmq.com/community-plugins.html 放在rabbitmq的plugins文件夹下 安装 xxx 为插件名（不包括后缀） rabbitmq-plugins enable xxx 安装成功后需要重新启动服务生效，在创建新的交换机时会提供一种新的类型 x-delayed-message package xyz.taoqz.bootlearn.config; import org.springframework.amqp.core.*; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import java.util.HashMap; import java.util.Map; /** * DelayQueueConfig * * @author taoqz * @date 2021/6/27 21:05 */ @Configuration public class DelayQueueConfig { private final String delay_exchange = \"delay.exchange\"; private final String delay_queue = \"delay.queue\"; private final String delay_routingKey = \"delay.routingKey\"; /** * 自定义交换机 * * @return */ @Bean(\"delayExchange\") public CustomExchange delayExchange() { final Map args = new HashMap<>(); // 添加标头,否则报错：Invalid argument, 'x-delayed-type' must be an existing exchange type args.put(\"x-delayed-type\", \"direct\"); return new CustomExchange(delay_exchange, \"x-delayed-message\", true, false, args); } @Bean(\"delayQueue\") public Queue delayQueue() { return new Queue(delay_queue); } @Bean public Binding delayBinding(@Qualifier(\"delayExchange\") CustomExchange delayExchange, @Qualifier(\"delayQueue\") Queue delayQueue) { return BindingBuilder.bind(delayQueue).to(delayExchange).with(delay_routingKey).noargs(); } } @Slf4j @RestController @RequestMapping(\"/rabbitmq\") public class RabbitMQController { private final String delay_exchange = \"delay.exchange\"; private final String delay_routingKey = \"delay.routingKey\"; @GetMapping(\"/sendDelayMsg/{msg}/{delayTime}\") public String sendDelayMsg(@PathVariable String msg, @PathVariable Integer delayTime) { rabbitTemplate.convertAndSend(delay_exchange, delay_routingKey, \"消息来自delayTime为\" + delayTime + \"的队列QA：\" + msg, message -> { message.getMessageProperties().setDelay(delayTime); return message; }); log.info(\"发送消息，delayTime:{}，当前时间：{}\", delayTime, new Date().toString()); return \"sendExpirationMsg OK!\"; } } package xyz.taoqz.bootlearn.rabbitmqlistener; import com.rabbitmq.client.Channel; import lombok.extern.slf4j.Slf4j; import org.springframework.amqp.core.Message; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.stereotype.Component; import java.util.Date; /** * DelayListener * * @author taoqz * @date 2021/6/27 21:16 */ @Slf4j @Component public class DelayListener { private final String delay_queue = \"delay.queue\"; @RabbitListener(queues = delay_queue) public void receiveDelay(Message message, Channel channel) { final String msg = new String(message.getBody()); log.info(\"当前时间：{}，接收到消息：{}\", new Date().toString(), msg); } } 效果 可以看到时效短的消息被先消费了，之前的延迟队列是通过消息再指定时间内没有被消费会被丢进死信队列的机制实现延迟队列，而是用插件方式，延时的操作直接由交换机控制，失效到了才会发送到消费者。 发布确认高级 这里的发布确认既有生产者发送至rabbitmq的发布确认，又包括了rabbitmq是否成功发送至消费者的发布确认。 相当于一套完整的消息发送确认。 配置文件 spring: rabbitmq: publisher-confirm-type: correlated @Configuration public class ConfirmConfig { // 交换机 private static final String exchange_name = \"confirm.exchange\"; // 队列 private static final String queue_name = \"confirm.queue\"; // 路由 private static final String routing_key_name = \"confirm.routing.key\"; @Bean(\"confirmExchange\") public DirectExchange confirmExchange(){ return new DirectExchange(exchange_name); } @Bean(\"confirmQueue\") public Queue confirmQueue(){ return QueueBuilder.durable(queue_name).build(); } @Bean public Binding binding(@Qualifier(\"confirmExchange\") DirectExchange confirmExchange, @Qualifier(\"confirmQueue\") Queue confirmQueue){ return BindingBuilder.bind(confirmQueue).to(confirmExchange).with(routing_key_name); } } @Slf4j @Component public class Consumer { public static final String queue_name = \"confirm.queue\"; @RabbitListener(queues = queue_name) public void receive(Message message, Channel channel) { log.info(\"接收到队列：\" + queue_name + \" 的消息：\" + new String(message.getBody())); // log.in(new String(message.getBody())); } } @Slf4j @Component public class MyConfirmCallback implements RabbitTemplate.ConfirmCallback { @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) { final String id = correlationData.getId(); if (ack) { log.info(\"交换机已接收到id为：\" + id + \"的消息\"); } else { log.info(\"交换机接收不到id为：\" + id + \"的消息，原因为：\" + cause); } } } @Slf4j @Component public class MyReturnCallback implements RabbitTemplate.ReturnsCallback { @Override public void returnedMessage(ReturnedMessage returned) { final String exchange = returned.getExchange(); final String message = new String(returned.getMessage().getBody()); final String routingKey = returned.getRoutingKey(); final int replyCode = returned.getReplyCode(); final String replyText = returned.getReplyText(); log.info(\"交换机：\" + exchange + \" routingkey：\" + routingKey + \" 消息：\" + message + \" replyCode：\" + replyCode + \" replyText： \" + replyText); } } @Slf4j @Component @Controller @RequestMapping(\"/publish\") public class Publisher { // 交换机 public static final String exchange_name = \"confirm.exchange\"; // 队列 public static final String queue_name = \"confirm.queue\"; // 路由 public static final String routing_key_name = \"confirm.routing.key\"; private final RabbitTemplate rabbitTemplate; private final MyConfirmCallback myConfirmCallback; private final MyReturnCallback returnCallback; public Publisher(RabbitTemplate rabbitTemplate, MyConfirmCallback myConfirmCallback, MyReturnCallback returnCallback) { this.rabbitTemplate = rabbitTemplate; this.myConfirmCallback = myConfirmCallback; this.returnCallback = returnCallback; } @PostConstruct public void init(){ // 注入消息发送至交换机的消息确认回调 rabbitTemplate.setConfirmCallback(myConfirmCallback); /** * true: 交换机无法将消息路由时，会将该消息返回给生产者 * false: 交换机无法将消息路由时，直接丢弃消息 */ rabbitTemplate.setMandatory(true); // 注入消息无法被消费时的回调（回退消息） rabbitTemplate.setReturnsCallback(returnCallback); } @ResponseBody @RequestMapping(\"/sendMsg/{msg}\") public String publish(@PathVariable String msg) { // 消息无法路由 final CorrelationData correlationData1 = new CorrelationData(\"1\"); rabbitTemplate.convertAndSend(exchange_name, routing_key_name+\"ss\", msg.getBytes(StandardCharsets.UTF_8), correlationData1); // 交换机不存在，接收不到消息 final CorrelationData correlationData2 = new CorrelationData(\"2\"); rabbitTemplate.convertAndSend(exchange_name + \"sdf\", routing_key_name, msg.getBytes(StandardCharsets.UTF_8), correlationData2); return \"OK!\"; } // 交换机：confirm.exchange routingkey：confirm.routing.keyss 消息：zzzzzzzzzzzzz replyCode：312 replyText： NO_ROUTE // 交换机已接收到id为：1的消息 // Shutdown Signal: channel error; protocol method: #method(reply-code=404, reply-text=NOT_FOUND - no exchange 'confirm.exchangesdf' in vhost '/', class-id=60, method-id=40) // 交换机接收不到id为：2的消息，原因为：channel error; protocol method: #method(reply-code=404, reply-text=NOT_FOUND - no exchange 'confirm.exchangesdf' in vhost '/', class-id=60, method-id=40) vue使用rabbitmq index.vue rabbitmq 11111 // rabbitmq-plugins enable rabbitmq_stomp // // import Stomp from 'stompjs' import z from './mqtt' import Stomp from 'stompjs' import {MQTT_SERVICE, MQTT_USERNAME, MQTT_PASSWORD, MQTT_host, MQTT_topic} from './mqtt' export default { data() { return { title: '测试信息', client: Stomp.client(MQTT_SERVICE), listData: [] } }, created() { this.onConnected()// stomp连接mq }, methods: { sssss() { // 订阅频道 // const topic = MQTT_topic; // const topic = '/queue/'; // const topic = '/quse/'; const topic = '/queue/helloworldQueue'; this.client.subscribe(topic, this.responseCallback, this.onFailed) }, async onConnected(frame) { await this.client.connect(MQTT_USERNAME, MQTT_PASSWORD, this.onConnected, this.onFailed, MQTT_host) // this.client.connect(MQTT_USERNAME,MQTT_PASSWORD, this.onConnected, this.onFailed)// 有的是不需要MQTT_host的，不需要的话，就不用传这个参数 } }, onFailed: function (frame) { console.log('MQ Failed:' + frame) }, responseCallback: function (frame) { // 接收消息处理 console.log('MQ msg=>', frame.body) this.title = frame.body; }, // connect() { // 初始化mqtt客户端，并连接mqtt服务 const headers = { login: MQTT_USERNAME, password: MQTT_PASSWORD } }, } mqtt.js export const MQTT_SERVICE = 'ws://127.0.0.1:15674/ws' // mqtt服务地址 export const MQTT_USERNAME = 'admin' // mqtt连接用户名 export const MQTT_PASSWORD = 'admin' // mqtt连接密码 export const MQTT_host = '/' // v-host export const MQTT_topic = '/exchange/js-top/helloworldQueue' // 订阅频道 // /helloworldQueue Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/微服务概述.html":{"url":"microservice/微服务概述.html","title":"微服务阶段","keywords":"","body":"SpringCloud微服务的特点优点缺点SpringCloud SpringCloud是微服务的一种实现,属于Spring旗下的项目之一,集成了很多其他优秀的流行框架,由于其团队的可靠性以及功能的强大,SpringCloud成为最火的微服务的实现方式 ​ 其中SpringCloud又有两种解决方案 ​ Spring Cloud Netflix ​ 该项目进入维护期,意味着没有新功能的添加,只进行bug修复 ​ Spring Cloud Alibaba: learning..... maven坐标 com.alibaba.cloud spring-cloud-alibaba-dependencies 2.1.0.RELEASE pom import 微服务的特点 单一职责：微服务中每一个服务都对应唯一的业务能力，做到单一职责 微：微服务的服务拆分粒度很小，例如一个用户管理就可以作为一个服务。每个服务虽小，但“五脏俱全”。 独立：自治是说服务间互相独立，互不干扰 团队独立：每个服务都是一个独立的开发团队，人数不能过多。 技术独立：因为是面向服务，提供Rest接口，使用什么技术没有别人干涉 前后端分离：采用前后端分离开发，提供统一Rest接口，后端不用再为PC、移动端开发不同接口 数据库分离：每个服务都使用自己的数据源 部署独立，服务间虽然有调用，但要做到服务重启不影响其它服务。有利于持续集成和持续交付。每个服务都是独立的组件，可复用，可替换，降低耦合，易维护 Docker部署服务 简单理解就是之前的每一个单体应用都可以称之为服务,举个例子,一个刚上线的小型商城项目,起初的用户量访问量很少,使用单体架构完全可以应对,但一旦用户访问量增多时,服务器或者数据库承受不了那么多的访问量,服务器会出现宕机,为了解决很多类似的情况,便有了微服务这种思想,将项目改造成微服务架构,针对访问量较多的模块可以让其占用更大的资源 优点 解决了复杂问题,可以将一个庞大的单体应用程序拆解成一套服务,虽然功能数量不变,但应用程序已经被拆解成可管理的块或者服务,每个服务提供相对单一的功能,更容易理解和维护 每个服务可以进行单独部署,并且可以根据该服务的所需来调配资源,避免资源浪费 缺点 使项目整体变得更加复杂,服务间的通信也变得麻烦 由于服务有自己独立的数据库,需要解决数据间的同步 测试会变得相对麻烦,一个服务或者模块会依赖其他的模块,进行相互调用,所以必须还要启动其依赖的服务 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/Zuul.html":{"url":"microservice/netflix/Zuul.html","title":"Zuul","keywords":"","body":"Zuul什么是Zuul为什么使用依赖注解配置路径映射规则过滤器fallback跨域其他Zuul 什么是Zuul Zuul是从设备和网站到Netflix流应用程序后端的所有请求的前门。作为边缘服务应用程序，Zuul旨在实现动态路由，监视，弹性和安全性。它还可以根据需要将请求路由到多个Amazon Auto Scaling组。 为什么使用 Netflix API流量的数量和多样性有时会导致生产问题迅速出现而没有警告。我们需要一个允许我们快速改变行为以对这些情况做出反应的系统。 Zuul使用了各种不同类型的过滤器，这使我们能够快速灵活地将功能应用于边缘服务。这些过滤器帮助我们执行以下功能： 身份验证和安全性-识别每个资源的身份验证要求，并拒绝不满足要求的请求。 见解和监控-在边缘跟踪有意义的数据和统计信息，以便为我们提供准确的生产视图。 动态路由-根据需要将请求动态路由到不同的后端群集。 压力测试-逐渐增加到群集的流量以评估性能。 减载-为每种类型的请求分配容量，并丢弃超出限制的请求。 静态响应处理-直接在边缘构建一些响应，而不是将其转发到内部集群 多区域弹性-在AWS区域之间路由请求，以多样化我们的ELB使用并将我们的优势拉近我们的成员 依赖 org.springframework.cloud spring-cloud-starter-netflix-zuul org.springframework.cloud spring-cloud-starter-netflix-eureka-client org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import 注解 @EnableZuulProxy // 开启网关功能 @EnableEurekaClient // 开启eureka客户端服务 配置 # 将服务注册到Eureka,并且拉取其他服务 eureka: client: registry-fetch-interval-seconds: 5 # 获取服务列表的周期：5s service-url: defaultZone: http://127.0.0.1:8083/eureka fetch-registry: true # 是否拉取其他服务,默认true instance: prefer-ip-address: true ip-address: 127.0.0.1 路径映射规则 指定url访问 zuul: routes: user-service: # 这里是路由id，随意写 path: /user-service/** # 这里是映射路径 # 访问/user-service/**时会请求 http://127.0.0.1:8081/ url: http://127.0.0.1:8081 # 映射路径对应的实际url地址 指定服务名 zuul: routes: user-service: # 这里是路由id，随意写 path: /user-service/** # 这里是映射路径 # 使用Eureka,通过服务名访问,并且会利用Ribbon的负载均衡 service-id: user-service # 指定服务名称 简化 路由名称往往和服务名一样,因此可以简化成如下,或者直接选择不配置,也会默认根据此规则发送请求,路由名称对应服务名称 zuul: prefix: /api # 添加路由前缀,可选项 routes: user-service: /user-service/** # 这里是映射路径 过滤器 ​ 和之前在servlet时学的过滤器功能类似 ​ 自定义过滤器 ​ 自定义需要实现类ZuulFilter,以下是其最重要的四个方法 public abstract class ZuulFilter implements IZuulFilter{ abstract public String filterType(); abstract public int filterOrder(); boolean shouldFilter();// 来自IZuulFilter Object run() throws ZuulException;// IZuulFilter } shouldFilter：返回一个Boolean`值，判断该过滤器是否需要执行。返回true执行，返回false不执行。 run：过滤器的具体业务逻辑。 filterType：返回字符串，代表过滤器的类型。包含以下4种： pre：请求在被路由之前执行 routing：在路由请求时调用 post：在routing和errror过滤器之后调用 error：处理请求时发生错误调用 filterOrder：通过返回的int值来定义过滤器的执行顺序，数字越小优先级越高。 示例 import com.netflix.zuul.ZuulFilter; import com.netflix.zuul.context.RequestContext; import com.netflix.zuul.exception.ZuulException; import io.jsonwebtoken.Claims; import org.springframework.stereotype.Component; import xyz.taoqz.utils.JWTUtil; import javax.servlet.http.HttpServletRequest; /** * 鉴权 */ @Component public class JWTFilter extends ZuulFilter { @Override public String filterType() { return \"pre\"; } @Override public int filterOrder() { return 1; } @Override public boolean shouldFilter() { return true; } /** * 编写权限过滤的核心业务： * 主要任务：确认是否放行 * 根据URL确认是否放行 * 有些URL是需要登陆之后才能访问，有些URL不需要登陆就可以访问 * 如果需要登陆的URL，需要获取token，并且解析token，成功了，就放行，不成功，就拦截 */ @Override public Object run() throws ZuulException { RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); if (request.getRequestURI().contains(\"/login\")){ return null; } String token = request.getHeader(\"authorization\"); // 结合了JWT使用 if (token != null && !(\"\").equals(token.trim())){ Claims example0102 = JWTUtil.parseToken(token.trim(), \"example0102\"); if (example0102 != null){ ctx.addZuulRequestHeader(\"authorization\",token); return null; } } // 拦截 ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); ctx.setResponseBody(\"{'msg':'校验失败'}\"); ctx.getResponse().setContentType(\"text/html;charset=utf-8\");// 不设置的话，中文乱码 return null; } } fallback 将所有服务收缩到到网关之后,需要通过网关请求服务,如果服务未启动或者请求超时,希望返回自定义信息而不是返回错误信息。 @Component public class ZuulFallback implements FallbackProvider { // 表明是为哪个微服务提供回退，*表示为所有微服务提供回退 @Override public String getRoute() { return \"*\"; } @Override public ClientHttpResponse fallbackResponse(String route, Throwable cause) { System.out.println(\"路由名称\"+route); System.out.println(cause.getMessage()); // 返回客户端的响应状态码 // return this.response(HttpStatus.OK); return this.response(HttpStatus.INTERNAL_SERVER_ERROR); } private ClientHttpResponse response(final HttpStatus status) { return new ClientHttpResponse() { @Override public HttpStatus getStatusCode() throws IOException { return status; } @Override public int getRawStatusCode() throws IOException { return status.value(); } @Override public String getStatusText() throws IOException { return status.getReasonPhrase(); } @Override public void close() { } @Override public InputStream getBody() throws IOException { RequestContext currentContext = RequestContext.getCurrentContext(); int status = currentContext.getResponse().getStatus(); System.out.println(status); // 响应给客户端的信息 String json = \"{\\\"msg\\\": \\\"服务不可用，请稍后再试。\\\"}\"; return new ByteArrayInputStream(json.getBytes()); } @Override public HttpHeaders getHeaders() { // headers设定 HttpHeaders headers = new HttpHeaders(); MediaType mt = new MediaType(\"application\", \"json\", Charset.forName(\"UTF-8\")); headers.setContentType(mt); return headers; } }; } } 跨域 import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.cors.CorsConfiguration; import org.springframework.web.cors.UrlBasedCorsConfigurationSource; import org.springframework.web.filter.CorsFilter; /** 全局跨域配置类 */ @Configuration public class GlobalCorsConfig { @Bean public CorsFilter corsFilter() { //1.添加CORS配置信息 CorsConfiguration config = new CorsConfiguration(); //放行哪些原始域 config.addAllowedOrigin(\"*\"); //是否发送Cookie信息 config.setAllowCredentials(true); //放行哪些原始域(请求方式) config.addAllowedMethod(\"OPTIONS\"); config.addAllowedMethod(\"HEAD\"); config.addAllowedMethod(\"GET\"); //get config.addAllowedMethod(\"PUT\"); //put config.addAllowedMethod(\"POST\"); //post config.addAllowedMethod(\"DELETE\"); //delete config.addAllowedMethod(\"PATCH\"); config.addAllowedHeader(\"*\"); //2.添加映射路径 UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource(); configSource.registerCorsConfiguration(\"/**\", config); //3.返回新的CorsFilter. return new CorsFilter(configSource); } } zuul: sensitive-headers: Access-Control-Allow-Origin ignored-headers: Access-Control-Allow-Origin,H-APP-Id,Token,APPToken 其他 ​ 解決请求头丢失问题,zuul会默认过滤掉部分请求头 zuul: sensitive-headers: Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/eureka/eureka.html":{"url":"microservice/netflix/eureka/eureka.html","title":"Eureka","keywords":"","body":"Eureka作用为什么要使用入门案例服务端客户端负载均衡高可用EurekaEureka ​ 服务发现与注册中心 ​ 由Netflix开发的服务发现框架,本身是一个基于REST的服务 ​ 主要组件 ​ Eureka Serve:服务端,提供服务注册和发现的功能 ​ Eureka Client:客户端,与Eureka Serve的交互,客户端启动后会自动注册到其启动时配置的地址的服务端 作用 ​ 对服务进行统一管理 ​ 提供服务的注册于发现,将所有的服务进行统一管理,不需要开发人员手动维护,需要使用哪个服务,直接在服务中心中获取就行 ​ 实现对服务的状态管理,如果某个服务下线,会有通知,专业术语是心跳,服务提供者会定期通过http方式向Eureka刷新自身的状态,同时Eureka会将所有提供者的地址发送给消费者,并定期更新 ​ 结合其他技术(Ribbon)实现了负载均衡 为什么要使用 ​ 微服务的基础还是服务,将一个单体应用拆分为微服务架构的应用,其中服务之间需要相互依赖,相互调用,但是服务被部署在不同的服务器上,如果服务一旦过多,那么会出现访问的地址(接口)难以进行管理,如果有集群,还需要自行实现负载均衡,并且其中有某个服务停掉也不会接到通知,为了解决等等之类的问题,便诞生了服务发现与注册中心 入门案例 ​ 分为服务端和客户端,引入Eureka都需要三步,添加依赖,配置,注解 ​ 首先都需要引入SpringCloud的依赖 ​ 该案例使用的SpringCloud的Hoxton.RELEASE版本,需要结合springboot2.2.x使用 org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import spring-milestones Spring Milestones https://repo.spring.io/milestone false 服务端 依赖 org.springframework.cloud spring-cloud-starter-netflix-eureka-server 配置 server: port: 8080 # 端口 spring: application: name: eureka-server # 应用名称，会在Eureka中显示 eureka: client: register-with-eureka: false # 是否注册自己的信息到EurekaServer，默认是true fetch-registry: false # 是否拉取其它服务的信息，默认是true service-url: # EurekaServer的地址，现在是自己的地址，如果是集群，需要加上其它Server的地址。 defaultZone: http://127.0.0.1:${server.port}/eureka 注解 ​ 在启动类上添加 @EnableEurekaServer // 声明这个应用是一个EurekaServer 访问:http://127.0.0.1:8080 客户端 依赖 org.springframework.cloud spring-cloud-starter-netflix-eureka-client 配置 server: port: 8081 spring: datasource: url: jdbc:mysql://localhost:3306/mydb?useUnicode=true&characterEncoding=utf8 username: root password: 123 driver-class-name: com.mysql.jdbc.Driver application: name: user-service # 应用名称 eureka: client: service-url: # EurekaServer地址 defaultZone: http://127.0.0.1:8080/eureka instance: prefer-ip-address: true # 当调用getHostname获取实例的hostname时，返回ip而不是host名称(也就是根据主机名还是ip访问) ip-address: 127.0.0.1 # 指定自己的ip信息，不指定的话会自己寻找 注解 // 两者选其一 @EnableEurekaClient // 开启EurekaClient功能 @EnableDiscoveryClient // 开启Eureka客户端 客户端又分为生产者和消费者,生产者也就是服务提供者,需要将自身注册到服务中心即可,而消费者需要到服务中心调用对应的服务,服务中心存储的相当于服务名称=多个服务地址的键值对 使用DiscoveryClient类 // 必须导入org.springframework.cloud.client.discovery.DiscoveryClient @Autowired private DiscoveryClient discoveryClient; public void demo(){ // 根据服务名称,拿到所有已注册的服务实例,因为可能会有集群所以是个集合 List instances = discoveryClient.getInstances(\"user-service\"); // 拿到其中一个实例 ServiceInstance serviceInstance = instances.get(0); // host主机地址 String host = serviceInstance.getHost(); // port端口号 int port = serviceInstance.getPort(); String url = \"http://\"+host+\":\"+port+\"controller地址及参数\"; } 结合RestTemplate使用 @Configuration public class RestTemplateConfig { @Bean public RestTemplate restTemplate(){ RestTemplate restTemplate = new RestTemplate(); // 解决乱码 restTemplate.getMessageConverters().add(1,new StringHttpMessageConverter(Charset.forName(\"UTF-8\"))); return restTemplate; } } #修改Eureka服务实例的显示 eureka: instance: instance-id: ${spring.application.name}:${server.port} 负载均衡 ​ 首先负载均衡是在集群的基础上,集群就是将相同功能的代码部署到不同的服务器,而负载均衡则是将流量平均分发到集群的不同服务上,同时做集群的目的也就是为了负载均衡,其中负载均衡又有很多策略,也就是实现负载均衡的算法 ​ Eureka中集成了Ribbon负载均衡器,使用时只需要简单的配置即可 ​ 使用 ​ 在RestTemplate的配置方法上添加@LoadBalanced注解： ​ 添加该注解后,才可以使用服务名的方式访问 @Configuration public class RestTemplateConfig { @Bean // 开启负载均衡 @LoadBalanced public RestTemplate restTemplate(){ RestTemplate restTemplate = new RestTemplate(); // 解决乱码 restTemplate.getMessageConverters().add(1,new StringHttpMessageConverter(Charset.forName(\"UTF-8\"))); return restTemplate; } } ​ 修改调用方式,从host+port改为直接调用服务名称的方式 http://服务名称/controller方法的映射地址 String url = \"http://user-service/user/\"+id; ​ 默认使用的是轮询机制,注入RibbonLoadBalanceClient类进行测试 @RunWith(SpringRunner.class) @SpringBootTest(classes = 启动类.class) public class LoadBalanceTest { @Autowired RibbonLoadBalancerClient client; @Test public void test(){ for (int i = 0; i ​ 修改负载均衡的策略 ​ 查看IRule接口的实现类,可以找到其他实现负载均衡的策略 ​ 在客户端添加配置 # 根 user-service: # 服务名 ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #策略 高可用Eureka 将多个EurekaServer相互注册为服务 启动多个EureakServe服务,进行相互注册,只需要将注册地址的ip互换 server: port: 10086 # 端口 spring: application: name: eureka-server # 应用名称，会在Eureka中显示 eureka: client: service-url: # 配置其他Eureka服务的地址，而不是自己，比如10087 defaultZone: http://127.0.0.1:10087/eureka 客户端注册到Eureka服务集群 eureka: client: service-url: # EurekaServer地址,多个地址以','隔开 EurekaServer集群只需注册在一台上即可共享 defaultZone: http://127.0.0.1:10086/eureka 服务续约 eureka: instance: lease-expiration-duration-in-seconds: 90 #超过90秒默认该服务宕机 lease-renewal-interval-in-seconds: 30 #每30秒发送一次心跳 当服务消费者启动时，会检测eureka.client.fetch-registry=true参数的值，如果为true，则会从Eureka Server服务的列表只读备份，然后缓存在本地。并且每隔30秒会重新获取并更新数据。我们可以通过下面的参数来修改： eureka: client: registry-fetch-interval-seconds: 5 失效剔除和自我保护 失效剔除 有些时候，我们的服务提供方并不一定会正常下线，可能因为内存溢出、网络故障等原因导致服务无法正常工作。Eureka Server需要将这样的服务剔除出服务列表。因此它会开启一个定时任务，每隔60秒对所有失效的服务（超过90秒未响应）进行剔除。 可以通过eureka.server.eviction-interval-timer-in-ms参数对其进行修改，单位是毫秒，生成环境不要修改。 这个会对我们开发带来极大的不便，你对服务重启，隔了60秒Eureka才反应过来。开发阶段可以适当调整，比如10S 自我保护 我们关停一个服务，就会在Eureka面板看到一条警告： 这是触发了Eureka的自我保护机制。当一个服务未按时进行心跳续约时，Eureka会统计最近15分钟心跳失败的服务实例的比例是否超过了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务剔除列表并不妥当，因为服务可能没有宕机。Eureka就会把当前实例的注册信息保护起来，不予剔除。生产环境下这很有效，保证了大多数服务依然可用。 但是这给我们的开发带来了麻烦， 因此开发阶段我们都会关闭自我保护模式： 在eureka的yml文件中配置 eureka: server: enable-self-preservation: false # 关闭自我保护模式（缺省为打开） eviction-interval-timer-in-ms: 1000 # 扫描失效服务的间隔时间（缺省为60*1000ms） 重试机制 ​ CAP原则：CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得 Eureka的服务治理强调了CAP原则中的AP，即可用性和可靠性。它与Zookeeper这一类强调CP（一致性，可靠性）的服务治理框架最大的区别在于：Eureka为了实现更高的服务可用性，牺牲了一定的一致性，极端情况下它宁愿接收故障实例也不愿丢掉健康实例，正如我们上面所说的自我保护机制。 但是，此时如果我们调用了这些不正常的服务，调用就会失败，从而导致其它服务不能正常工作！这显然不是我们愿意看到的。 spring: cloud: loadbalancer: retry: enabled: true # 开启Spring Cloud的重试功能 user-service: ribbon: ConnectTimeout: 250 # Ribbon的连接超时时间 ReadTimeout: 1000 # Ribbon的数据读取超时时间 OkToRetryOnAllOperations: true # 是否对所有操作都进行重试 MaxAutoRetriesNextServer: 1 # 切换实例的重试次数 MaxAutoRetries: 1 # 对当前实例的重试次数 根据如上配置，当访问到某个服务超时后，它会再次尝试访问下一个服务实例，如果不行就再换一个实例，如果不行，则返回失败。切换次数取决于MaxAutoRetriesNextServer参数的值 引入spring-retry依赖,测试 org.springframework.retry spring-retry Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/Hystrix.html":{"url":"microservice/netflix/Hystrix.html","title":"Hystrix","keywords":"","body":"Hystrix使用环境依赖注解逻辑代码配置Hystrix ​ 熔断,在分布式系统中,高并发的环境下服务可能因为压力会出现宕机,而其他的服务可能会依赖该服务,并且导致级联的失败,但是在生产环境中应该避免让用户看到错误的情况,所以可以使用熔断来解决相关问题 ​ 类似生活中的保险丝,当出现漏电等情况,会将保险丝熔断,避免出现火灾 使用 ​ 在服务调用方进行操作 环境 org.springframework.boot spring-boot-starter-parent 2.2.1.RELEASE org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import spring-milestones Spring Milestones https://repo.spring.io/milestone false 依赖 org.springframework.cloud spring-cloud-starter-netflix-hystrix 注解 // 在启动类上添加 @EnableHystrix // 开启Hystrix熔断 逻辑代码 // 在需要熔断的方法上添加注解并指定熔断的回调函数 @HystrixCommand(fallbackMethod = \"queryUserByIdFallback\") public User queryUserById(Long id) { String url = \"http://user-service-tao/user/\" + id; User user = restTemplate.getForObject(url, User.class); return user; } // 熔断后的回调函数 参数及返回值一致 public User queryUserByIdFallback(Long id){ User user = new User(); user.setId(id); user.setName(\"用户信息查询出现异常！\"); return user; } 配置 ​ 其默认的触发熔断的时间为1s,所以当结合重试使用时,需要将熔断的时间设置的比重试的时间长一些,不然重试就没有了其意义 hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 6000 # 设置hystrix的超时时间为6000ms Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/Feign.html":{"url":"microservice/netflix/Feign.html","title":"Feign","keywords":"","body":"Feign入门使用依赖注解配置逻辑代码请求压缩Feign ​ 简化远程调用代码,支持SpringMVC注解,不需要拼接url地址,同时整合了负载均衡Ribbon以及服务熔断(Hystrix) 入门使用 服务调用者 依赖 org.springframework.boot spring-boot-starter-parent 2.2.1.RELEASE org.springframework.cloud spring-cloud-starter-openfeign org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import spring-milestones Spring Milestones https://repo.spring.io/milestone false 注解 // 启动类上添加 @EnableFeignClients // 开启Feign功能 配置 ​ 虽然Feign集成了Hystrix熔断,但默认是关闭的 feign: hystrix: enabled: true # 开启Feign的熔断功能 ​ 同样支持对Ribbon负载均衡的配置 #服务名 如果不写服务名是全局配置 ribbon: ConnectTimeout: 250 # 连接超时时间(ms) ReadTimeout: 1000 # 通信超时时间(ms) OkToRetryOnAllOperations: true # 是否对所有操作重试 MaxAutoRetriesNextServer: 1 # 同一服务不同实例的重试次数 MaxAutoRetries: 1 # 同一实例的重试次数 逻辑代码 // 创建接口 添加访问的服务名称以及熔断时进行处理的class类(接口的实现类) @FeignClient(value = \"user-service-tao\",fallback = UserFeignClientFallback.class) public interface UserFeignClient { @GetMapping(\"/user/{id}\") User queryUserById(@PathVariable(\"id\") Long id); } // 实现Feign的接口,重写回调函数 @Component public class UserFeignClientFallback implements UserFeignClient{ @Override public User queryUserById(Long id) { User user = new User(); user.setId(id); user.setName(\"用户查询出现异常！\"); return user; } } // 注入后接口并调用 @Autowired private UserFeignClient userFeignClient; public User queryUserById(Long id){ User user = userFeignClient.queryUserById(id); return user; } 请求压缩 ​ Spring Cloud Feign 支持对请求和响应进行GZIP压缩，以减少通信过程中的性能损耗。通过下面的参数即可开启请求与响应的压缩功能： feign: compression: request: enabled: true # 开启请求压缩 response: enabled: true # 开启响应压缩 同时，我们也可以对请求的数据类型，以及触发压缩的大小下限进行设置： feign: compression: request: enabled: true # 开启请求压缩 mime-types: text/html,application/xml,application/json # 设置压缩的数据类型 min-request-size: 2048 # 设置触发压缩的大小下限 注：上面的数据类型、压缩大小下限均为默认值。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/SpringCloudConfig.html":{"url":"microservice/netflix/SpringCloudConfig.html","title":"Config","keywords":"","body":"SpringCloud Config介绍使用服务端依赖配置文件Git文件结构客户端依赖配置文件controller修改后的controllerSpringCloud Config 介绍 ​ 在分布式环境中,由于服务数量巨多,为了方便将配置文件统一管理,实时更新,所以需要分布式配置中心组件。在SpringCloud中提供了SpringCloud Config组件作为远程配置中心,该组件分为两个角色,一个是server端,一个是client端。Server端默认使用Git作为配置文件的存储中心,Client在启动时会请求Server获取配置文件中的内容。 使用 默认都使用springboot项目,springcloud本身也是基于springboot搭建的 服务端 依赖 org.springframework.boot spring-boot-starter-web org.springframework.cloud spring-cloud-config-server org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import 启动类上添加注解 @EnableConfigServer 配置文件 application.yml server: port: 8001 spring: application: name: spring-cloud-config-server # 使用指定的环境(根据配置文件的名称后缀指定) profiles: active: test cloud: config: server: git: uri: # 配置git仓库的地址 default-label: master #配置文件分支 search-paths: config # git仓库地址下的相对地址，可以配置多个，用,分割。 username: # git仓库的账号 password: # git仓库的密码 Git文件结构 项目(也就是配置文件中uri中的后缀)/文件夹(配置文件中search-paths的值)/配置文件 配置文件的命名规则: {application}-dev | test | prod.yml 客户端 依赖 org.springframework.boot spring-boot-starter-web org.springframework.cloud spring-cloud-starter-config org.springframework.boot spring-boot-starter-actuator org.springframework.cloud spring-cloud-dependencies Hoxton.RELEASE pom import 不需要在启动类上添加任何注解 配置文件 application.yml spring: application: name: config-demo-client # 开启或关闭接口优雅的关闭springboot项目 management: endpoint: shutdown: # 关闭 enabled: false # 包含所有端点(打开所有的监控点) endpoints: web: exposure: include: \"*\" 如果shutdown的配置为true,可以通过该接口(Post):http://localhost:8080/actuator/shutdown,关闭服务。关闭时得到的响应 { \"message\": \"Shutting down, bye...\" } bootstrap.properties # 配置文件的名称 spring.cloud.config.name=config-demo-client-test # 配置文件的环境（例如：dev,test,prod）也就是配置文件名称的后缀 spring.cloud.config.profile=test # spring cloud config的server端地址 spring.cloud.config.uri=http://localhost:8001/ # 配置文件所在git仓库中的分支 spring.cloud.config.label=master 关于spring-cloud的相关属性必须配置在bootstrap.properties中,config部分内容才能被正确加载。因为config的相关配置会先于application.properties,而bootstrap.properties的加载也是先于application.properties。 controller 创建一个controller用于读取配置文件中的属性进行测试 @RestController @RequestMapping(\"/user\") public class WebController { @Value(\"${data.user.username}\") private String username; @GetMapping public String fun(){ return username; } } 访问接口: http://localhost:8080/user 成功拿到了数据,但是更新git中配置文件内容后再次访问数据并未更新,此时便需要用到了监控包actuator 修改后的controller @RestController @RefreshScope // 使用该注解的类，会在接到SpringCloud配置中心配置刷新的时候，自动将新的配置更新到该类对应的字段中。 @RequestMapping(\"/user\") public class WebController { @Value(\"${data.user.username}\") private String username; @GetMapping public String getUsername(){ return username; } } 再次访问http://localhost:8080/user ,数据还是没有发生改变 需要使用post的方式访问该路径进行刷新:http://localhost:8080/actuator/refresh 访问配置文件中的所有内容(Get):http://localhost:8001/config-demo-client/dev 其中config-demo-client则是client端服务的名称,test则是对应的配置环境 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/netflix/POJO依赖.html":{"url":"microservice/netflix/POJO依赖.html","title":"POJO","keywords":"","body":"创建微服务pojo类所需依赖 UTF-8 UTF-8 1.8 1.3.2 2.0.2 1.1.9 5.1.32 1.2.3 0.7.0 2.9.6 org.projectlombok lombok org.mybatis.spring.boot mybatis-spring-boot-starter ${mybatis.starter.version} tk.mybatis mapper-spring-boot-starter ${mapper.starter.version} com.github.pagehelper pagehelper-spring-boot-starter ${pageHelper.starter.version} com.alibaba druid-spring-boot-starter ${druid.starter.version} mysql mysql-connector-java ${mysql.version} io.jsonwebtoken jjwt ${jjwt.version} joda-time joda-time ${joda-time.version} Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"microservice/alibaba/seata_nacos/seata结合nacos使用.html":{"url":"microservice/alibaba/seata_nacos/seata结合nacos使用.html","title":"seata","keywords":"","body":"Seata集合nacos使用新建config.txt文件service.vgroupMapping怎么写？开启seata配置新建ry-seata数据库新建nacos-config.sh文件修改seata/conf/registry.conf文件运行seata/bin下的seata-server.sh文件示例遇到的问题Type id handling not implemented for type java.lang.Object (by serializer of type...'xxx'.undo_log doesn't exits，某某数据库中undo_log表不存在Seata集合nacos使用 基于windows版,以下的配置中心在另外一个服务器，seata在windows服务器 下载：https://github.com/seata/seata/releases 配置： 新建config.txt文件 service.vgroupMapping.ruoyi-system-group=default service.vgroupMapping.ruoyi-order-group=default store.mode=db store.db.datasource=druid store.db.dbType=mysql store.db.driverClassName=com.mysql.jdbc.Driver store.db.url=jdbc:mysql://127.0.0.1:3306/ry-seata?useUnicode=true store.db.user=root store.db.password=123 store.db.minConn=5 store.db.maxConn=30 store.db.globalTable=global_table store.db.branchTable=branch_table store.db.queryLimit=100 store.db.lockTable=lock_table store.db.maxWait=5000 ​ 放在与seata的conf文件夹同一级目录 service.vgroupMapping怎么写？开启seata配置 ruoyi-system-group对应配置文件中的tx-service-group参数 # spring配置 spring: redis: host: 192.168.84.105 port: 6379 password: 123456 datasource: druid: stat-view-servlet: enabled: true loginUsername: admin loginPassword: 123456 dynamic: primary: master #设置默认的数据源或者数据源组,默认值即为master strict: false #严格匹配数据源,默认false. true未匹配到指定数据源时抛异常,false使用默认数据源 druid: initial-size: 5 min-idle: 5 maxActive: 20 maxWait: 60000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 1 FROM DUAL testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true maxPoolPreparedStatementPerConnectionSize: 20 filters: stat,slf4j connectionProperties: druid.stat.mergeSql\\=true;druid.stat.slowSqlMillis\\=5000 datasource: # 主库数据源 master: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://192.168.84.105:3306/ry-order?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8 username: root password: lastnine # 从库数据源 # slave: # username: # password: # url: # driver-class-name: seata: true # 开启seata代理，开启后默认每个数据源都代理，如果某个不需要代理可单独关闭 # seata配置 seata: # 默认关闭，如需启用spring.datasource.dynami.seata需要同时开启 enabled: true # Seata 应用编号，默认为 ${spring.application.name} application-id: ${spring.application.name} # Seata 事务组编号，用于 TC 集群名 tx-service-group: ${spring.application.name}-group # 关闭自动代理 enable-auto-data-source-proxy: false # 服务配置项 service: # 虚拟组和分组的映射 vgroup-mapping: ruoyi-system-group: default config: type: nacos nacos: serverAddr: 192.168.84.105:8848 group: SEATA_GROUP namespace: registry: type: nacos nacos: application: seata-server server-addr: 192.168.84.105:8848 namespace: # mybatis配置 mybatis: # 搜索指定包别名 typeAliasesPackage: com.ruoyi.order # 配置mapper的扫描，找到所有的mapper.xml映射文件 mapperLocations: classpath:mapper/**/*.xml 新建ry-seata数据库 DROP DATABASE IF EXISTS `ry-seata`; CREATE DATABASE `ry-seata` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; USE `ry-seata`; -- -------------------------------- The script used when storeMode is 'db' -------------------------------- -- the table to store GlobalSession data CREATE TABLE IF NOT EXISTS `global_table` ( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(32), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_transaction_id` (`transaction_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- the table to store BranchSession data CREATE TABLE IF NOT EXISTS `branch_table` ( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME(6), `gmt_modified` DATETIME(6), PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- the table to store lock data CREATE TABLE IF NOT EXISTS `lock_table` ( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(96), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_branch_id` (`branch_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- for AT mode you must to init this sql for you business database. the seata server not need it. CREATE TABLE IF NOT EXISTS `undo_log` ( `branch_id` BIGINT(20) NOT NULL COMMENT 'branch transaction id', `xid` VARCHAR(100) NOT NULL COMMENT 'global transaction id', `context` VARCHAR(128) NOT NULL COMMENT 'undo_log context,such as serialization', `rollback_info` LONGBLOB NOT NULL COMMENT 'rollback info', `log_status` INT(11) NOT NULL COMMENT '0:normal status,1:defense status', `log_created` DATETIME(6) NOT NULL COMMENT 'create datetime', `log_modified` DATETIME(6) NOT NULL COMMENT 'modify datetime', UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`) ) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8mb4 COMMENT ='AT transaction mode undo table'; 新建nacos-config.sh文件 放在seata的conf文件夹中 内容如下,需配置nacos地址 #!/usr/bin/env bash # Copyright 1999-2019 Seata.io Group. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at、 # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. while getopts \":h:p:g:t:u:w:\" opt do case $opt in h) host=$OPTARG ;; p) port=$OPTARG ;; g) group=$OPTARG ;; t) tenant=$OPTARG ;; u) username=$OPTARG ;; w) password=$OPTARG ;; ?) echo \" USAGE OPTION: $0 [-h host] [-p port] [-g group] [-t tenant] [-u username] [-w password] \" exit 1 ;; esac done urlencode() { for ((i=0; i \"${tempLog}\" 2>/dev/null if [[ -z $(cat \"${tempLog}\") ]]; then echo \" Please check the cluster status. \" exit 1 fi if [[ $(cat \"${tempLog}\") =~ \"true\" ]]; then echo \"Set $1=$2 successfully \" else echo \"Set $1=$2 failure \" (( failCount++ )) fi } count=0 for line in $(cat $(dirname \"$PWD\")/config.txt | sed s/[[:space:]]//g); do (( count++ )) key=${line%%=*} value=${line#*=} addConfig \"${key}\" \"${value}\" done echo \"=========================================================================\" echo \" Complete initialization parameters, total-count:$count , failure-count:$failCount \" echo \"=========================================================================\" if [[ ${failCount} -eq 0 ]]; then echo \" Init nacos config finished, please start seata-server. \" else echo \" init nacos config fail. \" fi 运行 nacos-config.sh 192.168.84.105,运行完后会将config.txt中的配置信息发送到nacos配置中心里 修改seata/conf/registry.conf文件 将registry模块和config模块中的配置改为nacos registry { # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = \"nacos\" nacos { application = \"seata-server\" serverAddr = \"192.168.84.105:8848\" group = \"SEATA_GROUP\" namespace = \"\" cluster = \"default\" username = \"nacos\" password = \"nacos\" } eureka { serviceUrl = \"http://localhost:8761/eureka\" application = \"default\" weight = \"1\" } redis { serverAddr = \"localhost:6379\" db = 0 password = \"\" cluster = \"default\" timeout = 0 } zk { cluster = \"default\" serverAddr = \"127.0.0.1:2181\" sessionTimeout = 6000 connectTimeout = 2000 username = \"\" password = \"\" } consul { cluster = \"default\" serverAddr = \"127.0.0.1:8500\" aclToken = \"\" } etcd3 { cluster = \"default\" serverAddr = \"http://localhost:2379\" } sofa { serverAddr = \"127.0.0.1:9603\" application = \"default\" region = \"DEFAULT_ZONE\" datacenter = \"DefaultDataCenter\" cluster = \"default\" group = \"SEATA_GROUP\" addressWaitTime = \"3000\" } file { name = \"file.conf\" } } config { # file、nacos 、apollo、zk、consul、etcd3 type = \"nacos\" nacos { serverAddr = \"192.168.84.105:8848\" namespace = \"\" group = \"SEATA_GROUP\" username = \"nacos\" password = \"nacos\" } consul { serverAddr = \"127.0.0.1:8500\" aclToken = \"\" } apollo { appId = \"seata-server\" ## apolloConfigService will cover apolloMeta apolloMeta = \"http://192.168.1.204:8801\" apolloConfigService = \"http://192.168.1.204:8080\" namespace = \"application\" apolloAccesskeySecret = \"\" cluster = \"seata\" } zk { serverAddr = \"127.0.0.1:2181\" sessionTimeout = 6000 connectTimeout = 2000 username = \"\" password = \"\" nodePath = \"/seata/seata.properties\" } etcd3 { serverAddr = \"http://localhost:2379\" } file { name = \"file.conf\" } } 运行seata/bin下的seata-server.sh文件 示例 运行两个服务user和order，两个服务使用的不同的数据库，相关的依赖待续。。。。 RemoteSysUserService为在order服务中使用feign远程调用user服务 @GlobalTransactional第一个开启事务的方法必须添加seata的全局事务注解 @Service public class MyService { @Autowired private ITOrderService itOrderService; @Autowired private RemoteSysUserService remoteSysUserService; @Transactional @GlobalTransactional public void add(){ final TOrder tOrder = new TOrder(); tOrder.setName(\"order\"); tOrder.setUid(123L); itOrderService.insertTOrder(tOrder); final SysUser sysUser = new SysUser(); sysUser.setUserName(\"taoqqqq\"); sysUser.setNickName(\"taoqqqqnn\"); remoteSysUserService.addUser(sysUser); int i = 1 / 0 ; } } 遇到的问题 Type id handling not implemented for type java.lang.Object (by serializer of type... 解决方案：将数据库字段为datatime类型的改为timestamp 'xxx'.undo_log doesn't exits，某某数据库中undo_log表不存在 解决方案：需要在设计分布式事务的每个数据库中添加一张undo_log表 CREATE TABLE `ry-order`.undo_log ( id BIGINT(20) NOT NULL AUTO_INCREMENT, branch_id BIGINT(20) NOT NULL, xid VARCHAR(100) NOT NULL, context VARCHAR(128) NOT NULL, rollback_info LONGBLOB NOT NULL, log_status INT(11) NOT NULL, log_created DATETIME NOT NULL, log_modified DATETIME NOT NULL, PRIMARY KEY (id), UNIQUE KEY ux_undo_log (xid, branch_id) ) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8mb4; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"httptool/HttpClient.html":{"url":"httptool/HttpClient.html","title":"HttpClient","keywords":"","body":"HttpClient什么是HttpClient入门案例发起Get请求带参数的Get请求发起POST请求带参数POST请求SpringBoot整合HttpClientHttpClient 什么是HttpClient HttpClient 是Apache Jakarta Common 下的子项目，可以用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。 特点： HttpClient别名：HttpComponents HttpClient可以发送get、post、put、delete、...等请求 入门案例 导入maven坐标 org.apache.httpcomponents httpclient 4.4 发起Get请求 1创建一个客户端 CloseableHttpClient 2创建一个get方法请求实例 HttpGet 3发送请求 execute 4获取响应的头信息 5获取响应的主题内容 6关闭响应对象 使用HttpClient发起Get请求的案例代码： public class DoGET { public static void main(String[] args) throws Exception { // 创建Httpclient对象,相当于打开了浏览器 CloseableHttpClient httpclient = HttpClients.createDefault(); // 创建HttpGet请求，相当于在浏览器输入地址 HttpGet httpGet = new HttpGet(\"http://www.baidu.com/\"); CloseableHttpResponse response = null; try { // 执行请求，相当于敲完地址后按下回车。获取响应 response = httpclient.execute(httpGet); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) { // 解析响应，获取数据 String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content); } } finally { if (response != null) { // 关闭资源 response.close(); } // 关闭浏览器 httpclient.close(); } } } 带参数的Get请求 1创建一个客户端 CloseableHttpClient 2 通过URIBuilder传递参数 3创建一个get方法请求实例 HttpGet 4发送请求 execute 5获取响应的头信息 6获取响应的主题内容 7关闭响应对象 访问网站的爬虫协议： public class DoGETParam { public static void main(String[] args) throws Exception { // 创建Httpclient对象 CloseableHttpClient httpclient = HttpClients.createDefault(); // 创建URI对象，并且设置请求参数 URI uri = new URIBuilder(\"http://www.baidu.com/s\").setParameter(\"wd\", \"java\").build(); // 创建http GET请求 HttpGet httpGet = new HttpGet(uri); // HttpGet get = new HttpGet(\"http://www.baidu.com/s?wd=java\"); CloseableHttpResponse response = null; try { // 执行请求 response = httpclient.execute(httpGet); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) { // 解析响应数据 String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content); } } finally { if (response != null) { response.close(); } httpclient.close(); } } } 发起POST请求 /* * 演示：使用HttpClient发起POST请求 */ public class DoPOST { public static void main(String[] args) throws Exception { // 创建Httpclient对象 CloseableHttpClient httpclient = HttpClients.createDefault(); // 创建http POST请求 HttpPost httpPost = new HttpPost(\"http://www.oschina.net/\"); // 把自己伪装成浏览器。否则开源中国会拦截访问 httpPost.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"); CloseableHttpResponse response = null; try { // 执行请求 response = httpclient.execute(httpPost); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) { // 解析响应数据 String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content); } } finally { if (response != null) { response.close(); } // 关闭浏览器 httpclient.close(); } } } 带参数POST请求 /* * 演示：使用HttpClient发起带有参数的POST请求 */ public class DoPOSTParam { public static void main(String[] args) throws Exception { // 创建Httpclient对象 CloseableHttpClient httpclient = HttpClients.createDefault(); // 创建http POST请求，访问开源中国 HttpPost httpPost = new HttpPost(\"http://www.oschina.net/search\"); // 根据开源中国的请求需要，设置post请求参数 List parameters = new ArrayList(0); parameters.add(new BasicNameValuePair(\"scope\", \"project\")); parameters.add(new BasicNameValuePair(\"q\", \"java\")); parameters.add(new BasicNameValuePair(\"fromerr\", \"8bDnUWwC\")); // 构造一个form表单式的实体 UrlEncodedFormEntity formEntity = new UrlEncodedFormEntity(parameters); // 将请求实体设置到httpPost对象中 httpPost.setEntity(formEntity); CloseableHttpResponse response = null; try { // 执行请求 response = httpclient.execute(httpPost); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) { // 解析响应体 String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content); } } finally { if (response != null) { response.close(); } // 关闭浏览器 httpclient.close(); } } } SpringBoot整合HttpClient SpringBoot官方并没有对HttpClient的启动器。所以我们需要自己完成配置，还好，我们刚才在测试案例中已经写过了。 不过，SpringBoot虽然没有提供启动器，但是却提供了一个统一的对Restful服务进行调用的模板类：RestTemplate，底层可以使用HttpClient来实现。有了这个我们就无需自己定义APIService了。 1、导入maven坐标 org.apache.httpcomponents httpclient 2、在application.properties添加如下配置: #The config for HttpClient http.maxTotal=300 http.defaultMaxPerRoute=50 http.connectTimeout=1000 http.connectionRequestTimeout=500 http.socketTimeout=5000 http.staleConnectionCheckEnabled=true 3、 创建HttpClientConfig类--类似util 在类中编写代码 /** * HttpClient的配置类 * */ @Configuration @ConfigurationProperties(prefix = \"http\", ignoreUnknownFields = true) public class HttpClientConfig { private Integer maxTotal;// 最大连接 private Integer defaultMaxPerRoute;// 每个host的最大连接 private Integer connectTimeout;// 连接超时时间 private Integer connectionRequestTimeout;// 请求超时时间 private Integer socketTimeout;// 响应超时时间 /** * HttpClient连接池 * @return */ @Bean public HttpClientConnectionManager httpClientConnectionManager() { PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager(); connectionManager.setMaxTotal(maxTotal); connectionManager.setDefaultMaxPerRoute(defaultMaxPerRoute); return connectionManager; } /** * 注册RequestConfig * @return */ @Bean public RequestConfig requestConfig() { return RequestConfig.custom().setConnectTimeout(connectTimeout) .setConnectionRequestTimeout(connectionRequestTimeout).setSocketTimeout(socketTimeout) .build(); } /** * 注册HttpClient * @param manager * @param config * @return */ @Bean public HttpClient httpClient(HttpClientConnectionManager manager, RequestConfig config) { return HttpClientBuilder.create().setConnectionManager(manager).setDefaultRequestConfig(config) .build(); } /** * 使用连接池管理连接 * @param httpClient * @return */ @Bean public ClientHttpRequestFactory requestFactory(HttpClient httpClient) { return new HttpComponentsClientHttpRequestFactory(httpClient); } /** * 使用HttpClient来初始化一个RestTemplate * @param requestFactory * @return */ @Bean public RestTemplate restTemplate(ClientHttpRequestFactory requestFactory) { RestTemplate template = new RestTemplate(requestFactory); List> list = template.getMessageConverters(); for (HttpMessageConverter mc : list) { if (mc instanceof StringHttpMessageConverter) { ((StringHttpMessageConverter) mc).setDefaultCharset(Charset.forName(\"UTF-8\")); } } return template; } public Integer getMaxTotal() { return maxTotal; } public void setMaxTotal(Integer maxTotal) { this.maxTotal = maxTotal; } public Integer getDefaultMaxPerRoute() { return defaultMaxPerRoute; } public void setDefaultMaxPerRoute(Integer defaultMaxPerRoute) { this.defaultMaxPerRoute = defaultMaxPerRoute; } public Integer getConnectTimeout() { return connectTimeout; } public void setConnectTimeout(Integer connectTimeout) { this.connectTimeout = connectTimeout; } public Integer getConnectionRequestTimeout() { return connectionRequestTimeout; } public void setConnectionRequestTimeout(Integer connectionRequestTimeout) { this.connectionRequestTimeout = connectionRequestTimeout; } public Integer getSocketTimeout() { return socketTimeout; } public void setSocketTimeout(Integer socketTimeout) { this.socketTimeout = socketTimeout; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"httptool/RestTemplate.html":{"url":"httptool/RestTemplate.html","title":"RestTemplate","keywords":"","body":"RestTemplateRestTemplate是什么?使用所需依赖解决中文乱码问题访问中国天气出现乱码简单使用RestTemplate RestTemplate是什么? ​ 是一个可以在服务器之间相互调用,发送http请求的工具包,封装了其他有相同功能的工具包,使用更加简洁方便 使用 所需依赖 org.springframework.boot spring-boot-starter-web 解决中文乱码问题 @Configuration public class RestTemplateConfig { //第二种 不用配置类,可以直接在SpringBoot 启动类注册 @Bean public RestTemplate restTemplate() { // 默认的RestTemplate，底层是走JDK的URLConnection方式。 // 设置中文乱码问题方式一 RestTemplate restTemplate = new RestTemplate(); restTemplate.getMessageConverters().add(1,new StringHttpMessageConverter(Charset.forName(\"UTF-8\"))); // 设置中文乱码问题方式二 // restTemplate.getMessageConverters().set(1, // new StringHttpMessageConverter(StandardCharsets.UTF_8)); // 支持中文编码 return restTemplate; } } 访问中国天气出现乱码 请求接口: http://wthrcdn.etouch.cn/weather_mini?city=北京 增加依赖 org.apache.httpcomponents httpclient 4.4 修改配置类 //获得的返回数据是经过 GZIP 压缩过的, 而默认的URLConnection无法支持所以考虑创建使用httpClient的RestTemplate RestTemplate restTemplate = new RestTemplate( new HttpComponentsClientHttpRequestFactory()); // 使用HttpClient，支持GZIP restTemplate.getMessageConverters().set(1, new StringHttpMessageConverter(StandardCharsets.UTF_8)); // 支持中文编码 简单使用 @RestController @RequestMapping(\"/rest\") public class RestTemplateController { @Autowired private RestTemplate restTemplate; private String urlPrefix = \"http://localhost:8080/\"; @GetMapping public ResponseEntity findAll(){ ResponseEntity forEntity = restTemplate.getForEntity(urlPrefix + \"brand\", BaseResult.class); return forEntity; } @GetMapping(\"/product\") public ResponseEntity findAllProduct(PageRequest pageRequest){ // 也可以直接将参数拼接在地址栏上 注意传递参数是 参数为null 会被拼接为字符串的问题 HashMap hashMap = new HashMap<>(); hashMap.put(\"pageNum\",pageRequest.getPageNum()); hashMap.put(\"pageSize\",pageRequest.getPageSize()); ResponseEntity forEntity = restTemplate.getForEntity(urlPrefix + \"product?pageNum={pageNum}&pageSize={pageSize}\",BaseResult.class,hashMap); return forEntity; } @PostMapping public ResponseEntity add(@RequestBody TbProduct tbProduct){ ResponseEntity forEntity = restTemplate.postForEntity(urlPrefix + \"product\",tbProduct, String.class); return ResponseEntity.ok(forEntity.getBody()); } @DeleteMapping(\"/{id}\") public ResponseEntity deleteById(@PathVariable Integer id){ restTemplate.delete(urlPrefix + \"product/\"+id); return ResponseEntity.ok(\"删除成功\"); } @PutMapping public ResponseEntity edit(@RequestBody TbProduct tbProduct){ restTemplate.put(urlPrefix + \"product\",tbProduct); return ResponseEntity.ok(\"修改成功\"); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"jdbc/jdbc.html":{"url":"jdbc/jdbc.html","title":"JDBC","keywords":"","body":"JDBC创建连接ConnectionStatement和PreparedStatementResultSet存储过程事务保存点使用元数据改造结果集JDBC 全名为Java DataBase Connectivity,由于市面上的数据库种类很多,根据每种数据库需要学习不同的API,sun公司为了方便统一定义了java连接数据库的接口,从此数据库厂商只需要实现对应的接口提供对应的驱动包即可 创建连接 添加依赖 mysql mysql-connector-java 8.0.21--> 5.1.47 注册驱动 获取连接 // 这是java1.6之前注册驱动的方式 // 加载驱动 // 方式一,会加载两次 // DriverManager.registerDriver(new Driver()); // 方式二 // Class.forName(\"com.mysql.jdbc.Driver\"); 在java1.6之后,jdbc4无需手动注册驱动,程序会根据驱动包内的文件自动注册 jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://127.0.0.1/jdbc_stu?useSSL=false jdbc.username=root jdbc.password=123 package xyz.taoqz.util; import java.io.IOException; import java.io.InputStream; import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; import java.util.Properties; /** * @author :almostTao * @date :Created in 2020/8/16 14:12 */ public class JDBCUtil { private static String driver = null; private static String url = null; private static String username = null; private static String password = null; static { try { InputStream resourceAsStream = JDBCUtil.class.getClassLoader().getResourceAsStream(\"db.properties\"); Properties properties = new Properties(); properties.load(resourceAsStream); driver = properties.getProperty(\"jdbc.driver\"); url = properties.getProperty(\"jdbc.url\"); username = properties.getProperty(\"jdbc.username\"); password = properties.getProperty(\"jdbc.password\"); } } public static Connection getConnection() throws SQLException { return DriverManager.getConnection(url,username,password); } // 获取连接 Connection connection = JDBCUtil.getConnection(); // 创建statement对象 Statement statement = connection.createStatement(); // 通过statement执行查询语句 ResultSet resultSet = statement.executeQuery(\"select * from jdbc_stu.school\"); while (resultSet.next()){ System.out.println(resultSet.getString(\"name\")); } Connection @Test public void insert() throws SQLException { Connection connection = JDBCUtil.getConnection(); // 设置自动提交为false connection.setAutoCommit(false); try { String sql = \"insert into jdbc_stu.school values (null,?)\"; // 获取由数据库自增的id PreparedStatement preparedStatement = connection.prepareStatement(sql, com.mysql.jdbc.PreparedStatement.RETURN_GENERATED_KEYS); preparedStatement.setString(1, \"北大\"); int i = preparedStatement.executeUpdate(); System.out.println(i); ResultSet resultSet = preparedStatement.getGeneratedKeys(); resultSet.next(); System.out.println(resultSet.getInt(1)); // System.out.println(2 / 0); // 手动提交 connection.commit(); } catch (SQLException e) { // 当我们尝试回滚时,报错 // com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Can't call rollback when autocommit=true connection.rollback(); e.printStackTrace(); } } Statement和PreparedStatement PreparedStatement相较于Statement多了占位符,可以有效的避免sql注入问题 @Test public void demoZhuRu() throws SQLException { Connection connection = JDBCUtil.getConnection(); Statement statement = connection.createStatement(); String param = \"''or 1=1\"; String sql = \"select * from jdbc_stu.school where name = \" + param; // 相当于执行 select * from jdbc_stu.school where name = ''or 1=1 // 直接将其拼接到了sql后 ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { System.out.println(resultSet.getString(\"name\")); } System.out.println(\"====================================================\"); sql = \"select * from jdbc_stu.school where name = ? \"; PreparedStatement preparedStatement = connection.prepareStatement(sql); // 在底层会使用 instanceof 关键字比较类型,从而调用对应的可以指定类型的方法 // preparedStatement.setObject(1,param); preparedStatement.setString(1,param); // 相当于执行 select * from jdbc_stu.school where name = '''or 1=1'; // 不会把参数中的 和sql语句语法有关的当成sql的一部分执行,有效防止了sql注入 resultSet = preparedStatement.executeQuery(); while (resultSet.next()) { System.out.println(resultSet.getString(\"name\")); } } 其他常用方法 public static void main(String[] args) throws Exception { Connection connection = JDBCUtil.getConnection(); Statement statement = connection.createStatement(); String sql = \"select * from jdbc_stu.school\"; ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { // 获取指定名称列的结果 System.out.println(resultSet.getString(\"name\")); } // 结果不明确,不推荐使用 // 查询使用executeQuery boolean execute = statement.execute(sql); System.out.println(execute); // 增删改使用 String updateSql = \"insert into school values (null,'哈工大')\"; statement.executeUpdate(updateSql); // 批处理 String batchSql = \"delete from school where id = 1;\"; String batchSql2 = \"delete from school where id = 2;\"; statement.addBatch(batchSql); statement.addBatch(batchSql2); // 需要注意的是,不能执行select语句 // Can not issue SELECT via executeUpdate() or executeLargeUpdate() int[] ints = statement.executeBatch(); // 返回执行结果 0 失败或为修改 1 成功 System.out.println(Arrays.toString(ints)); } ResultSet ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { // 获取指定名称列的结果 System.out.println(resultSet.getInt(\"id\")); System.out.println(resultSet.getString(\"name\")); } 存储过程 -- 创建存储过程 create procedure name_like(in nameparam varchar(10),out con int) begin set con = 0; select count(*) into con from school where name like nameparam; end; -- 调用 call name_like('%大%',@con); select @con; @Test public void callProcedure() throws SQLException { Connection connection = JDBCUtil.getConnection(); String callSql = \"{call name_like(?,?)}\"; CallableStatement callableStatement = connection.prepareCall(callSql); // 设置参数 callableStatement.setString(1,\"%清%\"); callableStatement.registerOutParameter(2, Types.INTEGER); callableStatement.execute(); // 获取返回值 String result = callableStatement.getString(2); System.out.println(result); } 事务保存点 @Test public void savepointDemo() throws Exception { Connection connection = JDBCUtil.getConnection(); // 开启事务 connection.setAutoCommit(false); String sql = \"insert into school values(null,?)\"; PreparedStatement preparedStatement = connection.prepareStatement(sql); preparedStatement.setString(1,\"天津大学\"); // 提交后,插入数据库 preparedStatement.execute(); // 设置保存点 Savepoint savepoint = connection.setSavepoint(); // 不会插入 preparedStatement.setString(1,\"清华大学\"); preparedStatement.execute(); // 回滚至指定的保存点 connection.rollback(savepoint); // 关闭事务,并提交 connection.setAutoCommit(true); } 事务的隔离级别 Connection.TRANSACTION_NONE Connection.TRANSACTION_READ_COMMITTED Connection.TRANSACTION_READ_UNCOMMITTED Connection.TRANSACTION_REPEATABLE_READ Connection.TRANSACTION_SERIALIZABLE connection.setTransactionIsolation(); 获取当前连接数据库的配置 queryBuf SELECT @@session.auto_increment_increment AS auto_increment_increment, @@character_set_client AS character_set_client, @@character_set_connection AS character_set_connection, @@character_set_results AS character_set_results, @@character_set_server AS character_set_server, @@collation_server AS collation_server, @@collation_connection AS collation_connection, @@init_connect AS init_connect, @@interactive_timeout AS interactive_timeout, @@license AS license, @@lower_case_table_names AS lower_case_table_names, @@max_allowed_packet AS max_allowed_packet, @@net_buffer_length AS net_buffer_length, @@net_write_timeout AS net_write_timeout, @@query_cache_size AS query_cache_size, @@query_cache_type AS query_cache_type, @@sql_mode AS sql_mode, @@system_time_zone AS system_time_zone, @@time_zone AS time_zone, @@transaction_isolation AS transaction_isolation, @@wait_timeout AS wait_timeout 使用元数据改造结果集 ResultSet返回的往往是列的值,而我们大多时候想直接拿到Bean对象,此时可以使用数据库的原数据改造ResultSet ParameterMetaData --参数的元数据 ResultSetMetaData --结果集的元数据 DataBaseMetaData --数据库的元数据 也可以使用DBUtils public interface ResultSetHandler { List handler(ResultSet resultSet) throws Exception; } package xyz.taoqz.handler; import java.lang.reflect.Field; import java.sql.ResultSet; import java.sql.ResultSetMetaData; import java.util.ArrayList; import java.util.List; /** * @author :almostTao * @date :Created in 2020/8/16 17:54 */ public class BeanHandler implements ResultSetHandler { private Class clazz; private List list; public BeanHandler(Class clazz) { this.clazz = clazz; list = new ArrayList<>(); } @Override public List handler(ResultSet resultSet) throws Exception { Object bean; while (resultSet.next()){ bean = clazz.newInstance(); //拿到结果集元数据 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); for (int i = 0; i Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"jike100/jike100.html":{"url":"jike100/jike100.html","title":"jike100","keywords":"","body":"ThreadLocalHashTableCopyOnWriteArrayListlist oom问题hashmap线程安全问题volatile线程状态ThreadLocal 在一个线程中保存一份变量,使用场景:同一线程但不能传参时可以使用,使用后记得调用remove(),否则会出现数据错乱问题,比如运行在Tomcat中,Tomcat使用了线程池,而线程池会复用,这就导致数据可能会串 HashTable 是线程安全的(key和value不允许存null),size()方法也是,而ConcurrentHashMap虽然保证了在同一线程读写的原子性,但是size()没有加锁,这就导致复合操作时会出现线程安全问题(ConcurrentHashMap容器本身保证了装数据时的安全性,但没有保证获取数量时的安全性(可见性,一个线程在操作时,其他线程可以获得size)) CopyOnWriteArrayList 适合读多写少的环境下,比如黑白名单,写(add)时会都用Arrays.copyOf创建新数组，频繁add时内存申请释放性能消耗大,高并发读的时候CopyOnWriteArray无锁，Collections.synchronizedList(包括get方法)有锁所以读的效率比较低下。 list oom问题 Arrays.asList()将数组转换为list,转换后的list是Arrays的内部类ArrayList,没有增加和删除的方法,并且如果该数组是基本数据类型,转换后的集合中只有一个元素,那就是这个数组,并且修改数组中的元素会影响转换后的集合,因为他本质还是数组用的还是同一个地址,达不到预期效果 解决数组为基本类型时的问题 // 使用java8进行装箱 int[] a = {1,2,3}; List collect = Arrays.stream(a).boxed().collect(Collectors.toList()); // 或者直接改为Integer[],再或者Arrays.asList后在使用new ArrayList()构造查u你创建对象 hashmap线程安全问题 1.7 数据重复:如果两条线程同时put一个key相同的元素时都判断map中该key不存在,一条线程将元素添加进map后,另一条线程获取最后一个节点的位置时可能就是线程1的key的位置 死循环 1.8 数据丢失 在多线程情况下put时计算出的插入的数组下标可能是相同的，这时可能出现值的覆盖从而导致size也是不准确的 一条线程与另一条线程的key的hash值相同,但不相等,也就是hash碰撞,此时put时在计算要添加的位置时由于都还没有添加进去,计算的位置相同,会出现覆盖的情况 volatile 当对非volatile变量读写时,每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU,每个线程可能在不同的CPU上处理,这意味着每个线程可以拷贝到不同的CPU缓存中。 而声明变量为volatile,则会跳过缓存这一步骤,JVM保证每次都会从内存中读变量 线程状态 1.新建状态,新建一个Thread对象但是还未调用start()方法 2.就绪或运行中,调用start()方法,如果获得cpu资源就会执行 3.阻塞,如果有同步锁的操作会进入阻塞状态 4.等待,调用wait()方法,会主动释放锁 5.超时等待,调用sleep()方法,不会释放锁 6.终止状态 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"mybatis/mybatis.html":{"url":"mybatis/mybatis.html","title":"Mybatis","keywords":"","body":"什么是Mybatis什么是ORM案例什么是Mybatis Mybatis是一个持久层ORM框架。内部封装了jdbc，使开发更简洁，更高效。 Mybatis使开发者只需要关注sql语句本身，简化JDBC操作，不需要在关注加载驱动、创建连接、处理SQL语句等繁杂的过程。 Mybatis可以通过xml或注解完成ORM映射关系配置。 什么是ORM ORM的全称是Object Relational Mapping，即对象关系映射。 描述的是对象和表之间的映射。操作Java对象，通过映射关系，就可以自动操作数据库。 在ORM关系中，数据库表对应Java中的类，一条记录对应一个对象，一个属性对应一个列。 常见的ORM框架：Mybatis、Hibernate 案例 该例子主要为了结合springboot、example以及PageHelper分页使用 pom.xml org.springframework.boot spring-boot-starter-parent 2.2.5.RELEASE UTF-8 UTF-8 1.8 1.3.2 2.0.2 1.1.9 5.1.32 1.2.3 org.springframework.boot spring-boot-starter-web org.projectlombok lombok org.mybatis.spring.boot mybatis-spring-boot-starter ${mybatis.starter.version} tk.mybatis mapper-spring-boot-starter ${mapper.starter.version} com.github.pagehelper pagehelper-spring-boot-starter ${pageHelper.starter.version} com.alibaba druid-spring-boot-starter ${druid.starter.version} mysql mysql-connector-java ${mysql.version} org.springframework.boot spring-boot-starter-test test org.junit.vintage junit-vintage-engine application.yml server: port: 10400 spring: datasource: url: jdbc:mysql://localhost:3306/数据库?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai username: root password: 123 driver-class-name: com.mysql.jdbc.Driver # 德鲁伊连接池 druid: # 初始化大小,最小,最大 initial-size: 5 min-idle: 5 max-active: 20 # 获取连接等待超时的时间 max-wait: 60000 # 配置一个连接在池中最小生存的时间,单位是毫秒 min-evictable-idle-time-millis: 300000 application: name: book mybatis: configuration: # 在控制台输出sql日志 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl # 指定xml文件位置 mapper-locations: classpath:mapperxml/*.xml BookMapper.xml id, book_name, produce_date, book_pub, book_num SELECT from book WHERE id IN #{id} mapper public interface BookMapper extends Mapper { List findByIds(String[] bookIds); } service @Service public class BookService{ @Resource private BookMapper bookMapper; public List findAll() { return bookMapper.selectAll(); } public List findByIds(String[] ids) { return bookMapper.findByIds(ids); } public List findByPage(Integer pageNum,Integer pageSize) { PageHelper.startPage(pageNum,pageSize); Example example = new Example(Book.class); example.createCriteria().orIsNotNull(\"bookPub\"); return bookMapper.selectByExample(example); } } controller @RequestMapping(\"/book\") @RestController public class BookController { @Autowired private BookService bookService; @GetMapping(\"/findByIds\") public List findByIds(String[] ids){ System.out.println(Arrays.toString(ids)); return bookService.findByIds(ids); } } domain @Data @Table(name = \"book\") public class Book { @Id @Column(name = \"id\") @GeneratedValue(generator = \"JDBC\") private Integer id; @Column(name = \"book_name\") private String bookName; // 传入数据库的格式 @DateTimeFormat(pattern = \"yyyy-MM-dd\") // 返回数据的格式 @JsonFormat(pattern = \"yyyy-MM-dd\",timezone = \"GMT+8\") @Column(name = \"produce_date\") private LocalDate produceDate; @Column(name = \"book_pub\") private String bookPub; @Column(name = \"book_num\") private Integer bookNum; } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mybatis/注意事项.html":{"url":"mybatis/注意事项.html","title":"注意事项","keywords":"","body":"注意事项@Id@Transient热部署注意事项 @Id ​ 一定不要忘了添加该注解,在使用通用mapper时,有很多方法会根据主键对数据进行操作 // 正确写法 // import javax.persistence.*; @Id // 对应列名 @Column(name = \"id\") // 自增id 在插入后可以通过getId()获取到插入后的id @GeneratedValue(generator = \"JDBC\") private Integer id; @Transient ​ 如果实体类中有属性不是表中字段,添加该注解避免出现不必要的异常 ​ Mybatis-3.2.5及以上版本必须添加 @Transient private String cids; 热部署 通用mapper不支持热部署 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mybatis/mybatis-plus.html":{"url":"mybatis/mybatis-plus.html","title":"mybatis-plus","keywords":"","body":"简介依赖自动生成代码配置分页插件使用分页示例代码官方文档简介 MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 依赖 org.springframework.boot spring-boot-starter-parent 2.2.1.RELEASE org.springframework.boot spring-boot-starter-jdbc mysql mysql-connector-java 5.1.47 com.baomidou mybatis-plus-generator 3.2.0 org.freemarker freemarker 2.3.29 org.springframework.boot spring-boot-starter-test test org.projectlombok lombok true com.baomidou mybatis-plus-boot-starter 3.2.0 自动生成代码 // 演示例子，执行 main 方法控制台输入模块表名回车自动生成对应项目目录中 public class CodeGenerator { /** * * 读取控制台内容 * */ public static String scanner(String tip) { Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(\"请输入\" + tip + \"：\"); System.out.println(help.toString()); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) { return ipt; } } throw new MybatisPlusException(\"请输入正确的\" + tip + \"！\"); } public static void main(String[] args) { // 代码生成器 AutoGenerator mpg = new AutoGenerator(); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\"user.dir\"); gc.setOutputDir(projectPath + \"/src/main/java\"); gc.setAuthor(\"Tao\"); gc.setOpen(false); // gc.setIdType(IdType.NONE); // gc.setSwagger2(true); 实体属性 Swagger2 注解 mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\"jdbc:mysql://localhost:3306/数据库?useUnicode=true&useSSL=false&characterEncoding=utf8\"); // dsc.setSchemaName(\"public\"); dsc.setDriverName(\"com.mysql.jdbc.Driver\"); dsc.setUsername(\"root\"); dsc.setPassword(\"密码\"); dsc.setDbType(DbType.MYSQL); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(\"模块名\")); pc.setParent(\"父包\"); // pc.setService(\"%sService\"); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { // to do nothing } }; // 如果模板引擎是 freemarker String templatePath = \"/templates/mapper.xml.ftl\"; // 如果模板引擎是 velocity // String templatePath = \"/templates/mapper.xml.vm\"; // 自定义输出配置 List focList = new ArrayList<>(); // 自定义配置会被优先输出 focList.add(new FileOutConfig(templatePath) { @Override public String outputFile(TableInfo tableInfo) { // 自定义输出文件名 ， 如果你 Entity 设置了前后缀、此处注意 xml 的名称会跟着发生变化！！ return projectPath + \"/src/main/resources/mapper/\" + pc.getModuleName() + \"/\" + tableInfo.getEntityName() + \"Mapper\" + StringPool.DOT_XML; } }); /* cfg.setFileCreate(new IFileCreate() { @Override public boolean isCreate(ConfigBuilder configBuilder, FileType fileType, String filePath) { // 判断自定义文件夹是否需要创建 checkDir(\"调用默认方法创建的目录\"); return false; } }); */ cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); // 配置模板 TemplateConfig templateConfig = new TemplateConfig(); // 配置自定义输出模板 //指定自定义模板路径，注意不要带上.ftl/.vm, 会根据使用的模板引擎自动识别 // templateConfig.setEntity(\"templates/entity2.java\"); // templateConfig.setService(); // templateConfig.setController(); templateConfig.setXml(null); mpg.setTemplate(templateConfig); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); // strategy.setSuperEntityClass(\"com.baomidou.ant.common.BaseEntity\"); strategy.setEntityLombokModel(true); strategy.setRestControllerStyle(true); // 公共父类 // strategy.setSuperControllerClass(\"com.baomidou.ant.common.BaseController\"); // 写于父类中的公共字段 strategy.setSuperEntityColumns(\"id\"); strategy.setInclude(scanner(\"表名，多个英文逗号分割\").split(\",\")); strategy.setControllerMappingHyphenStyle(true); strategy.setTablePrefix(pc.getModuleName() + \"_\"); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); } } 配置分页插件 @Configuration public class MyBatisPlusConfig { @Bean public PaginationInterceptor paginationInterceptor(){ // 设置请求的页面大于最大页后操作， true调回到首页，false 继续请求 默认false // paginationInterceptor.setOverflow(false); // 设置最大单页限制数量，默认 500 条，-1 不受限制 // paginationInterceptor.setLimit(500); PaginationInterceptor page = new PaginationInterceptor(); //设置方言类型 page.setDialectType(\"mysql\"); return page; } 使用分页 @Test public void page(){ User user = new User(); // 分页查询加条件 所有字段为空正常查询 user.setAge(20); IPage page = iUserService.page(new Page(1, 3),new QueryWrapper(user)); page.getRecords().forEach(System.out::println); } 示例代码 ​ https://github.com/TaoQZ/mybatis-plus-learning-exam.git 官方文档 ​ https://mp.baomidou.com/ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mybatis/mybatis_advance.html":{"url":"mybatis/mybatis_advance.html","title":"mybatis进阶","keywords":"","body":"MyBatis XML版延迟加载缓存一级缓存二级缓存自定义缓存注意事项问题resultMap和resultType的区别为什么Mapper中的方法不能重载多条件查询博客MyBatis XML版 MyBatis3官方文档 https://mybatis.org/mybatis-3/zh/configuration.html#environments 延迟加载 针对有关联关系的对象,只加载单表,在属性需要时进行加载(查询数据库),如果开启了延迟加载,根据配置决定是否延迟加载 可以通过日志测试 --> --> 缓存 缓存的意义将⽤户经常查询的数据放在缓存（内存）中，⽤户去查询数据就不⽤从磁盘上(关系型数据库 数据⽂件)查询，从缓存中查询，从⽽提⾼查询效率，解决了⾼并发系统的性能问题。 一级缓存 mybatis默认开启一级缓存 TeacherMapper mapper = MyBatisUtil.getMapper(TeacherMapper.class); System.out.println(mapper.findById(1)); System.out.println(mapper.findById(2)); System.out.println(mapper.findById(1)); 更新操作会清空缓存 update和insert操作 TeacherMapper mapper = MyBatisUtil.getMapper(TeacherMapper.class); System.out.println(mapper.findById(1)); // mapper.insert(new Teacher()); mapper.update(new Teacher()); System.out.println(mapper.findById(1)); delete操作 无论真正是否删除都会清除缓存 TeacherMapper mapper = MyBatisUtil.getMapper(TeacherMapper.class); System.out.println(mapper.findById(1)); mapper.deleteById(100); System.out.println(mapper.findById(1)); 清除缓存 如果是更新操作会清除缓存 或者缓存的级别是STATEMENT 如果不想设置为STATEMENT,但是想清除本次的缓存可以使用SqlSession的清除缓存的方法 sqlSession1.clearCache(); 最终会落实到PerpetualCache类上 private final Map cache = new HashMap<>(); @Override public void clear() { cache.clear(); } // 在下面类中存储了全局的配置信息 package org.apache.ibatis.session; public class Configuration {} 一级缓存是SqlSession级别的 ​ 从下图可以看出,分别使用mapper1查询和mapper2更新,在mapper1中获取仍是更新前的数据,也就是脏数据,可以在配置文件中设置缓存范围 ​ 这样每次执行完一个Mapper中的语句后都会将一级缓存清除,同时也证实了一级缓存是SqlSession级别的 SqlSession sqlSession1= MyBatisCacheUtil.openSession(); SqlSession sqlSession2 = MyBatisCacheUtil.openSession(); System.out.println(sqlSession1); System.out.println(sqlSession2); TeacherMapper mapper1 = sqlSession1.getMapper(TeacherMapper.class); TeacherMapper mapper2 = sqlSession2.getMapper(TeacherMapper.class); System.out.println(\"mapper1 select\"+mapper1.findById(1)); System.out.println(\"mapper1 select\"+mapper1.findById(1)); Teacher teacher = new Teacher(1, \"Pink老师\"); System.out.println(\"mapper2 update\"+mapper2.update(teacher)); System.out.println(\"mapper1 select\"+mapper1.findById(1)); System.out.println(\"mapper2 select\"+mapper2.findById(1)); 一级缓存的执行流程 跟读源码 几个重点关注的类 // 执行sql相关 CachingExecutor BaseExecutor // 动态代理相关 Method package java.lang.reflect; Invocation // 缓存相关 Cache CacheKey PerpetualCache 添加缓存 key的组成部分 StatementId : nameSpace + 方法id offset limit sql param environment 从缓存获取 获取不到时从数据库查询 最终还是要落实到PreparedStatement上 将数据保存到PerpetualCache类中定义的 private final Map cache = new HashMap<>(); 获取缓存 比较key时会用到的方法 CacheKey内部的equals()和hashcode() @Override public boolean equals(Object object) { if (this == object) { return true; } if (!(object instanceof CacheKey)) { return false; } final CacheKey cacheKey = (CacheKey) object; if (hashcode != cacheKey.hashcode) { return false; } if (checksum != cacheKey.checksum) { return false; } if (count != cacheKey.count) { return false; } for (int i = 0; i 二级缓存 ​ 首先总结一下一级缓存,默认开启一级缓存,也就是session级别的,同一个sqlsession内共享缓存,但是每次执行更新操作时会清空缓存,这里的更新操作泛指insert、update和delete,如果想禁用一级缓存则需要把等级调至STATEMENT,意为每一条执行语句都会独立执行,不会创建缓存,也可以调用sqlseesion的方法手动清除缓存。 ​ 缓存的接口是Cache,最终会落实到PerpetualCache类中,该类中定义了一个Map集合,操作缓存数据,也就是在操作该map集合,其中该map集合的key为CacheKey,此类会将通过update方法闯入内部的变量进行计算得出hashCode值,具体变量在上面有提出。 ​ 但由于一级缓存的范围只在sqlsession同一会话级别,如果想把范围缩小可以将范围改为STATEMENT级别,但是将范围扩大就需要使用到二级缓存了 开启二级缓存 从源码可以看出,二级缓存是默认开启的 但是二级缓存将范围扩大到了不同的会话,同时颗粒度细分到了namespace,也就是sql映射文件中,需要在使用二级缓存的配置文件中设置 测试是否开启二级缓存,不同的sqlsession是否共享缓存 @Test public void demo() { SqlSession sqlSession1 = MyBatisCacheUtil.openSession(); SqlSession sqlSession2 = MyBatisCacheUtil.openSession(); GradeMapper mapper1 = sqlSession1.getMapper(GradeMapper.class); GradeMapper mapper2 = sqlSession2.getMapper(GradeMapper.class); System.out.println(\"mapper1 \" + mapper1.findById(1)); System.out.println(\"mapper2 \" + mapper2.findById(1)); // sqlSession1.commit(); // sqlSession1.close(); System.out.println(\"mapper2 \" + mapper2.findById(1)); } 从上图中可以看到我们预期的不同的sqlsession间共享缓存并没有成功(但是一级缓存还是会生效),将代码改为如下后再次执行 @Test public void demo() { SqlSession sqlSession1 = MyBatisCacheUtil.openSession(); SqlSession sqlSession2 = MyBatisCacheUtil.openSession(); GradeMapper mapper1 = sqlSession1.getMapper(GradeMapper.class); GradeMapper mapper2 = sqlSession2.getMapper(GradeMapper.class); System.out.println(\"mapper1 \" + mapper1.findById(1)); // 手动提交 sqlSession1.commit(); // 或者关闭 // sqlSession1.close(); System.out.println(\"mapper2 \" + mapper2.findById(1)); System.out.println(\"mapper2 \" + mapper2.findById(1)); } 可以看到二级缓存也生效了 但是需要我们手动提交,自动提交不会生效,这是为什么,追踪提交的源码 此时还没有开始遍历,缓存中还没有数据 在手动提交后,执行图下方法后才会将数据真正的添加到缓存区中,之前存储在了由TransactionalCacheManager内部的TransactionalCache的 private final Map entriesToAddOnCommit 中,也就是暂存区,是为了避免数据不提交或者数据回滚后造成的数据脏读问题,只有在提交后才会将暂存处存储的数据放至缓存区 查询时获取缓存,从MappedStatement中获取缓存,此时的缓存是在读取配置文件时,如果添加了缓存的标签,会创建对应的缓存对象 根据映射文件填充MappedStatement 读取缓存相关的配置信息 根据读取到的属性创建Cache build时对cache做了包装,装饰器模式,具体的装饰链。 SynchronizedCache -> LoggingCache -> SerializedCache -> LruCache -> PerpetualCache。 SynchronizedCache：同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache：日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache：序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache：采用了Lru算法的Cache实现，移除最近最少使用的Key/Value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 其中我们看到日志输出的命中率就是由LoggingCache做的,还有一层LruCache(队列方式存储缓存) public class LoggingCache implements Cache { private final Log log; private final Cache delegate; protected int requests = 0; protected int hits = 0; @Override public Object getObject(Object key) { // 请求次数 requests++; final Object value = delegate.getObject(key); if (value != null) { // 命中次数 hits++; } if (log.isDebugEnabled()) { log.debug(\"Cache Hit Ratio [\" + getId() + \"]: \" + getHitRatio()); } return value; } private double getHitRatio() { return (double) hits / (double) requests; } } id是映射文件的namespace 获取id将缓存对象添加到Configuration中 二级缓存关联查询 虽然二级缓存使用了TransactionalCacheManager和TransactionalCache来保证数据提交后进行缓存的更新,避免了脏读,但是这也只是单个独立的namespace级别的,如果有关联对象,那么关联对象发生变化时,不会进行及时更新(感应不到其他namespace的变化) 效果 @Test public void refCache(){ SqlSession sqlSession1 = MyBatisCacheUtil.openSession(); SqlSession sqlSession2 = MyBatisCacheUtil.openSession(); GradeMapper gradeMapper = sqlSession1.getMapper(GradeMapper.class); SchoolMapper schoolMapper = sqlSession2.getMapper(SchoolMapper.class); System.out.println(gradeMapper.findById(1)); sqlSession1.commit(); School school = new School(); school.setCid(1); school.setCname(\"交大\"); schoolMapper.update(school); sqlSession2.commit(); System.out.println(gradeMapper.findById(1)); } 在对应的SchoolMapper.xml中添加 后的效果,可以感知主表的更新,也就是将范围扩大到了不同的namespace,其中只要有namespace有更新操作,就会更新共同的Cache @Test public void refCache(){ SqlSession sqlSession1 = MyBatisCacheUtil.openSession(); SqlSession sqlSession2 = MyBatisCacheUtil.openSession(); GradeMapper gradeMapper = sqlSession1.getMapper(GradeMapper.class); SchoolMapper schoolMapper = sqlSession2.getMapper(SchoolMapper.class); System.out.println(gradeMapper.findById(1)); sqlSession1.commit(); School school = new School(); school.setCid(1); school.setCname(\"交大\"); schoolMapper.update(school); // 必须手动提交,才会更新缓存 sqlSession2.commit(); System.out.println(gradeMapper.findById(1)); } 有更新操作时,将对应的cache的标记clearOnCommit改为true,在提交时清空缓存 映射文件中如果有cache-ref 标签,在读取配置文件创建初始信息时,如果其对应名称的cache还没有创建,会将其保存至一个linkedlist集合的尾部,等到对应的namespace的cache创建好时,与其使用同一个cache,以此来达到数据更新时同步更新同一个cache LruCache 其主要实现为如下,map是一个有存储限制的队列map,如果存储过大,将最前面的数据清除,也就是队列,先进先出,从而达到其清除最久未使用的缓存效果 /** * Lru (least recently used) cache decorator. * * @author Clinton Begin */ public class LruCache implements Cache { private final Cache delegate; private Map keyMap; private Object eldestKey; public LruCache(Cache delegate) { this.delegate = delegate; setSize(1024); } @Override public String getId() { return delegate.getId(); } @Override public int getSize() { return delegate.getSize(); } public void setSize(final int size) { keyMap = new LinkedHashMap(size, .75F, true) { private static final long serialVersionUID = 4267176411845948333L; @Override protected boolean removeEldestEntry(Map.Entry eldest) { boolean tooBig = size() > size; if (tooBig) { eldestKey = eldest.getKey(); } return tooBig; } }; } @Override public void putObject(Object key, Object value) { delegate.putObject(key, value); cycleKeyList(key); } @Override public Object getObject(Object key) { keyMap.get(key); // touch return delegate.getObject(key); } @Override public Object removeObject(Object key) { return delegate.removeObject(key); } @Override public void clear() { delegate.clear(); keyMap.clear(); } private void cycleKeyList(Object key) { keyMap.put(key, key); if (eldestKey != null) { delegate.removeObject(eldestKey); eldestKey = null; } } } private Object eldestKey; @Test public void demo4(){ int size = 3; LinkedHashMap map = new LinkedHashMap<>(5); map = new LinkedHashMap(size, .75F, true) { private static final long serialVersionUID = 4267176411845948333L; @Override protected boolean removeEldestEntry(Map.Entry eldest) { boolean tooBig = size() > size; if (tooBig) { eldestKey = eldest.getKey(); } return tooBig; } }; map.put(\"1\",\"1\"); map.put(\"2\",\"2\"); map.put(\"3\",\"3\"); map.put(\"4\",\"4\"); System.out.println(map); } // 打印结果 // {2=2, 3=3, 4=4} 二级缓存注意事项 实体类必须实现Serializable接口(Cache的装饰者其中有一层用到了序列化,可以通过提交后报错查看) 自定义缓存 其他配置 这行语句的默认配置 映射语句文件中的所有 select 语句的结果将会被缓存。 映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。 缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。 缓存不会定时进行刷新（也就是说，没有刷新间隔）。 缓存会保存列表或对象（无论查询方法返回哪种）的 1024 个引用。 缓存会被视为读/写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改。 这些属性可以修改 这个更高级的配置创建了一个 FIFO 缓存，每隔 60 秒刷新，最多可以存储结果对象或列表的 512 个引用，而且返回的对象被认为是只读的，因此对它们进行修改可能会在不同线程中的调用者产生冲突。 可用的清除策略有： LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 默认的清除策略是 LRU。 flushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 提示 二级缓存是事务性的。这意味着，当 SqlSession 完成并提交时，或是完成并回滚，但没有执行 flushCache=true 的 insert/delete/update 语句时，缓存会获得更新。 请注意，缓存的配置和缓存实例会被绑定到 SQL 映射文件的命名空间中。 因此，同一命名空间中的所有语句和缓存将通过命名空间绑定在一起。 每条语句可以自定义与缓存交互的方式，或将它们完全排除于缓存之外，这可以通过在每条语句上使用两个简单属性来达成。 默认情况下，语句会这样来配置： 鉴于这是默认行为，显然你永远不应该以这样的方式显式配置一条语句。但如果你想改变默认的行为，只需要设置 flushCache 和 useCache 属性。比如，某些情况下你可能希望特定 select 语句的结果排除于缓存之外，或希望一条 select 语句清空缓存。类似地，你可能希望某些 update 语句执行时不要刷新缓存。 注意事项 Mybatis的⼀级缓存是sqlSession级别的。只能访问⾃⼰的sqlSession内的缓存。如果Mybatis与Spring整合了，Spring会⾃动关闭sqlSession的。所以⼀级缓存会失效的。 ⼀级缓存的原理是map集合，Mybatis默认就⽀持⼀级缓存 ⼆级缓存是Mapper级别的。只要在Mapper命名空间下都可以使⽤⼆级缓存。需要我们⾃⼰⼿动 去配置⼆级缓存 Mybatis的缓存我们可以使⽤Ehcache框架来进⾏管理，Ehcache实现Cache接⼝就代表使⽤Ehcache来环境Mybatis缓存。 由于之前写的DaoImpl是有⾮常多的硬编码的。可以使⽤Mapper代理的⽅式来简化开发 命名空间要与JavaBean的全类名相同 sql⽚段语句的id要与Dao接⼝的⽅法名相同 ⽅法的参数和返回值要与SQL⽚段的接收参数类型和返回类型相同。 问题 resultMap和resultType的区别 ​ 两种都可以描述返回值的类型,在返回值为非对象和对象,无论对象是单个对象还是一个集合时都可以使用resultType,当需要使用关联对象的映射时需要使用resultMap,一般使用resultMap即可,一般都作用在select标签上,两者必须选其一,否则会报如下错误 Cause: org.apache.ibatis.executor.ExecutorException: A query was run and no Result Maps were found for the Mapped Statement 'xyz.taoqz.domain.Student.findAll'. It's likely that neither a Result Type nor a Result Map was specified. 为什么Mapper中的方法不能重载 mybatis在操作时底层会将所有的配置信息读取到Configuration类中，同时mapper中的每个方法也会被保存到一个map里，而这个map的key就是namespace+方法名，而方法重载则是方法名形同参数不同，所以这种存储机制不允许方法重载 多条件查询 or (user_id = '1') or (user_id = '1400032429183381505') --> --> --> --> --> --> 博客 http://www.justdojava.com/2019/07/13/MyBatis-OneLevelCache/ https://tech.meituan.com/2018/01/19/mybatis-cache.html https://mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache https://www.codenong.com/cs107068256/ https://juejin.im/post/6847902224975314958 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"maven/maven.html":{"url":"maven/maven.html","title":"Maven","keywords":"","body":"Mavenmaven是什么?为什么使用maven?下载与配置配置环境变量:阿里镜像本地仓库远程仓库IDEA中配置maven入门使用坐标version后缀生命周期手动安装jar包pom配置微服务下多环境多层级时单独打包pom文件常用标签依赖的生命周期注意事项pom标签大全详解Maven maven是什么? Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。 为什么使用maven? 在之前的项目中,每次都需要先导入很多jar包,并且出现依赖冲突没有直观显示,使用maven后不用在手动导入很多jar包, 只需要引入jar在仓库对应的坐标即可,大大减小了项目的体积 下载与配置 下载地址:https://maven.apache.org/download.cgi windows环境下选择二进制压缩包进行下载 配置环境变量: ​ 可以先在系统变量创建一个MAVEN_HOME指向maven安装路径,然后在path中添加此变量,并指向bin目 录%MAVEN_HOME%\\bin,这样做的目的是如果maven换了安装目录只需要更改MAVEN_HOME,而不需要更改path,避免出 现其他问题,配置好后使用mvn - v 命令查看版本 阿里镜像 主要的配置文件在conf文件夹中的settings.xml,其默认使用的是maven的中央仓库,为了解决国内下载速度慢的情况 找到mirrors标签添加 alimaven central aliyun maven http://maven.aliyun.com/nexus/content/groups/public/ maven安装后会有一个默认的仓库在c盘用户的文件夹.m2,更改其默认的仓库 本地仓库 在settings.xml中配置自定义本地仓库的位置 仓库所在磁盘路径 远程仓库 使用Nexus搭建maven私服 需要配置认证信息,在maven的settings.xml文件的serves节点添加 nexus-releases admin admin123 nexus-snapshots admin admin123 自动化部署,上传 命令 : mvn deploy nexus-releases Nexus Release Repository http://127.0.0.1:8081/repository/maven-releases/ nexus-snapshots Nexus Snapshot Repository http://127.0.0.1:8081/repository/maven-snapshots/ 配置代理仓库 修改url地址即可 nexus Nexus Repository http://127.0.0.1:8081/repository/maven-public/ true true nexus Nexus Plugin Repository http://127.0.0.1:8081/repository/maven-public/ true true IDEA中配置maven ​ 点击左上角File ---> other-settings ---> settings for new projects ​ 搜索maven ​ 设置创建新项目时的配置,更改本项目的配置在File ---> setting ---> 中搜索maven做同样的配置 解决IDEA中创建项目慢,因为默认使用的是远程创建 切换成使用本地的配置文件创建项目 入门使用 坐标 ​ 以下面的依赖作为示例解释坐标的概念 ​ 其实就是该jar(依赖)在仓库的位置 仓库所在磁盘位置\\org\\projectlombok\\lombok\\1.18.10 org.projectlombok 组名 lombok 模块名 1.18.10 版本号 version后缀 SNAPSHOT：快照版本,此jar包没有经过测试,很多bug RC：此jar包经过了充分测试，但是可能还有很多bug BETA：上线前的测试版本 RELEASE：正式版 生命周期 阶段 处理 描述 验证 validate 验证项目 验证项目是否正确且所有必须信息是可用的 编译 compile 执行编译 源代码编译在此阶段完成 测试 Test 测试 使用适当的单元测试框架（例如JUnit）运行测试。 包装 package 打包 创建JAR/WAR包如在 pom.xml 中定义提及的包 检查 verify 检查 对集成测试的结果进行检查，以保证质量达标 安装 install 安装 安装打包的项目到本地仓库，以供其他项目使用 部署 deploy 部署 拷贝最终的工程包到远程仓库中，以共享给其他开发人员和工程 手动安装jar包 com.aspose.words aspose-words-jdk16 7.0.0 mvn install:install-file -Dfile=jar包的位置（源文件） -DgroupId=上面的groupId -DartifactId=上面的artifactId -Dversion=上面的version -Dpackaging=jar mvn install:install-file -Dfile=F:\\下载\\谷歌下载\\aspose.words.jdk16-7.0.0.jar -DgroupId=com.aspose.words -DartifactId=aspose-words-jdk16 -Dversion=7.0.0 -Dpackaging=jar pom配置微服务下多环境 tao dev http://192.168.91.50:8848 4339db89-ce06-44e3-9e14-b4aed9b13c80 true # Spring spring: application: # 应用名称 name: profiles: # 环境配置 active: @env@ cloud: nacos: discovery: # 服务注册地址 server-addr: @nacosServerAddr@ namespace: @nacosNamespace@ config: # 配置中心地址 server-addr: @nacosServerAddr@ namespace: @nacosNamespace@ # 配置文件格式 file-extension: yml # 共享配置 shared-configs: - application-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension} 多层级时单独打包 # 打二级包 mvn clean package -pl food-auth -am # 打三级包 mvn clean package -pl .\\food-modules\\food-file\\ -am pom文件常用标签 org.springframework.boot spring-boot-starter-parent 2.2.1.RELEASE jar 子模块 UTF-8 UTF-8 1.8 5.1.32 org.springframework.boot spring-boot-starter-web mysql mysql-connector-java ${mysql.version} tk.mybatis mapper 3.5.2 javax.persistence persistence-api 依赖的生命周期 compile : 编译范围,默认值 provided : 打包时需要,例如写servlet程序时需要依赖servlet-api进行编译,但是运行在tomcat容器中有相同的依赖会引起依赖冲突,此时需要添加此注解 runtime : 仅参与项目的运行阶段,例如mysql的驱动 test : 该依赖仅参与测试相关的内容,例如junit system : 使用的非maven仓库中依赖,引用的是本地文件系统的jar 注意事项 解决依赖下载不完全的问题 创建.bat文件,更改目录完毕后,双击运行 set REPOSITORY_PATH=指向maven仓库 rem 正在搜索... for /f \"delims=\" %%i in ('dir /b /s \"%REPOSITORY_PATH%\\*lastUpdated*\"') do ( del /s /q %%i ) rem 搜索完毕 pause install打包到本地仓库 将项目封装为工具类时,应该打包成为一个不可运行的jar,所以需要将下方插件依赖删除 org.springframework.boot spring-boot-maven-plugin pom标签大全详解 4.0.0 asia.banseon banseon-maven2 jar 1.0-SNAPSHOT banseon-maven http://www.baidu.com/banseon A maven project to study maven. jira http://jira.baidu.com/banseon Demo banseon@126.com banseon@126.com banseon@126.com http:/hi.baidu.com/banseon/demo/dev/ HELLO WORLD banseon banseon@126.com Project Manager Architect demo http://hi.baidu.com/banseon No -5 Apache 2 http://www.baidu.com/banseon/LICENSE-2.0.txt repo A business-friendly OSS license scm:svn:http://svn.baidu.com/banseon/maven/banseon/banseon-maven2-trunk(dao-trunk) scm:svn:http://svn.baidu.com/banseon/maven/banseon/dao-trunk http://svn.baidu.com/banseon demo http://www.baidu.com/banseon ...... ...... Windows XP Windows x86 5.1.2600 mavenVersion 2.0.3 /usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/ /usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/ ...... ...... ...... ...... ...... ...... banseon-repository-proxy banseon-repository-proxy http://192.168.1.169:9999/repository/ default ...... org.apache.maven maven-artifact 3.8.1 jar test spring-core org.springframework true ...... banseon-maven2 banseon maven2 file://${basedir}/target/deploy banseon-maven2 Banseon-maven2 Snapshot Repository scp://svn.baidu.com/banseon:/usr/local/maven-snapshot banseon-site business api website scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web value。 --> Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"http/状态码.html":{"url":"http/状态码.html","title":"状态码","keywords":"","body":"Http状态码状态码分类Http状态码列表Http状态码 状态码分类 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 Http状态码列表 状态码 状态码英文名称 中文描述 100 Continue 继续。客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 200 OK 请求成功。一般用于GET与POST请求 201 Created 已创建。成功请求并创建了新的资源 202 Accepted 已接受。已经接受请求，但未处理完成 203 Non-Authoritative Information 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 204 No Content 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 205 Reset Content 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 206 Partial Content 部分内容。服务器成功处理了部分GET请求 300 Multiple Choices 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 301 Moved Permanently 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 303 See Other 查看其它地址。与301类似。使用GET和POST请求查看 304 Not Modified 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 305 Use Proxy 使用代理。所请求的资源必须通过代理访问 306 Unused 已经被废弃的HTTP状态码 307 Temporary Redirect 临时重定向。与302类似。使用GET请求重定向 400 Bad Request 客户端请求的语法错误，服务器无法理解 401 Unauthorized 请求要求用户的身份认证 402 Payment Required 保留，将来使用 403 Forbidden 服务器理解请求客户端的请求，但是拒绝执行此请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置\"您所请求的资源无法找到\"的个性页面 405 Method Not Allowed 客户端请求中的方法被禁止 406 Not Acceptable 服务器无法根据客户端请求的内容特性完成请求 407 Proxy Authentication Required 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 408 Request Time-out 服务器等待客户端发送的请求时间过长，超时 409 Conflict 服务器完成客户端的 PUT 请求时可能返回此代码，服务器处理请求时发生了冲突 410 Gone 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 411 Length Required 服务器无法处理客户端发送的不带Content-Length的请求信息 412 Precondition Failed 客户端请求信息的先决条件错误 413 Request Entity Too Large 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 414 Request-URI Too Large 请求的URI过长（URI通常为网址），服务器无法处理 415 Unsupported Media Type 服务器无法处理请求附带的媒体格式 416 Requested range not satisfiable 客户端请求的范围无效 417 Expectation Failed 服务器无法满足Expect的请求头信息 500 Internal Server Error 服务器内部错误，无法完成请求 501 Not Implemented 服务器不支持请求的功能，无法完成请求 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 504 Gateway Time-out 充当网关或代理的服务器，未及时从远端服务器获取请求 505 HTTP Version not supported 服务器不支持请求的HTTP协议的版本，无法完成处理 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"minio/minio.html":{"url":"minio/minio.html","title":"minio","keywords":"","body":"Linux安装miniominio下载地址设置访问权限后台启动结合java,springboot使用依赖工具类枚举类application.ymlcontrollerLinux安装minio minio下载地址 http://www.minio.org.cn/download.shtml#/linux 设置访问权限 ​ 访问权限的作用: minio中的资源通常需要nginx转发访问,如果想通过浏览器直接访问必须设置公开访问权限public,如果没有权限直接访问访问不到。 使用命令设置bucket的访问权限 mc policy set public /home/data/minioData/upload 在管理员界面设置bucket的访问权限(推荐) 后台启动 新建run.sh添加以下内容 添加运行权限 chmod 777 ./run.sh # 账号 export MINIO_ACCESS_KEY=minioadmin # 密码 export MINIO_SECRET_KEY=minioadmin # nohup minio的server所在路径 server minio的存储文件所在路径 nohup /usr/local/minio/minio server /home/data/minioData & 结合java,springboot使用 依赖 org.springframework.boot spring-boot-starter-web cn.hutool hutool-all 5.3.3 io.minio minio 6.0.8 工具类 package xyz.taoqz.utils; import cn.hutool.core.collection.ConcurrentHashSet; import cn.hutool.core.io.FileUtil; import com.sun.javaws.exceptions.InvalidArgumentException; import io.minio.MinioClient; import io.minio.errors.*; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Component; import org.springframework.util.StringUtils; import org.springframework.web.multipart.MultipartFile; import org.xmlpull.v1.XmlPullParserException; import xyz.taoqz.enums.FileBusinessModuleEnum; import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.io.InputStream; import java.security.InvalidKeyException; import java.security.NoSuchAlgorithmException; import java.text.SimpleDateFormat; import java.util.Date; import java.util.Set; import java.util.concurrent.atomic.AtomicInteger; /** * 文件上传工具类 * 上传文件目录结构为： * upload / 业务码 / 年月 / 日 / 时分秒毫秒 _ 文件名 * bucket 和 文件名 * * @author wujin * @date 2019/6/25 */ @Component public class FileUploadUtilByMinio { @Value(\"${minio.serverUrl}\") private String serverUrl; @Value(\"${minio.accessKey}\") private String accessKey; @Value(\"${minio.secretKey}\") private String secretKey; @Value(\"${minio.tempserverUrl}\") private String tempserverUrl; @Value(\"${minio.tempaccessKey}\") private String tempaccessKey; @Value(\"${minio.tempsecretKey}\") private String tempsecretKey; @Value(\"${minio.defaultBucket}\") private String defaultBucket; @Value(\"${spring.profiles.active}\") private String profileActive; @Value(\"${server.port}\") private String serverPort; private final Set bucketCreateSet = new ConcurrentHashSet<>(); private static final String EMPTY_STRING = \"\"; private static final String FILE_SEPARATOR = \"/\"; private static final String DATE_STRING = \"yyMM/dd\"; private static final String TIME_STRING = \"HHmmssSSS\"; private static Logger log = LoggerFactory.getLogger(FileUploadUtilByMinio.class); /** * 上传文件到 默认模块下 * * @author wujin * @date 2019/6/25 */ public String uploadByFilePath(String filePath) { return uploadByFilePath(filePath, FileBusinessModuleEnum.DEFAULT); } /** * 上传文件到 指定模块下 * * @author wujin * @date 2019/6/25 */ public String uploadByFilePath(String filePath, FileBusinessModuleEnum bussinessModuleEnum) { return uploadByFilePath(filePath, bussinessModuleEnum, getFileNameByFilePath(filePath)); } /** * 自定义文件名到 指定模块下 * * @author wujin * @date 2019/6/25 */ public String uploadByFilePath(String filePath, FileBusinessModuleEnum bussinessModuleEnum, String fileName) { return uploadByFilePath(filePath, bussinessModuleEnum, defaultBucket, fileName); } /** * 自定义文件名到 指定模块、指定bucket * * @author wujin * @date 2019/6/25 */ private String uploadByFilePath(String filePath, FileBusinessModuleEnum bussinessModuleEnum, String fileBucket, String fileName) { try { MinioClient minioClient = new MinioClient(tempserverUrl, tempaccessKey, tempsecretKey); checkAndCreateBucket(minioClient, fileBucket); // 业务名 + 年月日 + 时分秒和文件名 String objectName = getRandomFilePathByBussiness(bussinessModuleEnum, fileName); minioClient.putObject(fileBucket, objectName, filePath); log.info(\"minIO upload success\"); String url = minioClient.getObjectUrl(fileBucket, objectName); log.info(\"minIO upload url====: \" + url); return FILE_SEPARATOR + fileBucket + FILE_SEPARATOR + objectName; } catch (Exception e) { log.error(\"minIO upload Error occurred: \" + e); } return EMPTY_STRING; } /** * @param fileName (包含uuid + 文件名) * @author wujin * 2019年10月8日9:24:13 */ public String uploadForDocConvert(String filePath, String fileName) { try { MinioClient minioClient = new MinioClient(tempserverUrl, tempaccessKey, tempsecretKey); checkAndCreateBucket(minioClient, defaultBucket); // 业务名 + 年月/日 + 文件名 String objectName = profileActive + FILE_SEPARATOR + FileBusinessModuleEnum.QUESTION_BANK_PIC.getBussCode() + FILE_SEPARATOR + getDateStringByType(DATE_STRING, new Date()) + FILE_SEPARATOR + fileName; minioClient.putObject(defaultBucket, objectName, filePath); log.info(\"minIO upload success\"); String url = minioClient.getObjectUrl(defaultBucket, objectName); log.info(\"minIO upload url====: \" + url); return FILE_SEPARATOR + defaultBucket + FILE_SEPARATOR + objectName; } catch (Exception e) { log.error(\"minIO upload Error occurred: \" + e); } return EMPTY_STRING; } private static AtomicInteger fileFlag = new AtomicInteger(10000); private String getRandomFilePathByBussiness(FileBusinessModuleEnum bussinessModuleEnum, String fileName) { String filePath = profileActive + FILE_SEPARATOR + bussinessModuleEnum.getBussCode() + FILE_SEPARATOR + getDateStringByType(DATE_STRING, new Date()) + FILE_SEPARATOR + getDateStringByType(TIME_STRING, new Date()) + \"_\" + serverPort + \"_\" + fileFlag.incrementAndGet() + \"f\"; if (!StringUtils.isEmpty(fileName) && fileName.lastIndexOf(\".\") != -1) { String fileType = fileName.substring(fileName.lastIndexOf(\".\") + 1); filePath += (\".\" + fileType); } return filePath; } /** * 上传file 到某模块 及默认bucket * * @author wujin * @date 2019/6/26 */ public String uploadByMultipartFile(MultipartFile file, FileBusinessModuleEnum bussinessModuleEnum) { return uploadByMultipartFile(file, bussinessModuleEnum, defaultBucket); } public String uploadByFile(File file, FileBusinessModuleEnum bussinessModuleEnum) { return uploadByFile(file, bussinessModuleEnum, defaultBucket); } /** * 上传file 到某模块 某bucket * * @author wujin * @date 2019/6/26 */ private String uploadByMultipartFile(MultipartFile file, FileBusinessModuleEnum bussinessModuleEnum, String fileBucket) { try { MinioClient minioClient = new MinioClient(tempserverUrl, tempaccessKey, tempsecretKey); checkAndCreateBucket(minioClient, fileBucket); // 业务名 + 年月日 + 时分秒和文件名 String objectName = getRandomFilePathByBussiness(bussinessModuleEnum, file.getOriginalFilename()); minioClient.putObject(fileBucket, objectName, file.getInputStream(), file.getContentType()); log.info(\"minIO upload success\"); /*String url = minioClient.getminioClientObjectUrl(fileBucket,objectName); log.info(\"minIO upload url====: \"+url);*/ return FILE_SEPARATOR + fileBucket + FILE_SEPARATOR + objectName; } catch (Exception e) { log.error(\"minIO upload Error occurred: \" + e); } return EMPTY_STRING; } private String uploadByFile(File file, FileBusinessModuleEnum bussinessModuleEnum, String fileBucket) { try { MinioClient minioClient = new MinioClient(tempserverUrl, tempaccessKey, tempsecretKey); checkAndCreateBucket(minioClient, fileBucket); // 业务名 + 年月日 + 时分秒和文件名 String objectName = getRandomFilePathByBussiness(bussinessModuleEnum, file.getName()); FileInputStream fileInputStream = new FileInputStream(file); minioClient.putObject(fileBucket, objectName,fileInputStream, FileUtil.getMimeType(file.getAbsolutePath())); fileInputStream.close(); log.info(\"minIO upload success\"); /*String url = minioClient.getminioClientObjectUrl(fileBucket,objectName); log.info(\"minIO upload url====: \"+url);*/ return FILE_SEPARATOR + fileBucket + FILE_SEPARATOR + objectName; } catch (Exception e) { log.error(\"minIO upload Error occurred: \" + e); } return EMPTY_STRING; } public void deleteFileByPath(String filePath) { if (!StringUtils.isEmpty(filePath) && FILE_SEPARATOR.equals(filePath.substring(0, 1))) { filePath = filePath.substring(1); if (filePath.contains(FILE_SEPARATOR)) { String bucket = filePath.substring(0, filePath.indexOf(FILE_SEPARATOR)); String objectName = filePath.substring(filePath.indexOf(FILE_SEPARATOR) + 1); try { MinioClient minioClient = new MinioClient(serverUrl, accessKey, secretKey); minioClient.removeObject(bucket, objectName); } catch (Exception e) { log.error(\"minIO upload Error occurred: \" + e); } } } } public String getFileTypeByFileName(String fileName) { if (StringUtils.isEmpty(fileName) || fileName.lastIndexOf(\".\") == -1) { return \"\"; } else { return fileName.substring(fileName.lastIndexOf(\".\") + 1); } } private String getDateStringByType(String dateType, Date date) { SimpleDateFormat sdf = new SimpleDateFormat(dateType); return sdf.format(date); } private String getFileNameByFilePath(String filePath) { if (!StringUtils.isEmpty(filePath)) { if (filePath.contains(File.separator)) { return filePath.substring(filePath.lastIndexOf(File.separator) + 1); } else if (filePath.contains(\"/\")) { return filePath.substring(filePath.lastIndexOf(\"/\") + 1); } else if (filePath.contains(\"\\\\\")) { return filePath.substring(filePath.lastIndexOf(\"\\\\\") + 1); } } return EMPTY_STRING; } public InputStream downloadFileByStream(String bucket, String path) { InputStream inputStream = null; try { MinioClient minioClient = new MinioClient(serverUrl, accessKey, secretKey); inputStream = minioClient.getObject(bucket, path); } catch (InvalidEndpointException | InvalidPortException | InvalidBucketNameException | NoSuchAlgorithmException | InsufficientDataException | IOException | InvalidKeyException | NoResponseException | XmlPullParserException | ErrorResponseException | InternalException | io.minio.errors.InvalidArgumentException e) { e.printStackTrace(); } return inputStream; } public void downloadFile(String bucket, String path, String targetFile) { try { MinioClient minioClient = new MinioClient(serverUrl, accessKey, secretKey); minioClient.getObject(bucket, path, targetFile); } catch (Exception e) { e.printStackTrace(); } } private void checkAndCreateBucket(MinioClient minioClient, String fileBucket) throws IOException, InvalidKeyException, NoSuchAlgorithmException, InsufficientDataException, InternalException, NoResponseException, InvalidBucketNameException, XmlPullParserException, ErrorResponseException, RegionConflictException { if (!bucketCreateSet.contains(fileBucket)) { synchronized (bucketCreateSet) { boolean isExist = minioClient.bucketExists(fileBucket); log.info(\"Bucket already exists flag: \" + fileBucket); if (!isExist) { minioClient.makeBucket(fileBucket); } bucketCreateSet.add(fileBucket); } } } public boolean statObject(String filePath){ boolean flag = false; if (!StringUtils.isEmpty(filePath) && FILE_SEPARATOR.equals(filePath.substring(0, 1))) { filePath = filePath.substring(1); if (filePath.contains(FILE_SEPARATOR)) { String bucket = filePath.substring(0, filePath.indexOf(FILE_SEPARATOR)); String objectName = filePath.substring(filePath.indexOf(FILE_SEPARATOR) + 1); try { MinioClient minioClient = new MinioClient(serverUrl, accessKey, secretKey); minioClient.statObject(bucket, objectName); flag = true; } catch (Exception e) { log.error(\"minIO Object does not exist: \" + e); } } } return flag; } } 枚举类 package xyz.taoqz.enums; import java.util.HashMap; import java.util.Map; /** * @author wujin */ public enum FileBusinessModuleEnum { /** * 目录 */ DEFAULT(\"default\", \"通用目录\") ; private final String bussCode; private final String bussMsg; FileBusinessModuleEnum(String bussCode, String bussMsg) { this.bussCode = bussCode; this.bussMsg = bussMsg; } private static final Map CURRENT_ENUM_MAP = new HashMap<>(); static { for (FileBusinessModuleEnum type : FileBusinessModuleEnum.values()) { CURRENT_ENUM_MAP.put(type.getBussCode(), type); } } public static FileBusinessModuleEnum getEnumByCode(String code) { return CURRENT_ENUM_MAP.get(code) == null ? FileBusinessModuleEnum.DEFAULT : CURRENT_ENUM_MAP.get(code); } public String getBussCode() { return bussCode; } public String getBussMsg() { return bussMsg; } } application.yml server: port: 9099 spring: profiles: active: dev servlet: multipart: max-request-size: 200MB max-file-size: 100MB minio: serverUrl: http://39.107.142.3:9000 accessKey: minio secretKey: tao.120908!!! tempserverUrl: http://39.107.142.3:9000 tempaccessKey: minio tempsecretKey: tao.120908!!! defaultBucket: myupload controller @RestController @RequestMapping(\"/upload\") public class UploadController { private final FileUploadUtilByMinio fileUploadUtilByMinio; public UploadController(FileUploadUtilByMinio fileUploadUtilByMinio) { this.fileUploadUtilByMinio = fileUploadUtilByMinio; } @PostMapping public String fun(MultipartFile file) { fileUploadUtilByMinio.uploadByMultipartFile(file, FileBusinessModuleEnum.BOOK_CATALOG); return \"OK!!\"; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"minio/minio搭建集群.html":{"url":"minio/minio搭建集群.html","title":"minio搭建集群","keywords":"","body":"minio搭建集群下载搭建nginx负载均衡集群服务器下线影响minio搭建集群 minio最少要求4台服务器 在一台服务器上操作,其他服务器也会有影响 下载 https://min.io/download#/linux 搭建 以下命令需要在所有集群服务器上运行 # 创建minio文件(存储minio软件及数据),将下载的minio保存至/usr/local/minio下 mkdir -p /usr/local/minio/miniodata # 使之可运行 chmod +x minio # 账号 export MINIO_ACCESS_KEY=minio # 密码 export MINIO_SECRET_KEY=minio123 # 运行启动 /usr/local/minio/miniodata 则为真正存储数据的文件夹 # 集群通信认证也是根据上面的账号密码 ./minio server http://192.168.84.103/usr/local/minio/miniodata http://192.168.84.105/usr/local/minio/miniodata http://192.168.84.107/usr/local/minio/miniodata http://192.168.84.108/usr/local/minio/miniodata nginx负载均衡 集群搭载完后可在任意一台服务器上搭建nginx进行负载均衡 nginx主要配置 upstream myserver { # ip_hash; # ip_hash 方式 # fair; # fair 方式 server 192.168.84.103:9000; # 负载均衡目的服务地址 server 192.168.84.105:9000; # weight=2; weight 方式，不写默认为 1 server 192.168.84.107:9000; server 192.168.84.108:9000; } server { listen 8088; location / { # 此处的myserver对应上面 upstream后的名称,名称中最好不要加_ 下划线 proxy_pass http://myserver; proxy_connect_timeout 10; # 显示具体负载的机器的ip,X-Route-Ip随便命名 add_header X-Route-Ip $upstream_addr; add_header X-Route-Status $upstream_status; } } 集群服务器下线影响 以四台为例 三台: 可进行正常操作 两台: 可正常访问,不可对文件进行操作(上传,删除等) 一台: 访问出错 We encountered an internal error, please try again.: cause(Read failed. Insufficient number of disks online) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"nginx/nginx.html":{"url":"nginx/nginx.html","title":"nginx","keywords":"","body":"Nginx概述应用场景环境安装nginx相关文件防火墙安装node&nvm&gitnginx常用命令windows下常用命令nginx配置语法配置文件结构图配置语法规则典型配置全局变量配置server配置反向代理跨域配置什么是跨域?跨域产生的原因?解决跨域配置gzip负载均衡nginx负载均衡策略server 后缀配置解析适配PC和移动端配置HTTPS其他常用技巧图片防盗链请求过滤配置图片、字体等静态文件缓存HTTP请求转发到HTTPS遇到的问题参考链接Nginx 概述 Nginx 是一款高性能的 HTTP 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。由俄罗斯的程序设计师 Igor Sysoev 所开发，官方测试 Nginx 能够支支撑 5 万并发链接，并且 CPU、内存等资源消耗却非常低，运行非常稳定。 应用场景 HTTP 服务器：Nginx 是一个 HTTP 服务可以独立提供 HTTP 服务。可以做网页静态服务器。 虚拟主机：可以实现在一台服务器虚拟出多个网站。例如个人网站使用的虚拟主机。 反向代理，负载均衡：当网站的访问量达到一定程度后，单台服务器不能满足用户的请求时，需要用多台服务器集群可以使用 Nginx 做反向代理。并且多台服务器可以平均分担负载，不会因为某台服务器负载高宕机而某台服务器闲置的情况 环境 Centos7、nginx1.16.1 安装 # 安装 yum install nginx # 查看版本 nginx -v nginx相关文件 其中/etc/nginx为主要配置文件所在目录 # 查看nginx相关文件 rpm -ql nginx 防火墙 如果开启了防火墙,需要开启指定端口或者在服务器厂商配置中设置防火墙规则 systemctl start firewalld # 开启防火墙 systemctl stop firewalld # 关闭防火墙 systemctl status firewalld # 查看防火墙开启状态，显示running则是正在运行 firewall-cmd --reload # 重启防火墙，永久打开端口需要reload一下 # 添加开启端口，--permanent表示永久打开，不加是临时打开重启之后失效 firewall-cmd --permanent --zone=public --add-port=8888/tcp # 查看防火墙，添加的端口也可以看到 firewall-cmd --list-all 安装node&nvm&git node:用来运行javascript代码,由于js代码只能在浏览器运行会有很多限制,node.js为其提供了运行环境 nvm:nvm和npm的区别,npm可以集中管理依赖,可以是官方第三方开发者发布的一些优秀的工具,可以使用npm下载,而node.js更新的频率很快,可能依赖在作者发布时使用的node版本和当前自己使用的node版本冲突用不了,则可使用nvm下载指定的node版本。 # 下载 nvm，或者看官网的步骤 https://github.com/nvm-sh/nvm#install--update-script curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bash source ~/.bashrc # 安装完毕后，更新配置文件即可使用 nvm 命令 nvm ls-remote # 查看远程 node 版本 nvm install v12.16.3 # 选一个你要安装的版本安装，我这里选择 12.16.3 nvm list # 安装完毕查看安装的 node 版本 node -v # 查看是否安装好了 yum install git # git 安装 nginx常用命令 可使用-h查看所有命令 nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启 nginx -s reopen # 重启 Nginx nginx -s stop # 快速关闭 nginx -s quit # 等待工作进程处理完成后关闭 nginx -T # 查看当前 Nginx 最终的配置 nginx -t -c # 检查配置是否有问题，如果已经在配置目录，则不需要-c systemctl 是 Linux 系统应用管理工具 systemd 的主命令，用于管理系统，我们也可以用它来对 Nginx 进行管理，相关命令如下： systemctl start nginx # 启动 Nginx systemctl stop nginx # 停止 Nginx systemctl restart nginx # 重启 Nginx systemctl reload nginx # 重新加载 Nginx，用于修改配置后 systemctl enable nginx # 设置开机启动 Nginx systemctl disable nginx # 关闭开机启动 Nginx systemctl status nginx # 查看 Nginx 运行状态 windows下常用命令 # 启动 start nginx # 查看是否运行 tasklist /fi \"imagename eq nginx.exe\" # 检查配置文件是否正确 nginx -t -c /nginx安装路径/conf/nginx.conf # 重新加载配置文件 nginx -s reload # 快速停止 nginx -s stop # 完整有序的关闭 nginx -s quit nginx配置语法 /etc/nginx/nginx.conf为nginx的主要配置文件 配置文件结构图 main # 全局配置，对全局生效 ├── events # 配置影响 Nginx 服务器或与用户的网络连接 ├── http # 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置 │ ├── upstream # 配置后端服务器具体地址，负载均衡配置不可或缺的部分 │ ├── server # 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块 │ ├── server │ │ ├── location # server 块可以包含多个 location 块，location 指令用于匹配 uri │ │ ├── location │ │ └── ... │ └── ... └── ... 配置语法规则 配置文件由指令与指令块构成； 每条指令以 ; 分号结尾，指令与参数间以空格符号分隔； 指令块以 {} 大括号将多条指令组织在一起； include 语句允许组合多个配置文件以提升可维护性； 使用 # 符号添加注释，提高可读性； 使用 $ 符号使用变量； 部分指令的参数支持正则表达式； 典型配置 user nginx; # 运行用户，默认即是nginx，可以不进行设置 worker_processes 1; # Nginx 进程数，一般设置为和 CPU 核数一样 error_log /var/log/nginx/error.log warn; # Nginx 的错误日志存放目录 pid /var/run/nginx.pid; # Nginx 服务启动时的 pid 存放位置 events { use epoll; # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的) worker_connections 1024; # 每个进程允许最大并发数 } http { # 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置 # 设置日志模式 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; # Nginx访问日志存放位置 sendfile on; # 开启高效传输模式 tcp_nopush on; # 减少网络报文段的数量 tcp_nodelay on; keepalive_timeout 65; # 保持连接的时间，也叫超时时间，单位秒 types_hash_max_size 2048; include /etc/nginx/mime.types; # 文件扩展名与类型映射表 default_type application/octet-stream; # 默认文件类型 include /etc/nginx/conf.d/*.conf; # 加载子配置项 server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 deny 172.168.22.11; # 禁止访问的ip地址，可以为all allow 172.168.33.44；# 允许访问的ip地址，可以为all } error_page 500 502 503 504 /50x.html; # 默认50x对应的访问页面 error_page 400 404 error.html; # 同上 } } server 块可以包含多个 location 块，location 指令用于匹配 uri，语法： location [ = | ~ | ~* | ^~] uri { ... } 指令后面： = 精确匹配路径，用于不含正则表达式的 uri 前，如果匹配成功，不再进行后续的查找； ^~ 用于不含正则表达式的 uri 前，表示如果该符号后面的字符是最佳匹配，采用该规则，不再进行后续的查找； ~ 表示用该符号后面的正则去匹配路径，区分大小写； ~* 表示用该符号后面的正则去匹配路径，不区分大小写。跟 ~ 优先级都比较低，如有多个location的正则能匹配的话，则使用正则表达式最长的那个； 如果 uri 包含正则表达式，则必须要有 ~ 或 ~* 标志。 全局变量 Nginx 有一些常用的全局变量(内置预定义变量)，你可以在配置的任何位置使用它们，如下表： 全局变量名 功能 $host 请求信息中的 Host，如果请求中没有 Host 行，则等于设置的服务器名，不包含端口 $request_method 客户端请求类型，如 GET、POST $remote_addr 客户端的 IP 地址 $args 请求中的参数 $arg_PARAMETER GET 请求中变量名 PARAMETER 参数的值，例如：$http_user_agent(Uaer-Agent 值), $http_referer... $content_length 请求头中的 Content-length 字段 $http_user_agent 客户端agent信息 $http_cookie 客户端cookie信息 $remote_addr 客户端的IP地址 $remote_port 客户端的端口 $http_user_agent 客户端agent信息 $server_protocol 请求使用的协议，如 HTTP/1.0、HTTP/1.1 $server_addr 服务器地址 $server_name 服务器名称 $server_port 服务器的端口号 $scheme HTTP 方法（如http，https） 配置server 基于端口的虚拟主机 一个http指令块中可以有多个server指令块,可以监听不同的端口。同时在主配置文件(/etc/nginx/nginx.conf)的http指令块中有一行配置为 include /etc/nginx/conf.d/*.conf 表示该文件夹下所有的*.conf文件都会被该主配置文件作为子配置项进行加载 使用阿里云开通的域名做demo,首先在阿里云的域名解析中设置一个二级域名并将二级域名的ip地址指向服务器ip,设置完毕后在 /etc/nginx/conf.d下创建一个以.conf为结尾的文件,并写入如下内容 server { # 配置监听端口 listen 81; # 配置域名 server_name gcp.taoqz.xyz; location / { # 网站的根目录 root /usr/share/nginx/html/tao; # 网站的首页 index index.html; } } 配置完成后再配置文件目录下使用命令测试配置文件是否有误 # 测试配置文件 nginx -t # 成功时输出的内容 #nginx: the configuration file /etc/nginx/nginx.conf syntax is ok #nginx: configuration file /etc/nginx/nginx.conf test is successful # 使nginx重新加载配置 nginx -s reload 在/usr/share/nginx/html下创建tao文件夹,在tao下创建html文件内容如下 Title Taoqz.xzy 使用浏览器访问 域名:82 便可访问到该主页,并且可以通过该域名访问其他端口 配置反向代理 # 进入主配置文件 vi /etc/nginx/nginx.conf 在http指令块中新添加一个server指令块 server { listen 80; # server_name gcp.taoqz.xyz; location / { # 配置代理的最终目标,当访问80端口时会自动跳转至b站 # proxy_pass http://www.bilibili.com; # 也可以通过该配置,访问本机中不同的服务 proxy_pass http://127.0.0.1:82; } # 也可以根据不同的请求路径转发到不同的端口 # 假设该路径为测试路径 location ~ /test { # proxy_pass http://127.0.0.1:8080; } # 假设该路径为生产路径 location ~ /dev { # proxy_pass http://127.0.0.1:8081; } } 也可使用upstream配置代理的方式,例如 # 该指令配置在http指令块中,同样支持配置多个 # myserver 为自定义 upstream myserver { # server为固定 server 192.168.xx.xxx:9091; } # 在具体的server指令块中添加,即可达到使用代理的方式配置虚拟主机 location / { proxy_pass http://tomcatServer1; } 其他指令 proxy_set_header：在将客户端请求发送给后端服务器之前，更改来自客户端的请求头信息； proxy_connect_timeout：配置 Nginx 与后端代理服务器尝试建立连接的超时时间； proxy_read_timeout：配置 Nginx 向后端服务器组发出 read 请求后，等待相应的超时时间； proxy_send_timeout：配置 Nginx 向后端服务器组发出 write 请求后，等待相应的超时时间； proxy_redirect：用于修改后端服务器返回的响应头中的 Location 和 Refresh。 跨域配置 什么是跨域? 跨域是跨域名的访问，当两个系统间域名不同，ip不同，子域名不同，协议不同，端口不同等都会引起跨域问题。 跨域产生的原因? 1.浏览器的同源策略，引起了跨域问题，当然不是所有的跨域都会引起跨域问题，比如从本地数据库请求数据不会引起跨域问题。 2.跨域问题是浏览器对ajax请求的一种安全限制，一个页面发起的ajax请求只能用于当前页面同域名的路径，避免跨站攻击。 3.如果域名和端口都相同，但是请求路径不同，不属于跨域。 解决跨域 1.可以使用反向代理的方式解决 2.配置header头 server { listen 80; server_name be.sherlocked93.club; add_header 'Access-Control-Allow-Origin'$http_origin; # 全局变量获得当前请求origin，带cookie的请求不支持* add_header 'Access-Control-Allow-Credentials''true'; # 为 true 可带上 cookie add_header 'Access-Control-Allow-Methods''GET, POST, OPTIONS'; # 允许请求方法 add_header 'Access-Control-Allow-Headers'$http_access_control_request_headers; # 允许请求的 header，可以为 * add_header 'Access-Control-Expose-Headers''Content-Length,Content-Range'; if ($request_method = 'OPTIONS') { add_header 'Access-Control-Max-Age' 1728000; # OPTIONS 请求的有效期，在有效期内不用发出另一条预检请求 add_header 'Content-Type''text/plain; charset=utf-8'; add_header 'Content-Length' 0; return 204; # 200 也可以 } location / { root /usr/share/nginx/html/be; index index.html; } } 配置gzip gzip压缩需要浏览器配合支持 在/etc/nginx/conf.d下创建文件gzip.conf,内容如下 gzip on; # 默认off，是否开启gzip gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; # 上面两个开启基本就能跑起了，下面的愿意折腾就了解一下 gzip_static on; gzip_proxied any; gzip_vary on; gzip_comp_level 6; gzip_buffers 16 8k; # 建议添加,文件低于1kb不会进行压缩,否则可能会导致压缩后的文件比原本的文件还要大 # gzip_min_length 1k; gzip_http_version 1.1; 相关配置解释 gzip_types：要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用； gzip_static：默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容； gzip_proxied：默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩； gzip_vary：用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩； gzip_comp_level：gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6； gzip_buffers：获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得； gzip_min_length：允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大； gzip_http_version：默认 1.1，启用 gzip 所需的 HTTP 最低版本； ​ 这个配置可以插入到 http 模块整个服务器的配置里，也可以插入到需要使用的虚拟主机的 server 或者下面的 location 模块中，当然像上面我们这样写的话就是被 include 到 http 模块中了。 负载均衡 负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。 负载均衡，英文名称为 Load Balance，其意思就是分摊到多个操作单元上进行执行，例如 Web 服务器、FTP 服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 在nginx中配置负载均衡 在/etc/nginx/conf.d下创建一个junheng.conf,内容如下 # 配置一个代理,指向本机两个不同的端口,用于测试负载均衡 # myserver 自定义的名称,最好不要加下划线 upstream myserver { # ip_hash; # ip_hash 方式 # fair; # fair 方式 server 127.0.0.1:8081; # 负载均衡目的服务地址 server 127.0.0.1:8082 weight=2; # weight 方式，不写默认为 1 } server { # 监听本机80端口,测试时访问该端口 listen 80; location / { # 此处的myserver对应上面 upstream后的名称,名称中最好不要加_ 下划线 proxy_pass http://myserver; proxy_connect_timeout 10; } } 再创建两个配置文件 my8081.conf server { # 配置监听端口 listen 8081; location / { # 网站的根目录 root /usr/share/nginx/html/my8081; # 网站的首页 index index.html; } } my8082.conf server { # 配置监听端口 listen 8082; location / { # 网站的根目录 root /usr/share/nginx/html/my8082; # 网站的首页 index index.html; } } 并在/usr/share/nginx/html下分别创建文件夹和对应的index.html,在html中写入对应配置的端口号,用于查看负载均衡的效果 nginx负载均衡策略 轮询，默认方式，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务挂了，能自动剔除； weight，权重分配，指定轮询几率，权重越高，在被访问的概率越大，用于后端服务器性能不均的情况； ip_hash，每个请求按访问 IP 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决动态网页 session 共享问题。负载均衡每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的； fair（第三方），按后端服务器的响应时间分配，响应时间短的优先分配，依赖第三方插件 nginx-upstream-fair，需要先安装； server 后缀配置解析 upstream：每个设备的状态: down：表示当前的 server 暂时不参与负载 weight：默认为 1 weight 越大，负载的权重就越大。 max_fails：允许请求失败的次数默认为 1 当超过最大次数时，返回 proxy_next_upstream 模块定义的错误 fail_timeout:max_fails 次失败后，暂停的时间。 backup：其它所有的非 backup 机器 down 或者忙的时候，请求 backup 机器。所以这台机器压力会最轻 总结:只是在upstream指令块中多添加了几个不同的服务器地址映射关系,然后根据nginx的策略达到负载均衡 适配PC和移动端 根据用户的不同设备返回不同样式的站点,大型网站在开发时可能会采用分开编写的方式,不做响应式,根据用户请求的user-agent来进行判断 在/etc/nginx/conf.d下创建一个 pcormo.conf文件,内容如下 server { listen 80; location / { root /usr/share/nginx/html/pc; # 判断请求头是否为移动端,如果为移动端,覆盖设置网站端点为mobile if ($http_user_agent ~* '(Android|webOS|iPhone|iPod|BlackBerry)') { root /usr/share/nginx/html/mobile; } index index.html; } } 在/usr/share/nginx/html文件夹下分别创建pc和mobile文件夹,并且在两个文件夹下分别创建两个index.html,其中内容有区别即可,使用浏览器进行访问,测试移动端时使用F12点击左上角的手机/平板小图标即可 配置HTTPS 在阿里云申请SSL证书并且审核通过后,进行如下配置 参考链接:https://yq.aliyun.com/articles/672835 查看所有端口使用情况 netstat -lntp 停止指定端口 kill 21307 将一下代码配置到nginx的配置文件中,其中key和pem是阿里云认证成功后提供的,根据服务器类型进行相应的下载并应用 server { # 服务器端口使用443，开启ssl, 这里ssl就是上面安装的ssl模块 listen 443 ssl; # 域名，多个以空格分开 server_name www.taoqz.xyz; # ssl证书地址 ssl_certificate /etc/nginx/https/3870913_www.taoqz.xyz.pem; # pem文件的路径 ssl_certificate_key /etc/nginx/https/3870913_www.taoqz.xyz.key; # key文件的路径 # ssl验证相关配置 ssl_session_timeout 5m; #缓存有效期 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #加密算法 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #安全链接可选的加密协议 ssl_prefer_server_ciphers on; #使用服务器端的首选算法 location / { root html; index index.html index.htm; } } # 监听80端口,并且将请求重定向至https server { listen 80; server_name www.taoqz.xyz; return 301 https://$server_name$request_uri; } 其他常用技巧 图片防盗链 server { listen 80; server_name *.taoqz.xyz; # 图片防盗链 location ~* \\.(gif|jpg|jpeg|png|bmp|swf)$ { valid_referers none blocked 192.168.0.2; # 只允许本机 IP 外链引用 if ($invalid_referer){ return 403; } } } 请求过滤 # 非指定请求全返回 403 if ( $request_method !~ ^(GET|POST|HEAD)$ ) { return 403; } location / { # IP访问限制（只允许IP是 192.168.0.2 机器访问） allow 192.168.0.2; deny all; root html; index index.html index.htm; } 配置图片、字体等静态文件缓存 由于图片、字体、音频、视频等静态文件在打包的时候通常会增加了 hash，所以缓存可以设置的长一点，先设置强制缓存，再设置协商缓存；如果存在没有 hash 值的静态文件，建议不设置强制缓存，仅通过协商缓存判断是否需要使用缓存 # 图片缓存时间设置 location ~ .*\\.(css|js|jpg|png|gif|swf|woff|woff2|eot|svg|ttf|otf|mp3|m4a|aac|txt)$ { expires 10d; } # 如果不希望缓存 expires -1; HTTP请求转发到HTTPS 配置好HTTPS但同时HTTP还是可以访问的,可以将其转发到HTTPS server { listen 80; server_name www.taoqz.xyz; # 单域名重定向 if ($host = 'www.sherlocked93.club'){ return 301 https://www.taoqz.xyz$request_uri; } # 全局非 https 协议时重定向 if ($scheme != 'https') { return 301 https://$server_name$request_uri; } # 或者全部重定向 return 301 https://$server_name$request_uri; # 以上配置选择自己需要的即可，不用全部加 } 遇到的问题 在初次使用时修改配置文件中的启动端口,遇到如下错误 bind() to 0.0.0.0:8090 failed (13: Permission denied) 解决方式 semanage port -l | grep http_port_t 从上面的输出中可以看到，使用强制模式的SELinux，http只允许绑定到列出的端口。解决方案是将要绑定的端口添加到列表中 semanage port -a -t http_port_t -p tcp 8090 # 如出现已定义的情况( Port tcp/8082 already defined)可将 -a 改为 -m semanage port -m -t http_port_t -p tcp 8082 参考链接 https://www.funtl.com/zh/apache-dubbo-codeing/%E4%BB%80%E4%B9%88%E6%98%AF-Nginx.html#%E6%9C%AC%E8%8A%82%E8%A7%86%E9%A2%91 https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486730&idx=1&sn=2031330f25c91be1b1bbb4b48aeba63e&chksm=cea242c1f9d5cbd7896d2f3ccdc474afcba389e1f469bda8e125ee5e9cac3d68588eeb675dd6&scene=126&sessionid=1588727246&key=cbc079077f0681e356fcb12704c163affecc46215be17bc7d4581215976af2708a4c88e0f2e59bdf3c5a9e2a99440c66f3ad70a2d7205244c3a785614b6712a875d8a18f0d8a2c92508874e478783aa0&ascene=1&uin=OTc3ODM2OTA4&devicetype=Windows+10&version=62080079&lang=zh_CN&exportkey=A4dNeu1H5t2edCZrAPdB1%2Fk%3D&pass_ticket=m8SSL7pQd0nGH068CX8BK%2BkrUZqjZkUrcsA%2BkVD2CcJXaVClTYXs9QOfLUweFFyP Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"nginx/https配置.html":{"url":"nginx/https配置.html","title":"https配置","keywords":"","body":"server { # 服务器端口使用443，开启ssl, 这里ssl就是上面安装的ssl模块 listen 443 ssl; # 域名，多个以空格分开 server_name www.taoqz.xyz; # ssl证书地址 ssl_certificate /etc/nginx/https/3870913_www.taoqz.xyz.pem; # pem文件的路径 ssl_certificate_key /etc/nginx/https/3870913_www.taoqz.xyz.key; # key文件的路径 # ssl验证相关配置 ssl_session_timeout 5m; #缓存有效期 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #加密算法 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #安全链接可选的加密协议 ssl_prefer_server_ciphers on; #使用服务器端的首选算法 location / { root html; index index.html index.htm; } } server { listen 80; server_name www.taoqz.xyz; return 301 https://$server_name$request_uri; } 生成认证文件 # 生成服务器端私钥 openssl genrsa -out server.key 2048 # 生成服务器端公钥 openssl rsa -in server.key -pubout -out server.pem # 生成客户端私钥 openssl genrsa -out client.key 2048 # 生成客户端公钥 openssl rsa -in client.key -pubout -out client.pem # 生成 CA 私钥 openssl genrsa -out ca.key 2048 # X.509 Certificate Signing Request (CSR) Management. openssl req -new -key ca.key -out ca.csr # X.509 Certificate Data Management. openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt # 服务器端需要向 CA 机构申请签名证书，在申请签名证书之前依然是创建自己的 CSR 文件 openssl req -new -key server.key -out server.csr # 向自己的 CA 机构申请证书，签名过程需要 CA 的证书和私钥参与，最终颁发一个带有 CA 签名的证书 openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial -in server.csr -out server.crt # client 端 openssl req -new -key client.key -out client.csr # client 端到 CA 签名 openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial -in client.csr -out client.crt # ssl证书地址 ssl_certificate /etc/nginx/conf.d/server.crt; # pem文件的路径 ssl_certificate_key /etc/nginx/conf.d/server.key; # key文件的路径 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"nginx/docker-nginx.html":{"url":"nginx/docker-nginx.html","title":"docker-nginx","keywords":"","body":"nginx.conf user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; server { listen 80; listen [::]:80; server_name localhost; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } } docker run docker run -p 80:80 -p 443:443 --name nginx --network host -v /home/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /home/nginx/conf/conf.d:/etc/nginx/conf.d -v /home/nginx/log:/var/log/nginx -v /home/nginx/html:/usr/share/nginx/html -d nginx 或者先起容器，把容器里的配置文件复制到宿主机上 # 将容器nginx.conf文件复制到宿主机 docker cp nginx:/etc/nginx/nginx.conf /usr/local/tao/nginx/conf/nginx.conf # 将容器conf.d文件夹下内容复制到宿主机 docker cp nginx:/etc/nginx/conf.d /usr/local/tao/nginx/conf/conf.d # 将容器中的html文件夹复制到宿主机 docker cp nginx:/usr/share/nginx/html /usr/local/tao/nginx/html Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mongodb/Mongo的安装与卸载.html":{"url":"mongodb/Mongo的安装与卸载.html","title":"Mongo的安装与卸载","keywords":"","body":"卸载:停止对应服务:卸载服务:卸载应用:安装:步骤:配置环境变量:卸载: 停止对应服务: ​ 使用管理员命令窗口 win+x+a ​ net stop 服务名 卸载服务: ​ mongod.exe --remove --serviceName \"mongo的服务名\" 卸载应用: ​ 在系统应用中进行卸载 安装: 步骤: 1.下载对应安装包 mongodb中文官网:https://www.mongodb.org.cn/tutorial/55.html 2.点击安装,选择自定义配置 3.选择安装目录 4.进行配置 配置环境变量: ​ 配置环境变量的目的:为了在电脑中任意位置都可以使用mongo(其他应用程序也是如此) ​ 系统变量和用户变量:系统变量针对当前系统有效,而用户变量只对当前用户有效 ​ 步骤: 此电脑 --> 右键 --> 属性 高级系统设置 --> 环境变量 系统变量 --> 新建 在系统变量Path的变量值中添加%mongo%\\bin; 指向mongo的bin目录 测试启动 打开命令行cmd 默认端口 27017 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"mongodb/Mongo介绍及命令行使用.html":{"url":"mongodb/Mongo介绍及命令行使用.html","title":"Mongo的介绍及命令行使用","keywords":"","body":"1. 简介2. Mongdb与MySQL的区别2.1 结构2.2 数据及存储方式2.3 优缺点2.4 使用场景3.数据类型3.1 数值类型3.2 数组类型3.3 日期类型3.4 ObjectId3.5 typeof 和$type3.6 null4.命令行启动4.1 启动服务4.2 运行mongodb4.3 基本操作5.使用5.1 基本概念5.2 添加insertOne(对象)insertMany(数组)insert(文档对象或数组)savesave和insert的区别5.3 删除deteleOnedeleteManyremove5.4 查询findfindOne5.5 更新updateOneupdateManyupdate结合参数使用6.常用操作符$set$unset$inc$exists$in$or$and$sort$skip $limit7.操作数组8.索引8.1.索引的创建及删除8.2.执行计划9. 整合springboot简单增删改查9.1 项目环境9.2 pom依赖及配置9.3 代码1. 简介 ​ MongoDB是一个基于分布式文件存储的数据库,是非关系型数据库(NoSQL) ​ 分布式文件存储:在mongodb中也叫分片管理,将数据库中存储的数据分散在不同的机器上,减缓了数据库的压力,提高响应速度 2. Mongdb与MySQL的区别 2.1 结构 MySQL MongoDB 对应关系 database database 库 table collection 表;集合 row document 一行数据(记录);一个文档 column field 列;字段 2.2 数据及存储方式 SQL NoSQL 存储方式 存在特定结构表中 更加灵活和可扩展,简单说就是在集合中每个文档可以有不相同个数或类型 表/集合数据的关系 必须定义好表和字段结构后才能添加数据,虽然表结构定义后可更新,比较复杂 数据可以在任何时候任何地方添加，不需要先定义表 多表查询 可以使用JOIN表连接方式将多个关系数据表中的数据用一条简单的查询语句查询出来 大多非关系型数据库不支持(MongoDB 3.2后可以指定同一数据库中的集合以执行连接 ) 数据耦合性 不允许删除已经被使用的外部数据(外键),规范方式设置外键,也可手动维护 NoSQL中没有这种强耦合的概念，可以随时删除任何数据 2.3 优缺点 NoSQL SQL 优点 扩展简单,快速读写,成本低 是所有关系型数据库的通用语言(差别不大),移植性好,数据统计方便直观,事务处理,保持数据的一致性 缺点 不提供对SQL的支持,大部分非关系型数据不支持事务,现有 产品不够成熟(MongoDB4.0提供了事务的API) 扩展困难,读写慢(当数据量达到一定规模时),成本高(企业级的需要付费的) 2.4 使用场景 ​ 单从功能上讲,NoSQL几乎所有的功能,在关系型数据库上都能满足,所以选择NoSQL主要是和关系型数据库进 行结合使用,各取所长 ​ 比如说数据库中的表结构需要经常变化,增加字段,如果在一个百万级数据量的关系型数据库中新增字段会有很 多问题,而使用非关系型数据库极大提升扩展性,也可以作为缓存数据库,典型就是Redis 3.数据类型 ​ mongodb存储数据的格式为BSON,和JSON很像,是JSON的扩展 mongodb支持的所有的数据类型: Type Number Alias Notes Double 1 “double” String 2 “string” Object 3 “object” Array 4 “array” Binary data 5 “binData” Undefined 6 “undefined” Deprecated. ObjectId 7 “objectId” Boolean 8 “bool” Date 9 “date” Null 10 “null” Regular Expression 11 “regex” DBPointer 12 “dbPointer” Deprecated. JavaScript 13 “javascript” Symbol 14 “symbol” Deprecated. JavaScript (with scope) 15 “javascriptWithScope” 32-bit integer 16 “int” Timestamp 17 “timestamp” 64-bit integer 18 “long” Decimal128 19 “decimal” New in version 3.4. Min key -1 “minKey” Max key 127 “maxKey” ​ ​ JSON : 布尔、数字、字符串、数组和对象 ​ 没有日期类型,只有一种数字类型,无法区分浮点数和整数,也没法表示正则表达式或者函数 ​ BSON : 是一种类JSON的二进制形式的存储格式,有JSON没有的一些数据类型,如Date,BinData(二进制数据) 3.1 数值类型 ​ mongodb的数值类型默认使用64位浮点型数值,整型可以使用NumberInt(值)或者NumberLong (值) db.emp.insert({age:18}) db.emp.find({age:18}) // 如果有int类型会同时查询出来 { \"_id\" : ObjectId(\"5dca61c0f366526745ae2f13\"), \"age\" : 18.0 } ====================================== // NumberInt 插入 NumberLong 同理 db.emp.insert({age:NumberInt(18)}) // $type : 可以使用此命令查询对应的数据类型 // 可以使用上面表格中的Number 也可以使用 Alias db.emp.find({age:{$type:\"int\"}}) { \"_id\" : ObjectId(\"5dca62c9f366526745ae2f16\"), \"age\" : NumberInt(18) } 3.2 数组类型 ​ db.emp.insert({ _id:new ObjectId(), name:'taoqz', arr:[ '字符串', 100, new Date(\"2019-11-11\") ] }) db.emp.findOne({name:\"taoqz\"}) // 数组中支持不同的类型 { \"_id\" : ObjectId(\"5dca6971f366526745ae2f17\"), \"name\" : \"taoqz\", \"arr\" : [ \"字符串\", 100.0, ISODate(\"2019-11-11T00:00:00.000+0000\") ] } 3.3 日期类型 // 构建一个格林尼治时间 可以看到正好和我们的时间相差8小时，我们是+8时区，也就是时差相差8 new Date() new Date(\"1999-10-09T18:56:01\") // 真实时间是要比下面的时间大八个小时的 ISODate(\"2019-11-12T08:30:50.060Z\") // 此问题在Java中结合springboot时可以在对应的字段上添加下面注释 @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\",timezone = \"GMT+8\") 3.4 ObjectId ​ ObjectId使用12字节的存储空间，每个字节可以存储两个十六进制数字，所以一共可以存储24个十六进制数 字组成的字符串，在这24个字符串中，前8位表示时间戳，接下来6位是一个机器码，接下来4位表示进程id，最后 6位表示计数器 ​ 是文档的唯一标识,没有自增,如果不添加id会自动添加一个 \"_id\" : ObjectId(\"值\") db.emp.insert({_id:new ObjectId(),name:\"zz\"}) db.emp.findOne({name:\"zz\"}) { \"_id\" : ObjectId(\"5dca6eb4f366526745ae2f18\"), \"name\" : \"zz\" } 3.5 typeof 和$type ​ typeof 1 number typeof \"zz\" string ​ 每个数据类型对应一个数字，在MongoDB中可以使用$type操作符查看字段的数据类型 ​ $type:根据数据类型查找数据 {字段:{$type:数字}} {字段:{$type:\"数据类型\"}} 3.6 null ​ 用于表示空值或者不存在的字段 ,在查询的时候也可能会使用到 4.命令行启动 4.1 启动服务 ​ net start 服务名 (管理员cmd) 4.2 运行mongodb ​ mongo 4.3 基本操作 查看所有数据库 show dbs 查看所有集合 show tables | show collections 查看当前所在数据库 db 使用数据库 use db_name 删除数据库 db.dropDatabase() # 删除当前正在使用的数据库 使用集合 db.collection_name.find() # 查所有满足条件数据 db.collection_name.findOne() # 查满足条件的一条数据 db.collection_name.count() # 统计集合下面有多少数量的数据 删除集合 db.collection_name.drop() 创建数据库和集合 # 当你使用一个不存在的mongo数据库时,就自动创建了一个mongo数据库 # 同理当你往一个空集合里面插入了一条数据之后就自动创建了一个集合 查看当前数据库版本 db.version() 备份数据库 // 导出 如果没有设置账号密码 可以省略 mongodump --host IP --port 端口 -u 用户名 -p 密码 -d 数据库 -o 文件路径 // 导入 mongorestore --host --port -d 文件路径 5.使用 5.1 基本概念 ​ 数据库(database) ​ 集合(collection) ​ 文档(document) ​ 一个数据库由多个集合组成,一个集合中有许多文档,文档是最小的存储单位 5.2 添加 insertOne(对象) // 添加一条数据 db.emp.insertOne({name:\"张三\"}) // 返回的数据,也可以定义变量接收 { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"5dca9ef4f366526745ae2f1a\") } insertMany(数组) // 添加多条数据 db.emp.insertMany([{name:\"李四\"},{name:\"王五\"}]) { \"acknowledged\" : true, \"insertedIds\" : [ ObjectId(\"5dca9f91f366526745ae2f1c\"), ObjectId(\"5dca9f91f366526745ae2f1d\") ] } insert(文档对象或数组) // 添加一条或者多条 db.emp.insert({name:\"赵六\"}) WriteResult({ \"nInserted\" : 1 }) db.emp.insert([{name:\"田七\"},{name:\"吴八\"}]) BulkWriteResult({ \"writeErrors\" : [ ], \"writeConcernErrors\" : [ ], \"nInserted\" : 2, \"nUpserted\" : 0, \"nMatched\" : 0, \"nModified\" : 0, \"nRemoved\" : 0, \"upserted\" : [ ] }) // 查询所有 db.emp.find({}) { \"_id\" : ObjectId(\"5dca9f2bf366526745ae2f1b\"), \"name\" : \"张三\" } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dca9f91f366526745ae2f1c\"), \"name\" : \"李四\" } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dca9f91f366526745ae2f1d\"), \"name\" : \"王五\" } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dca9feef366526745ae2f1e\"), \"name\" : \"赵六\" } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dcaa017f366526745ae2f1f\"), \"name\" : \"田七\" } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dcaa017f366526745ae2f20\"), \"name\" : \"吴八\" } // 可以看到在插入时并没有指定id,确自动生成了一个_id // 在插入的时候如果已经存在了相同的id会报错 // 如果想要存在相同的id的时候不报错而是更新数据 请使用 save方法 ​ save // 当集合中没有此条数据,进行插入 db.emp.save({_id:\"1\",name:\"阿里\"}) WriteResult({ \"nMatched\" : 0, \"nUpserted\" : 1, \"nModified\" : 0, \"_id\" : \"1\" }) ----------------------------------------------------------------------------- // 有数据时会进行替换,而不是更新,更新会有专门针对字段更新的API db.emp.save({_id:\"1\",name:\"腾讯\",boss:\"马化腾\"}) // 一处匹配 一处修改 WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) ----------------------------------------------------------------------------- // 查询结果 { \"_id\" : \"1\", \"name\" : \"腾讯\", \"boss\" : \"马化腾\" } save和insert的区别 save和insert的区别 如果save没有指定id,直接插入 ​ 如果save指定了id,并且id已存在会替换已有的文档 5.3 删除 ​ // 插入两条数据 { \"_id\" : ObjectId(\"5dcaa5445feb59ece4310397\"), \"name\" : \"小王\", \"age\" : NumberInt(18) } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dcaa5445feb59ece4310398\"), \"name\" : \"小红\", \"age\" : NumberInt(18) } deteleOne // 参数为空文档会集合中第一条文档 db.student.deleteOne({}) { \"acknowledged\" : true, \"deletedCount\" : 1.0 } // 两条命令效果一样 // $eq:== $gt:> $gte:>= $lt: deleteMany // 删除多条数据 // 如果传入空文档 会删除整个集合中的数据 db.student.deleteMany({}) { \"acknowledged\" : true, \"deletedCount\" : 2.0 } // 使用该命令会删除所有的匹配项 db.student.deleteMany({age:18}) { \"acknowledged\" : true, \"deletedCount\" : 2.0 } remove // 指定删除 会删除所有匹配项 db.student.remove({name:\"小王\"}) db.student.remove({age:18}) 5.4 查询 find // 查询所有 // 直接使用集合名称 db.student.find() // 指定集合名称 db.getCollection(\"student\").find({}) // 根据id查询(有点特殊) db.student.find({_id:ObjectId(\"5e8c28f8b0851e3157b69d07\")}) // 其他字段可以直接查询 db.student.find({age:2}) findOne // 只查询到第一个匹配到的 db.student.findOne({age:{$eq:18}}) // 会查询到所有匹配的 db.student.find({age:{$eq:18}}) 5.5 更新 ​ 数据 { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } // ---------------------------------------------- { \"_id\" : 2.0, \"name\" : \"红楼梦\", \"publisher\" : \"曹雪芹\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } updateOne // 更新 db.books.updateOne({flag:1},{$set:{name:\"newName\"}}) { \"acknowledged\" : true, \"matchedCount\" : 1.0, \"modifiedCount\" : 1.0 } // 只更新了第一条 { \"_id\" : 1.0, \"name\" : \"newName\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } // ---------------------------------------------- { \"_id\" : 2.0, \"name\" : \"红楼梦\", \"publisher\" : \"曹雪芹\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } updateMany // 重新初始化数据后使用该命令会修改所有匹配项 db.books.updateMany({flag:1},{$set:{name:\"newName\"}}) update 和使用updateOne效果一样 结合参数使用 // 语法 db.collection.update( , , { upsert: , multi: , writeConcern: } ) query: 查询的条件 update: 可以配合操作符更新字段 upsert: 可选 当没有匹配项时,是否作为新的对象插入,默认false不插入 multi: 可选 默认false,只更新匹配的第一条记录,如果为true,将跟新所有匹配项 // 例如 可以弥补上面update时只更新一个匹配项 db.books.update({flag:1},{$set:{name:\"newName\"}},false,true) // 没有匹配项会添加 db.books.update({name:\"西游记\"},{name:\"西游记\"},true) { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } // ---------------------------------------------- { \"_id\" : 2.0, \"name\" : \"红楼梦\", \"publisher\" : \"曹雪芹\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } // ---------------------------------------------- { \"_id\" : ObjectId(\"5dcab070763517547f7d8d95\"), \"name\" : \"西游记\" } 6.常用操作符 $set ​ 在更新时可以一个或多个字段 ​ 如果更新的字段不存在会添加该字段 { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0 } // 更新flag和view字段 但flag字段之前不存在 进行了添加 db.books.update({_id:1},{$set:{flag:11,view:100}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 100.0, \"flag\" : 11.0 } $unset ​ 删除字段 { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 1.0 } // 将flag字段删除,其中flag的值可以随意写不用匹配 db.books.update({_id:1},{$unset:{flag:11}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0 } $inc ​ 自增或自减 { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0 } // 将view字段自增10 db.books.update({_id:1},{$inc:{view:+10}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 10.0 } $exists ​ 匹配字段是否存在 // 格式 {字段:{$exists:boolean}} // 如果为true 匹配到所有有该字段的文档,包括null值 // false 则匹配没有该字段的文档 db.books.find({flag:{$exists:true}}) $in ​ 相当于mysql中的in ​ 是否在对应的范围 // 会匹配到对应字段在对应范围中的所有文档 db.books.find({_id:{$in:[1,3]}}) $or ​ 或者 ​ 格式:{ $or: [ { }, { }, ... , { } ] } { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 100.0, \"flag\" : 11.0 } // ---------------------------------------------- { \"_id\" : 2.0, \"name\" : \"西游记\", \"publisher\" : \"吴承恩\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 0.0, \"flag\" : 0.0 } // ---------------------------------------------- { \"_id\" : 3.0, \"name\" : \"zz\" } // ---------------------------------------------- { \"_id\" : 4.0, \"name\" : \"qwe\", \"flag\" : null } // 查询名字水浒传或者view大于等于0或者拥有flag字段的文档 db.books.find({$or:[ {name:\"水浒传\"}, {view:{$gte:0}}, {flag:{$exists:true}} ]}) $and ​ 并且 ​ 格式:{ $and: [ { }, { } , ... , { } ] } { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 100.0, \"flag\" : 11.0 } // ---------------------------------------------- { \"_id\" : 2.0, \"name\" : \"西游记\", \"publisher\" : \"吴承恩\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 100.0 } // 查询view大于等于100 并且存在flag字段的文档 db.books.find({$and:[ {view:{$gte:100}}, {flag:{$exists:true}} ]}) { \"_id\" : 1.0, \"name\" : \"水浒传\", \"publisher\" : \"施耐庵\", \"tags\" : [ \"小说\", \"电视剧\" ], \"view\" : 100.0, \"flag\" : 11.0 } $sort ​ 排序 // 按照id 倒叙排序 // -1 从大到小 // 1 从小到大 db.books.find({}).sort({_id:-1}) $skip $limit ​ $skip 采用一个正整数，该整数指定要跳过的最大文档数。 ​ $limit 采用一个正整数，该整数指定要传递的最大文档数。 ​ 作用相当于mysql中的limit 两者可以结合使用 skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()。 7.操作数组 8.索引 索引可以极大的提高我们的查询效率,如果没有索引,MongoDB在读数据时必须扫描集合中的每个文件过滤并选取符合条件的记录。这种全局扫描在数据量大的情况下,效率是很慢的。索引是一种特殊的数据结构,存储在一个易于遍历读取数据的集合中,索引是对数据库表中的某一列或多列的值进行排序的一种结构。 8.1.索引的创建及删除 // 查看索引 db.user.getIndexes() // 结果集 默认会将ObjectId添加到索引 { \"v\" : 2.0, \"key\" : { \"_id\" : 1.0 }, \"name\" : \"_id_\", \"ns\" : \"testdb.user\" } // 创建索引 ({'字段名称':[1|-1]}) db.user.createIndex({'age':1}) { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 1.0, \"numIndexesAfter\" : 2.0, \"ok\" : 1.0 } // 创建联合索引 db.user.createIndex({'age':1,'username':-1}) // 再次查询 [ { \"v\" : 2.0, \"key\" : { \"_id\" : 1.0 }, \"name\" : \"_id_\", \"ns\" : \"testdb.user\" }, { \"v\" : 2.0, \"key\" : { \"age\" : 1.0, \"username\" : -1.0 }, \"name\" : \"age_1_username_-1\", \"ns\" : \"testdb.user\" } ] // 删除索引 需要根据查询所有索引时索引的name进行删除 db.user.dropIndex('age_1') // 删除联合索引 同样是根据索引的名称进行删除 db.user.dropIndex('age_1_username_-1') // 查看索引的大小 单位:字节 db.user.totalIndexSize() 8.2.执行计划 执行计划是分析查询语句性能的重要工具,同时可以查看我们的索引是否生效。 // 查看执行计划 db.user.find({_id:ObjectId(\"5e8c28f8b0851e3157b69d07\")}).explain() // 执行结果 { \"queryPlanner\" : { \"plannerVersion\" : 1.0, \"namespace\" : \"testdb.user\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"_id\" : { \"$eq\" : ObjectId(\"5e8c28f8b0851e3157b69d07\") } }, // 最佳执行计划 \"winningPlan\" : { // stage:查询方式 // 常见的有COLLSCAN/全盘扫描、IXSCAN/索引扫描、FETCH/根据索引去检索文档、SHARD_MERGE/合并分片结果、IDHACK/针对_id进行查询 \"stage\" : \"IDHACK\" }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { \"host\" : \"8105bc201a59\", \"port\" : 27017.0, \"version\" : \"4.0.3\", \"gitVersion\" : \"7ea530946fa7880364d88c8d8b6026bbc9ffa48c\" }, \"ok\" : 1.0 } // 测试查询没有使用索引的列 db.user.find({age:2}).explain() { \"queryPlanner\" : { \"plannerVersion\" : 1.0, \"namespace\" : \"testdb.user\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"age\" : { \"$eq\" : 2.0 } }, \"winningPlan\" : { // 使用的是全局扫描 \"stage\" : \"COLLSCAN\", \"filter\" : { \"age\" : { \"$eq\" : 2.0 } }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { \"host\" : \"8105bc201a59\", \"port\" : 27017.0, \"version\" : \"4.0.3\", \"gitVersion\" : \"7ea530946fa7880364d88c8d8b6026bbc9ffa48c\" }, \"ok\" : 1.0 } 9. 整合springboot简单增删改查 9.1 项目环境 ​ mongodb: 4.0.9 ​ java:1.8 ​ springboot:2.2.0 ​ idea : 2019 ​ 使用了lombok简化实体类的代码 9.2 pom依赖及配置 spring.data.mongodb.uri=mongodb://127.0.0.1:27017/testdb org.springframework.boot spring-boot-starter-parent 2.2.0.RELEASE org.springframework.boot spring-boot-starter-data-mongodb org.springframework.boot spring-boot-starter-web org.projectlombok lombok 1.18.8 9.3 代码 实体类 @Data // 指定集合名称 @Document(collection = \"students\") // @Document 也可省略集合名称,如果数据库中没有该集合,mongo会自动根据当前类名首字母小写进行创建集合,这也是mongo的特性 public class Student { // 表名这是ObjectId @Id private String id; // 将mongo中的字段名与pojo中的字段名进行映射,字段名称和mongo字段名一样时会自动进行映射,也就是不写该注解也可以 @Field(name = \"sid\") private Integer sid; private String name; @DateTimeFormat(pattern = \"yyyy-MM-dd HH:mm:dd\") @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:dd\",timezone = \"GMT+8\") private Date birthday; private String classId; private Classes classes; private String hobbies[]; // @Transient 如果pojo中有mongo不存在的字段,在映射时进行排除 // @Indexed 将某个字段设置为索引 } controller @RestController @CrossOrigin(\"*\") @RequestMapping(\"/student\") public class StudentController { @Autowired private StudentDao studentDao; /** * 查询所有 * @return */ @GetMapping public List fun() { return studentDao.findAll(); } /** * 添加 * @param student */ @PostMapping public void add(@RequestBody Student student) { studentDao.save(student); } /** * 修改 * @param student */ @PutMapping public void edit(@RequestBody Student student) { studentDao.update(student); } /** * 根据字段及值 进行删除 * @param filedName * @param value */ @DeleteMapping public void deleteByFiledName(@RequestParam(name = \"filedName\") String filedName, String value) { studentDao.deleteByFiledName(filedName, value); } } dao @Repository public class StudentDao { @Autowired private MongoTemplate mongoTemplate; /** * 分页查询 * @param pageNum 页数 从0开始所以需要-1 * @param pageSize 页面大小 * @return */ public List findByPage(Integer pageNum,Integer pageSize){ PageRequest of = PageRequest.of(pageNum - 1, 10); Query with = new Query().with(of); return mongoTemplate.find(with,Student.class); } /** * 查询所有 * @return */ public List findAll(){ List all = mongoTemplate.findAll(Student.class); for (Student student : all) { if (student.getClassId() != null){ // 拿到每个学生对应的班级id 创建ObjectId对象 ObjectId objectId = new ObjectId(student.getClassId()); // 根据班级id查询班级 Classes one = mongoTemplate.findOne(new Query(Criteria.where(\"id\").is(objectId)), Classes.class); // 将班级添加到学生对象中 student.setClasses(one); } } return all; } /** * 添加 * @param student */ public void save(Student student) { // 可以在笔记中看到两者的区别 mongoTemplate.insert(student); // mongoTemplate.save(student); } /** * 更新 * @param student */ public void update(Student student) { // 查询条件 根据id Query id = Query.query(Criteria.where(\"_id\").is(student.getId())); // 创建文档对象 Document document = new Document(); // 向文档中追加数据 更新哪个字段添加哪个字段 document.append(\"name\",student.getName()).append(\"birthday\",student.getBirthday()) .append(\"hobbies\",student.getHobbies()); // 根据id 根据文档对象进行更新 // 因为是根据ObjectId进行更新所以唯一 使用updateFirst 只更新第一条 mongoTemplate.updateFirst(id, Update.fromDocument(document),Student.class); // 更新所有能匹配的文档 // mongoTemplate.updateMulti(); } /** * 根据字段名称删除文档 * @param filedName 字段名称 * @param value 字段对应的值 */ public void deleteByFiledName(String filedName,String value){ Query query = Query.query(Criteria.where(filedName).is(value)); mongoTemplate.remove(query, Student.class); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"aliyun/阿里云域名.html":{"url":"aliyun/阿里云域名.html","title":"阿里云域名","keywords":"","body":"阿里云购买域名注册并登陆阿里云挑选并购买域名进入控制台备案解析域名阿里云购买域名 ​ 想要搭建一个网站肯定少不了一个域名,分享一下自己在阿里云购买域名的经验 ​ 还有一个提供免费域名的网站,我没有购买成功,每次购买都会有未知错误,所以就没有后续了,哈哈 [ https://www.freenom.com/zh/index.html?lang=zh]: 注册并登陆阿里云 ​ 可以选择直接使用支付宝登陆 挑选并购买域名 进入控制台备案 ​ 购买成功后点击右上角进入控制台,在顶部搜索栏输入域名进入域名控制台. ​ 现在还不能使用我们的域名,需要我们进行备案,添加域名模板 ​ 注意事项: ​ 身份证照片:会控制大小,55k-1MB左右需要注意一下.如果手机像素太高,可以拍完后使用QQ截图 ​ 地址信息:最好和你的身份证上一致 ​ 邮箱:可能会让你验证一下,记不太清楚了 ​ 备案验证成功后进入左侧列表中的域名列表,点击解析 解析域名 ​ 解析域名就是将你的域名指向网站IP,方便记忆 ​ 进入解析设置页面后点击添加记录,其中每个选项都有解释,添加记录的旁边还有新手引导 ​ 更详细的解释有阿里提供的文档:https://help.aliyun.com/knowledge_detail/29725.html ​ 10分钟左右就可以通过域名访问你的网站了 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"aliyun/OSS存储.html":{"url":"aliyun/OSS存储.html","title":"OSS存储","keywords":"","body":"阿里云OSS基本概念存储空间(bucket)访问域名(endpoint)访问秘钥(AccessKey)结合springboot使用pom依赖配置文件具体代码添加水印访问原图保护阿里云OSS 登录阿里云搜索对象存储 OSS进行开通 基本概念 文档: https://help.aliyun.com/document_detail/31827.html?spm=a2c4g.11186623.2.7.549b58d5qcqqsh#concept-izx-fmt-tdb 所需配置(概念)所在位置 首先需要进入对象存储OSS的控制台 存储空间(bucket) 注意下方还有一个读写权限,选择为公共读 访问域名(endpoint) 点击进入指定的bucket 访问秘钥(AccessKey) 结合springboot使用 jdk 1.8 springboot 2.1.0 pom依赖 com.aliyun.oss aliyun-sdk-oss 2.8.3 org.projectlombok lombok 1.18.4 joda-time joda-time 2.9.9 org.apache.commons commons-lang3 3.7 配置文件 创建alioss.properties文件,文件名可自定义 # 填写在基本概念中找到的值 aliyun.endpoint= aliyun.accessKeyId= aliyun.accessKeySecret= aliyun.bucketName= # 访问前缀,可以在具体的上传的文件中找到url aliyun.urlPrefix= # 测试用 #aliyun.param= 如果上传的文件过大,在主配置文件中添加 spring.servlet.multipart.max-file-size=10MB spring.servlet.multipart.max-request-size=20MB 具体代码 config // lombok 简化实体类操作 @Data // 指定该类是一个配置类 @Configuration // 指定配置文件 @PropertySource(\"classpath:alioss.properties\") // 指定配置参数的前缀 @ConfigurationProperties(prefix = \"aliyun\") public class AliOSSConfig { private String endpoint; private String accessKeyId; private String accessKeySecret; private String bucketName; private String urlPrefix; // 另一种方式从配置文件中读取参数 @Value(\"${aliyun.param}\") private String param; // 将OSSClient对象交由spring管理 @Bean public OSSClient ossClient(){ return new OSSClient(endpoint,accessKeyId,accessKeySecret); } } controller @RestController @RequestMapping(\"/pic/upload\") public class PicUploadController { @Autowired private PicUploadService picUploadService; @PostMapping public PicUploadResult upload(@RequestParam(\"file\") MultipartFile file){ return picUploadService.upload(file); } } service @Service public class PicUploadService { // 允许上传的格式 private static final String[] IMAGE_TYPE = new String[]{\".bmp\", \".jpg\", \".jpeg\", \".gif\", \".png\"}; @Autowired private OSS ossClient; @Autowired private AliOSSConfig aliOSSConfig; public PicUploadResult upload(MultipartFile uploadFile) { // 校验图片格式 boolean isLegal = false; for (String type : IMAGE_TYPE) { if (StringUtils.endsWithIgnoreCase(uploadFile.getOriginalFilename(), type)) { isLegal = true; break; } } // 封装Result对象，并且将文件的byte数组放置到result对象中 PicUploadResult fileUploadResult = new PicUploadResult(); if(!isLegal){ fileUploadResult.setStatus(\"error\"); return fileUploadResult; } // 文件新路径 String fileName = uploadFile.getOriginalFilename(); String filePath = getFilePath(fileName); // 上传到阿里云 try { ossClient.putObject(aliOSSConfig.getBucketName(), filePath, new ByteArrayInputStream(uploadFile.getBytes())); } catch (Exception e) { e.printStackTrace(); //上传失败 fileUploadResult.setStatus(\"error\"); return fileUploadResult; } fileUploadResult.setStatus(\"done\"); fileUploadResult.setName(this.aliOSSConfig.getUrlPrefix() + filePath); fileUploadResult.setUid(String.valueOf(System.currentTimeMillis())); return fileUploadResult; } // 用于生成文件夹及以时间戳命名的文件名 private String getFilePath(String sourceFileName) { DateTime dateTime = new DateTime(); return \"images/\" + dateTime.toString(\"yyyy\") + \"/\" + dateTime.toString(\"MM\") + \"/\" + dateTime.toString(\"dd\") + \"/\" + System.currentTimeMillis() + RandomUtils.nextInt(100, 9999) + \".\" + StringUtils.substringAfterLast(sourceFileName, \".\"); } } 将返回的数据进行封装 @Data public class PicUploadResult { // 文件唯一标识 private String uid; // 文件名 private String name; // 状态有：uploading done error removed private String status; // 服务端响应内容，如：'{\"status\": \"success\"}' private String response; } 添加水印 修改水印样式 访问 保存规则后之前的文件也会生效,只需在图片的路径后添加在访问设置时的自定义分隔符及水印规则名称 例如 原图保护 也就是在访问设置自定义分隔符的上方有一个原图保护,其功能为只有添加水印后才可访问,需要点击开启,选择*号应用于所有文件即可 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"swagger/SpringBoot整合Swagger2.html":{"url":"swagger/SpringBoot整合Swagger2.html","title":"Springboot整合Swagger2","keywords":"","body":"Swagger2依赖配置类示例代码其他主要注解Swagger2 ​ Swagger 是一个根据接口生成API文档,进行测试 ​ 这里主要是结合springboot方便测试使用 依赖 这里只列出springboot版本 以及swagger所需的依赖 org.springframework.boot spring-boot-starter-parent 2.2.0.RELEASE io.springfox springfox-swagger2 2.9.2 io.springfox springfox-swagger-ui 2.9.2 配置类 主要配置生成的API的title和开启Swagger接口文档 @Configuration //标记配置类 @EnableSwagger2 //开启在线接口文档 public class Swagger2Config { /** * 添加摘要信息(Docket) */ @Bean public Docket controllerApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(new ApiInfoBuilder() .title(\"接口测试\") .description(\"接口测试\") // .contact(new Contact(\"一只袜子\", null, null)) .version(\"版本号:1.0\") .build()) .select() .apis(RequestHandlerSelectors.basePackage(\"xyz.taoqz.controller\")) .paths(PathSelectors.any()) .build(); } } 示例代码 @Api(value = \"emp\") @RestController @CrossOrigin @RequestMapping(\"/emp\") public class EmpController { @Autowired private IEmpService iEmpService; @ApiOperation(value = \"根据id删除\",httpMethod = \"DELETE\") @DeleteMapping(\"/{id}\") public ResponseEntity deleteById(@PathVariable Integer id){ try { iEmpService.removeById(id); baseResult.setMsg(\"删除成功\"); baseResult.setStateCode(StateCode.SUCCESS); } catch (Exception e) { baseResult.setMsg(\"删除失败\"); baseResult.setStateCode(StateCode.FAIL); } return ResponseEntity.ok(baseResult); } } 生成后的api文档页面 访问接口: http://localhost:8080/swagger-ui.html 使用: 点击其中一个接口 Try it out 填写好参数后 Excute发送请求 推荐使用Idea中的插件RestServices,可以更加方便进行测试 其他主要注解 https://blog.csdn.net/xupeng874395012/article/details/68946676 https://blog.csdn.net/weixin_41846320/article/details/82970204 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"elementUI/ElementUI.html":{"url":"elementUI/ElementUI.html","title":"elementUI","keywords":"","body":"网站快速成型工具vue中使用安装与配置使用样例NavMenu导航菜单Table表格Pagination 分页Radio单选框Checkbox 多选框Tree 树形Tabs 标签页DatePicker 日期选择器Cascader级联选择器网站快速成型工具 ​ Element，一套为开发者、设计师和产品经理准备的基于 Vue 2.0 的桌面端组件库 vue中使用 安装与配置 // 安装 npm i element-ui // 配置 import ElementUI from 'element-ui'; import 'element-ui/lib/theme-chalk/index.css'; Vue.use(ElementUI); 使用样例 NavMenu导航菜单 ​ default-active=\"2\" : 默认打开菜单的index ​ 设置为根据路由显示: ​ :default-active=\"$route.path\" ​ 同时在相同位置添加 :router=\"true\" 首页 xxx Table表格 ​ 获取当前行数据 { {scope.row} } Pagination 分页 Radio单选框 ​ 需要设置v-model绑定变量，选中意味着变量的值为相应 Radio label属性的值，label可以是String、Number或Boolean 禁用 Checkbox 多选框 ​ 注意 :label绑定对应id ​ value此时对应的应该是一个数字类型的数组 Tree 树形 // 数据格式 对应数据的字段名 defaultProps: { children: '', label: '' } // 获取值 // 全选 this.$refs.tree.getCheckedKeys()+'' // 半选 也就是父级 // this.$refs.tree.getHalfCheckedKeys() Tabs 标签页 ​ 标签 ​ js export default { name: 'home', data(){ return{ activename:'/home', tabs:[] } }, methods:{ // 点击选项卡后触发的方法 handleClick(tab, event) { // 跳转到对应选项卡的path this.$router.push(tab.name) }, // 添加选项卡 addTab(routeName,routePath){ // 如果是首页 不添加直接返回 if (routeName == 'home'){ return } // 设置标记 用来标记选项卡数组中是否已存在要添加的 // 如果存在 则选中该选项卡 let flag = true; this.tabs.forEach(ele => { if (ele.title == routeName){ flag = false; this.activename = ele.path } }) // 添加选项卡 // title 使用路由的名称 // path 使用路由的path if (flag){ this.tabs.push({ title: routeName, path: routePath }) // 将新添加的选项卡激活 this.activename = routePath } }, // 移除选项卡 removeTab(targetName){ this.tabs.forEach((ele,index) => { if (ele.path == targetName){ this.tabs.splice(index,1) } }) // 如果选项卡组中还有其他选项卡,将最后一个激活 否则跳转首页 if (this.tabs.length != 0){ this.activename = this.tabs[this.tabs.length-1].path this.$router.push(this.tabs[this.tabs.length-1].path) }else { this.$router.push('/home') } } }, // 监听路由 发生变化时添加选项卡 watch:{ '$route'(to,from){ this.addTab(to.name,to.path) } }, // 页面激活添加选项卡 mounted() { this.addTab(this.$route.name,this.$route.path) } } DatePicker 日期选择器 Cascader级联选择器 // 改变时触发的方法 data(){ return{ values:[], props:{ // 最后获取的是该value值 value:'', label:'', children:'' } } }, method:{ handleChange(value) { console.log(value); }, } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"templateng/Thymeleaf.html":{"url":"templateng/Thymeleaf.html","title":"Thymeleaf","keywords":"","body":"Thymeleaf所需依赖及配置与springboot整合热部署简单上手语法3.工具类推荐阅读Thymeleaf ​ Thymeleaf是一个模板引擎,可以完全代替JSP,解决了JSP在服务器未启动时不能预览的弊端,在后台给出真实数据时可以动态将预览数据替换,可以让前后端人员方便的进行预览和查看数据,也是SpringBoot的推荐模板引擎之一 ​ 所需依赖及配置 ​ 以下所有配置基于springboot ​ 开发环境 Spring Boot 2.1.6 Thymeleaf 3.0.11 Jdk 8 Windows 10 IDEA 2019 ​ pom文件所需依赖 org.springframework.boot spring-boot-starter-thymeleaf ​ 模板位置 默认的映射路径:src/main/resources/templates ​ properties #页面不加载缓存，修改即时生效 spring.thymeleaf.cache=false # 模版存放路径 spring.thymeleaf.prefix=classpath:/templates/ # 模版后缀 spring.thymeleaf.suffix=.html 与springboot整合热部署 ​ idea中所需配置 ​ 双击shift键搜索registry 将下图中第一个选项点勾 setting中配置下图 设置完毕后thymeleaf模板引擎的热部署就完成了 使用: ​ 页面修改后在Java文件中使用快捷键Shift+Ctrl+F9进行重新编译Recompile,这样只会更改页面中的内容,Java文件如果修改则不会改变 ​ 解决: ​ 在pom文件中引入 org.springframework.boot spring-boot-devtools org.springframework.boot spring-boot-maven-plugin true ​ properties: #开启热部署 spring.devtools.restart.enabled=true #也可以单独设置重启目录 不用手动 有弊端修改页面仍然需要重新编译 #spring.devtools.restart.additional-paths=src/main/java 设置完成后springboot+Thymeleaf的热部署就真正完成了 简单上手 index.html Title 2333 controller @Controller public class HelloController { @RequestMapping(\"/hello\") public String fun(Model model){ model.addAttribute(\"hello\",\"hello thymeleaf\"); return \"index\"; } } 如果直接打开显示的是其初始值2333 运行服务器打开后的网页显示的是后台传来的实体数据 语法 ​ 完成上面的操作也是方便我们进行测试 1.引入提示 ​ 在html页面引入命名空间 2.常用标签 2.1.URL表达式 2.1.1. th:href th:src 引入css和js 2.1.2. th:fragment th:replace 引入重复代码 header.html 为标签起个名字 2.1.3. th:href 超链接 可以传值也可以直接给值 百度 百度 带参数 taoqz 会被解释成:/controller?name=zz&age=12 2.2.变量表达式 2.2.1. th:text th:utext 获取单个值 服务器启动后,如果没有此名称的数据,显示为空白 如果是引用数据类型,本身为null也会显示空白,如果是null访问其属性会报错 服务器启动后,如果有值会被替换 上面是原样输出,下面可以输出后台传来的html,并且识别样式 model.addAttribute(\"html\",\"user\"); 2.2.2. th:each 循环遍历之list NAME AGE 2.2.3. th:each 循环遍历之map 序号 KEY value.name value.age userStatus:遍历状态 2.2.4.th:if th:unless 条件判断 = 18}\"> 成年 = 18}\"> 未成年 2.2.5. th:switch th:case 18岁 19岁 其他 2.2.5. th:block ​ 定义一个代码块,并且可以执行里面的属性 2.3.选择或星号表达式 Name: Sebastian. Surname: Pepper. Nationality: Saturn. //等价于 Name: Sebastian. Surname: Pepper. Nationality: Saturn. 2.4. 三元运算符 if-then :(如果）？（然后） 样例 if-then-else :(如果）？（那么）:(否则） 样例一 默认:(值）？:(默认值） 20 3.工具类 #dates: java.util的实用方法。对象:日期格式、组件提取等. #calendars: 类似于#日期,但对于java.util。日历对象 #numbers: 格式化数字对象的实用方法。 #strings:字符串对象的实用方法:包含startsWith,将/附加等。 #objects: 实用方法的对象。 #bools: 布尔评价的实用方法。 #arrays: 数组的实用方法。 #lists: list集合。 #sets:set集合。 #maps: map集合。 #aggregates: 实用程序方法用于创建聚集在数组或集合. #messages: 实用程序方法获取外部信息内部变量表达式,以同样的方式,因为他们将获得使用# {…}语法 #ids: 实用程序方法来处理可能重复的id属性(例如,由于迭代)。 #httpServletRequest:用于web应用中获取request请求的参数 #session:用于web应用中获取session的参数 Dates #dates : utility methods for java.util.Date objects: /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Dates * ====================================================================== */ /* * Null-safe toString() */ ${#strings.toString(obj)} // also array*, list* and set* /* * Format date with the standard locale format * Also works with arrays, lists or sets */ ${#dates.format(date)} ${#dates.arrayFormat(datesArray)} ${#dates.listFormat(datesList)} ${#dates.setFormat(datesSet)} /* * Format date with the specified pattern * Also works with arrays, lists or sets */ ${#dates.format(date, 'dd/MMM/yyyy HH:mm')} ${#dates.arrayFormat(datesArray, 'dd/MMM/yyyy HH:mm')} ${#dates.listFormat(datesList, 'dd/MMM/yyyy HH:mm')} ${#dates.setFormat(datesSet, 'dd/MMM/yyyy HH:mm')} /* * Obtain date properties * Also works with arrays, lists or sets */ ${#dates.day(date)} // also arrayDay(...), listDay(...), etc. ${#dates.month(date)} // also arrayMonth(...), listMonth(...), etc. ${#dates.monthName(date)} // also arrayMonthName(...), listMonthName(...), etc. ${#dates.monthNameShort(date)} // also arrayMonthNameShort(...), listMonthNameShort(...), etc. ${#dates.year(date)} // also arrayYear(...), listYear(...), etc. ${#dates.dayOfWeek(date)} // also arrayDayOfWeek(...), listDayOfWeek(...), etc. ${#dates.dayOfWeekName(date)} // also arrayDayOfWeekName(...), listDayOfWeekName(...), etc. ${#dates.dayOfWeekNameShort(date)} // also arrayDayOfWeekNameShort(...), listDayOfWeekNameShort(...), etc. ${#dates.hour(date)} // also arrayHour(...), listHour(...), etc. ${#dates.minute(date)} // also arrayMinute(...), listMinute(...), etc. ${#dates.second(date)} // also arraySecond(...), listSecond(...), etc. ${#dates.millisecond(date)} // also arrayMillisecond(...), listMillisecond(...), etc. /* * Create date (java.util.Date) objects from its components */ ${#dates.create(year,month,day)} ${#dates.create(year,month,day,hour,minute)} ${#dates.create(year,month,day,hour,minute,second)} ${#dates.create(year,month,day,hour,minute,second,millisecond)} /* * Create a date (java.util.Date) object for the current date and time */ ${#dates.createNow()} /* * Create a date (java.util.Date) object for the current date (time set to 00:00) */ ${#dates.createToday()} Calendars #calendars : analogous to #dates, but for java.util.Calendar objects: /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Calendars * ====================================================================== */ /* * Format calendar with the standard locale format * Also works with arrays, lists or sets */ ${#calendars.format(cal)} ${#calendars.arrayFormat(calArray)} ${#calendars.listFormat(calList)} ${#calendars.setFormat(calSet)} /* * Format calendar with the specified pattern * Also works with arrays, lists or sets */ ${#calendars.format(cal, 'dd/MMM/yyyy HH:mm')} ${#calendars.arrayFormat(calArray, 'dd/MMM/yyyy HH:mm')} ${#calendars.listFormat(calList, 'dd/MMM/yyyy HH:mm')} ${#calendars.setFormat(calSet, 'dd/MMM/yyyy HH:mm')} /* * Obtain calendar properties * Also works with arrays, lists or sets */ ${#calendars.day(date)} // also arrayDay(...), listDay(...), etc. ${#calendars.month(date)} // also arrayMonth(...), listMonth(...), etc. ${#calendars.monthName(date)} // also arrayMonthName(...), listMonthName(...), etc. ${#calendars.monthNameShort(date)} // also arrayMonthNameShort(...), listMonthNameShort(...), etc. ${#calendars.year(date)} // also arrayYear(...), listYear(...), etc. ${#calendars.dayOfWeek(date)} // also arrayDayOfWeek(...), listDayOfWeek(...), etc. ${#calendars.dayOfWeekName(date)} // also arrayDayOfWeekName(...), listDayOfWeekName(...), etc. ${#calendars.dayOfWeekNameShort(date)} // also arrayDayOfWeekNameShort(...), listDayOfWeekNameShort(...), etc. ${#calendars.hour(date)} // also arrayHour(...), listHour(...), etc. ${#calendars.minute(date)} // also arrayMinute(...), listMinute(...), etc. ${#calendars.second(date)} // also arraySecond(...), listSecond(...), etc. ${#calendars.millisecond(date)} // also arrayMillisecond(...), listMillisecond(...), etc. /* * Create calendar (java.util.Calendar) objects from its components */ ${#calendars.create(year,month,day)} ${#calendars.create(year,month,day,hour,minute)} ${#calendars.create(year,month,day,hour,minute,second)} ${#calendars.create(year,month,day,hour,minute,second,millisecond)} /* * Create a calendar (java.util.Calendar) object for the current date and time */ ${#calendars.createNow()} /* * Create a calendar (java.util.Calendar) object for the current date (time set to 00:00) */ ${#calendars.createToday()} Numbers #numbers : utility methods for number objects: /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Numbers * ====================================================================== */ /* * ========================== * Formatting integer numbers * ========================== */ /* * Set minimum integer digits. * Also works with arrays, lists or sets */ ${#numbers.formatInteger(num,3)} ${#numbers.arrayFormatInteger(numArray,3)} ${#numbers.listFormatInteger(numList,3)} ${#numbers.setFormatInteger(numSet,3)} /* * Set minimum integer digits and thousands separator: * 'POINT', 'COMMA', 'NONE' or 'DEFAULT' (by locale). * Also works with arrays, lists or sets */ ${#numbers.formatInteger(num,3,'POINT')} ${#numbers.arrayFormatInteger(numArray,3,'POINT')} ${#numbers.listFormatInteger(numList,3,'POINT')} ${#numbers.setFormatInteger(numSet,3,'POINT')} /* * ========================== * Formatting decimal numbers * ========================== */ /* * Set minimum integer digits and (exact) decimal digits. * Also works with arrays, lists or sets */ ${#numbers.formatDecimal(num,3,2)} ${#numbers.arrayFormatDecimal(numArray,3,2)} ${#numbers.listFormatDecimal(numList,3,2)} ${#numbers.setFormatDecimal(numSet,3,2)} /* * Set minimum integer digits and (exact) decimal digits, and also decimal separator. * Also works with arrays, lists or sets */ ${#numbers.formatDecimal(num,3,2,'COMMA')} ${#numbers.arrayFormatDecimal(numArray,3,2,'COMMA')} ${#numbers.listFormatDecimal(numList,3,2,'COMMA')} ${#numbers.setFormatDecimal(numSet,3,2,'COMMA')} /* * Set minimum integer digits and (exact) decimal digits, and also thousands and * decimal separator. * Also works with arrays, lists or sets */ ${#numbers.formatDecimal(num,3,'POINT',2,'COMMA')} ${#numbers.arrayFormatDecimal(numArray,3,'POINT',2,'COMMA')} ${#numbers.listFormatDecimal(numList,3,'POINT',2,'COMMA')} ${#numbers.setFormatDecimal(numSet,3,'POINT',2,'COMMA')} /* * ========================== * Utility methods * ========================== */ /* * Create a sequence (array) of integer numbers going * from x to y */ ${#numbers.sequence(from,to)} ${#numbers.sequence(from,to,step)} Strings #strings : utility methods for String objects: /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Strings * ====================================================================== */ /* * Check whether a String is empty (or null). Performs a trim() operation before check * Also works with arrays, lists or sets */ ${#strings.isEmpty(name)} ${#strings.arrayIsEmpty(nameArr)} ${#strings.listIsEmpty(nameList)} ${#strings.setIsEmpty(nameSet)} /* * Perform an 'isEmpty()' check on a string and return it if false, defaulting to * another specified string if true. * Also works with arrays, lists or sets */ ${#strings.defaultString(text,default)} ${#strings.arrayDefaultString(textArr,default)} ${#strings.listDefaultString(textList,default)} ${#strings.setDefaultString(textSet,default)} /* * Check whether a fragment is contained in a String * Also works with arrays, lists or sets */ ${#strings.contains(name,'ez')} // also array*, list* and set* ${#strings.containsIgnoreCase(name,'ez')} // also array*, list* and set* /* * Check whether a String starts or ends with a fragment * Also works with arrays, lists or sets */ ${#strings.startsWith(name,'Don')} // also array*, list* and set* ${#strings.endsWith(name,endingFragment)} // also array*, list* and set* /* * Substring-related operations * Also works with arrays, lists or sets */ ${#strings.indexOf(name,frag)} // also array*, list* and set* ${#strings.substring(name,3,5)} // also array*, list* and set* ${#strings.substringAfter(name,prefix)} // also array*, list* and set* ${#strings.substringBefore(name,suffix)} // also array*, list* and set* ${#strings.replace(name,'las','ler')} // also array*, list* and set* /* * Append and prepend * Also works with arrays, lists or sets */ ${#strings.prepend(str,prefix)} // also array*, list* and set* ${#strings.append(str,suffix)} // also array*, list* and set* /* * Change case * Also works with arrays, lists or sets */ ${#strings.toUpperCase(name)} // also array*, list* and set* ${#strings.toLowerCase(name)} // also array*, list* and set* /* * Split and join */ ${#strings.arrayJoin(namesArray,',')} ${#strings.listJoin(namesList,',')} ${#strings.setJoin(namesSet,',')} ${#strings.arraySplit(namesStr,',')} // returns String[] ${#strings.listSplit(namesStr,',')} // returns List ${#strings.setSplit(namesStr,',')} // returns Set /* * Trim * Also works with arrays, lists or sets */ ${#strings.trim(str)} // also array*, list* and set* /* * Compute length * Also works with arrays, lists or sets */ ${#strings.length(str)} // also array*, list* and set* /* * Abbreviate text making it have a maximum size of n. If text is bigger, it * will be clipped and finished in \"...\" * Also works with arrays, lists or sets */ ${#strings.abbreviate(str,10)} // also array*, list* and set* /* * Convert the first character to upper-case (and vice-versa) */ ${#strings.capitalize(str)} // also array*, list* and set* ${#strings.unCapitalize(str)} // also array*, list* and set* /* * Convert the first character of every word to upper-case */ ${#strings.capitalizeWords(str)} // also array*, list* and set* ${#strings.capitalizeWords(str,delimiters)} // also array*, list* and set* /* * Escape the string */ ${#strings.escapeXml(str)} // also array*, list* and set* ${#strings.escapeJava(str)} // also array*, list* and set* ${#strings.escapeJavaScript(str)} // also array*, list* and set* ${#strings.unescapeJava(str)} // also array*, list* and set* ${#strings.unescapeJavaScript(str)} // also array*, list* and set* /* * Null-safe comparison and concatenation */ ${#strings.equals(str)} ${#strings.equalsIgnoreCase(str)} ${#strings.concat(str)} ${#strings.concatReplaceNulls(str)} /* * Random */ ${#strings.randomAlphanumeric(count)} Objects #objects : utility methods for objects in general /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Objects * ====================================================================== */ /* * Return obj if it is not null, and default otherwise * Also works with arrays, lists or sets */ ${#objects.nullSafe(obj,default)} ${#objects.arrayNullSafe(objArray,default)} ${#objects.listNullSafe(objList,default)} ${#objects.setNullSafe(objSet,default)} Booleans #bools : utility methods for boolean evaluation /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Bools * ====================================================================== */ /* * Evaluate a condition in the same way that it would be evaluated in a th:if tag * (see conditional evaluation chapter afterwards). * Also works with arrays, lists or sets */ ${#bools.isTrue(obj)} ${#bools.arrayIsTrue(objArray)} ${#bools.listIsTrue(objList)} ${#bools.setIsTrue(objSet)} /* * Evaluate with negation * Also works with arrays, lists or sets */ ${#bools.isFalse(cond)} ${#bools.arrayIsFalse(condArray)} ${#bools.listIsFalse(condList)} ${#bools.setIsFalse(condSet)} /* * Evaluate and apply AND operator * Receive an array, a list or a set as parameter */ ${#bools.arrayAnd(condArray)} ${#bools.listAnd(condList)} ${#bools.setAnd(condSet)} /* * Evaluate and apply OR operator * Receive an array, a list or a set as parameter */ ${#bools.arrayOr(condArray)} ${#bools.listOr(condList)} ${#bools.setOr(condSet)} Arrays #arrays : utility methods for arrays /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Arrays * ====================================================================== */ /* * Converts to array, trying to infer array component class. * Note that if resulting array is empty, or if the elements * of the target object are not all of the same class, * this method will return Object[]. */ ${#arrays.toArray(object)} /* * Convert to arrays of the specified component class. */ ${#arrays.toStringArray(object)} ${#arrays.toIntegerArray(object)} ${#arrays.toLongArray(object)} ${#arrays.toDoubleArray(object)} ${#arrays.toFloatArray(object)} ${#arrays.toBooleanArray(object)} /* * Compute length */ ${#arrays.length(array)} /* * Check whether array is empty */ ${#arrays.isEmpty(array)} /* * Check if element or elements are contained in array */ ${#arrays.contains(array, element)} ${#arrays.containsAll(array, elements)} Lists #lists : utility methods for lists /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Lists * ====================================================================== */ /* * Converts to list */ ${#lists.toList(object)} /* * Compute size */ ${#lists.size(list)} /* * Check whether list is empty */ ${#lists.isEmpty(list)} /* * Check if element or elements are contained in list */ ${#lists.contains(list, element)} ${#lists.containsAll(list, elements)} /* * Sort a copy of the given list. The members of the list must implement * comparable or you must define a comparator. */ ${#lists.sort(list)} ${#lists.sort(list, comparator)} Sets #sets : utility methods for sets /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Sets * ====================================================================== */ /* * Converts to set */ ${#sets.toSet(object)} /* * Compute size */ ${#sets.size(set)} /* * Check whether set is empty */ ${#sets.isEmpty(set)} /* * Check if element or elements are contained in set */ ${#sets.contains(set, element)} ${#sets.containsAll(set, elements)} Maps #maps : utility methods for maps /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Maps * ====================================================================== */ /* * Compute size */ ${#maps.size(map)} /* * Check whether map is empty */ ${#maps.isEmpty(map)} /* * Check if key/s or value/s are contained in maps */ ${#maps.containsKey(map, key)} ${#maps.containsAllKeys(map, keys)} ${#maps.containsValue(map, value)} ${#maps.containsAllValues(map, value)} Aggregates #aggregates : utility methods for creating aggregates on arrays or collections /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Aggregates * ====================================================================== */ /* * Compute sum. Returns null if array or collection is empty */ ${#aggregates.sum(array)} ${#aggregates.sum(collection)} /* * Compute average. Returns null if array or collection is empty */ ${#aggregates.avg(array)} ${#aggregates.avg(collection)} Messages #messages : utility methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #{...} syntax. /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Messages * ====================================================================== */ /* * Obtain externalized messages. Can receive a single key, a key plus arguments, * or an array/list/set of keys (in which case it will return an array/list/set of * externalized messages). * If a message is not found, a default message (like '??msgKey??') is returned. */ ${#messages.msg('msgKey')} ${#messages.msg('msgKey', param1)} ${#messages.msg('msgKey', param1, param2)} ${#messages.msg('msgKey', param1, param2, param3)} ${#messages.msgWithParams('msgKey', new Object[] {param1, param2, param3, param4})} ${#messages.arrayMsg(messageKeyArray)} ${#messages.listMsg(messageKeyList)} ${#messages.setMsg(messageKeySet)} /* * Obtain externalized messages or null. Null is returned instead of a default * message if a message for the specified key is not found. */ ${#messages.msgOrNull('msgKey')} ${#messages.msgOrNull('msgKey', param1)} ${#messages.msgOrNull('msgKey', param1, param2)} ${#messages.msgOrNull('msgKey', param1, param2, param3)} ${#messages.msgOrNullWithParams('msgKey', new Object[] {param1, param2, param3, param4})} ${#messages.arrayMsgOrNull(messageKeyArray)} ${#messages.listMsgOrNull(messageKeyList)} ${#messages.setMsgOrNull(messageKeySet)} IDs #ids : utility methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). /* * ====================================================================== * See javadoc API for class org.thymeleaf.expression.Ids * ====================================================================== */ /* * Normally used in th:id attributes, for appending a counter to the id attribute value * so that it remains unique even when involved in an iteration process. */ ${#ids.seq('someId')} /* * Normally used in th:for attributes in tags, so that these labels can refer to Ids * generated by means if the #ids.seq(...) function. * * Depending on whether the goes before or after the element with the #ids.seq(...) * function, the \"next\" (label goes before \"seq\") or the \"prev\" function (label goes after * \"seq\") function should be called. */ ${#ids.next('someId')} ${#ids.prev('someId')} Request Test Session [...] 推荐阅读 https://juejin.im/post/5b8fc1fc5188255c34060d5d https://juejin.im/post/5c271fbde51d451b1c6ded58#heading-2 https://www.cnblogs.com/ityouknow/p/5833560.html https://www.jianshu.com/p/f79a98173677 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"docker/docker安装及常用命令.html":{"url":"docker/docker安装及常用命令.html","title":"docker安装及常用命令","keywords":"","body":"Docker安装及使用安装安装docker-compose常用命令Docker安装及使用 安装 # 更新数据源 apt-get update # 卸载旧版本 apt-get remove docker docker-engine docker.io containerd runc # 使用apt安装 apt install docker.io # 安装所需依赖 # apt-get -y install apt-transport-https ca-certificates curl software-properties-common # 安装 GPG 证书 # curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # 新增数据源 # add-apt-repository \"deb [arch=amd64] # http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" # 更新并安装 Docker CE # apt-get update && apt-get install -y docker-ce 验证安装是否成功 docker version 使用阿里云镜像 tee /etc/docker/daemon.json 安装docker-compose # 运行以下命令以下载Docker Compose的当前稳定版本： sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # 将可执行权限应用于二进制文件： sudo chmod +x /usr/local/bin/docker-compose # 如果命令docker-compose在安装后失败，请检查路径。您也可以创建指向/usr/bin或路径中任何其他目录的符号链接 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose # 测试安装。 $ docker-compose --version docker-compose version 1.25.5, build 1110ad01 常用命令 查看 Docker 版本 docker version 从 Docker 文件构建 Docker 映像 docker build -t image-name docker-file-location 运行 Docker 映像 docker run -d image-name 创建容器并运行 docker run -itd --name redis -p 6379:6379 redis i:交互式操作 t:终端 d:后台运行 p:将容器内部使用的网络端口映射到我们使用的主机上(前面是主机开放的端口,后面是docker容器开放的端口) --name 自定义的容器名称 最后的名称是镜像名称 使用run命令时直接进入容器 不能加d参数,但这种方式退出容器后也会停止容器 docker run -it 镜像 /bin/bash 查看可用的 Docker 映像 docker images 查看最近的运行容器 docker ps -l 查看所有正在运行的容器 docker ps -a 停止运行容器 docker stop container_id 停止正在运行的所有容器 docker stop $(docker ps -a -q) 运行一个停止的容器 docker start 容器name 删除一个镜像 docker rmi image-name 删除所有镜像 docker rmi $(docker images -q) 强制删除所有镜像 docker rmi -f $(docker images -q) 删除所有虚悬镜像 docker rmi $(docker images -q -f dangling=true) 删除所有容器 docker rm $(docker ps -a -q) 进入 Docker 容器 docker exec -it container-id /bin/bash 查看所有数据卷 docker volume ls 删除指定数据卷 docker volume rm [volume_name] 删除所有未关联的数据卷 docker volume rm $(docker volume ls -qf dangling=true) 从主机复制文件到容器 docker cp host_path containerID:container_path 从容器复制文件到主机 docker cp containerID:container_path host_path Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"docker/IDEA连接docker.html":{"url":"docker/IDEA连接docker.html","title":"IDEA连接docker","keywords":"","body":"IDEA配置docker1.配置Docker的远程访问2.替换内容3.重启Docker服务4.查看2357端口监听情况5.测试6.IDEA配置Docker7.参考链接IDEA配置docker 1.配置Docker的远程访问 vim /lib/systemd/system/docker.service 2.替换内容 # ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock # 将改行替换为如下 ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 3.重启Docker服务 systemctl daemon-reload systemctl restart docker 4.查看2357端口监听情况 netstat -nlpt 5.测试 浏览器访问 # 在服务器上访问 curl http://127.0.0.1:2375/version # 或者使用本地浏览器窗口访问访问 6.IDEA配置Docker 双击shift键,搜索plugins搜索Docker插件并安装 在Settings ---> Docker ---> Registry ---> Address中配置阿里云中国区加速镜像 在Settings ---> Docker ---> TCP socket ---> Engine API URL 配置docker地址 tcp://服务器IP:2375 显示Connection successful后表示连接成功!!! 7.参考链接 http://xuewei.world:8000/2020/02/13/idea%e9%9b%86%e6%88%90docker%e7%9a%84%e5%bf%ab%e6%8d%b7%e9%83%a8%e7%bd%b2/ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"docker/部署springboot项目.html":{"url":"docker/部署springboot项目.html","title":"部署springboot项目","keywords":"","body":"docker部署SpringBoot项目pom配置运行命令docker部署SpringBoot项目 创建好一个springboot项目后在本地运行测试没有问题后继续 pom配置 需要在项目的pom.xml文件中添加 jar org.springframework.boot spring-boot-maven-plugin com.spotify docker-maven-plugin 1.0.0 http://39.107.xxx.x:2375 ${project.artifactId} latest java weixinliang /ROOT [\"java\",\"-version\"]--> [\"java\",\"-jar\",\"${project.build.finalName}.jar\"]--> [\"java\" , \"-Djava.security.egd-file:/dev/./urandom\" , \"-jar\" ,\"${project.build.finalName}.jar\"] src/main/docker --> /ROOT ${project.build.directory} ${project.build.finalName}.jar 运行命令 mvn clean package docker:build 可以在IDEA中配置docker指定,先创建指定容器后运行 参考链接:https://www.cnblogs.com/softidea/p/8044031.html Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"docker/problem.html":{"url":"docker/problem.html","title":"遇到的问题","keywords":"","body":"删除镜像 当有多个镜像名称相同时进行删除会出现以下错误 Error response from daemon: conflict: unable to delete c50e9004f4ce (must be forced) - image is referenced in multi ple repositories 解决方案,使用其版本tag进行删除 docker rmi 镜像名称:1.0 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"docker/docker-compose.html":{"url":"docker/docker-compose.html","title":"docker-compose","keywords":"","body":"下载 curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose 加权限 sudo chmod +x /usr/local/bin/docker-compose 启动 会自动找文件名称为docker-compose.yml的文件 docker-compose up -d 停止 docker-compose down 部署MySQL version: '3.1' services: db: # 目前 latest 版本为 MySQL8.x image: mysql restart: always environment: MYSQL_ROOT_PASSWORD: 123456 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3306:3306 volumes: - ./data:/var/lib/mysql # MySQL 的 Web 客户端 adminer: image: adminer restart: always ports: - 8080:8080 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"harbor/harbor.html":{"url":"harbor/harbor.html","title":"Harbor","keywords":"","body":"Harbor使用指南下载harbor tar包 解压后生成如下文件harbor.ymldocker-compose/etc/docker/daemon.json服务构建完成后执行启动服务（容器）执行jenkins远程执行其他服务器上的shell脚本在A（jenkins）上执行命令：在B（服务）上执行命令：Harbor使用指南 下载harbor tar包 解压后生成如下文件 harbor.yml 其中 harbor.yml 是由 harbor.yml.tmpl 复制而来 harbor的主要配置在harbor.yml，根据需求修改即可（目前接触的配置 hostname 和 密码） 配置完成后先运行 ./prepare 运行完成后会生成docker-compose.yml文件 再运行 ./install install后会 下载harbor镜像至docker docker-compose 使用docker compose命令进行启停（在docker-compose.yml文件 目录下执行） # 启动 docker compose up -d # 停止 docker compose down 所有镜像下载并启动成功即可 默认账号密码 admin Harbor12345 /etc/docker/daemon.json 修改所有部署服务器上的docker配置 vim /etc/docker/daemon.json # 追加如下内容 harbor服务器地址 #\"insecure-registries\": [\"192.168.91.51:5000\"] 修改后要重启docker服务 systemctl restart docker 服务构建完成后执行 # 查看当前所在文件夹 pwd && # 根据当前目录下的Dockerfile打镜像 docker build --build-arg JAR_FILE=./target/hello-harbor-1.0.jar -t hello-harbor . && # 给这个镜像打tag docker tag hello-harbor 192.168.91.51:5000/harbor-cloud/hello-harbor:v1 && # 登录到harbor服务器 docker login -u admin -p Harbor12345 192.168.91.51:5000 && # 将镜像推送到harbor服务器 docker push 192.168.91.51:5000/harbor-cloud/hello-harbor:v1 && # 原来的镜像tag会变成none 这条命令是清楚所有tag为none的镜像 docker rmi $(docker images -f \"dangling=true\" -q) && # 目标服务器-1 ssh root@192.168.91.51 sh /home/project/hello-harbor/start.sh # 目标服务器-2 ssh root@192.168.91.52 sh /home/project/hello-harbor/start.sh 启动服务（容器）执行 目标服务器的 start.sh # stop container docker stop hello-harbor # rm container docker rm hello-harbor # rmi images docker rmi $(docker images | grep \"hello-harbor\" | awk '{print $3}') # login harbor docker login -u admin -p Harbor12345 192.168.91.51:5000 # pull image docker pull 192.168.91.51:5000/harbor-cloud/hello-harbor:v1 # run container docker run --name hello-harbor -p 9098:9098 -v /home/log:/logs -v /etc/localtime:/etc/localtime --net=host -d 192.168.91.51:5000/harbor-cloud/hello-harbor:v1 jenkins远程执行其他服务器上的shell脚本 A为jenkins所在机器（ip: 192.168.91.50） B为项目要部署的机器（ip：192.168.91.51） A，B都为CentOS 在A（jenkins）上执行命令： ssh-keygen -t rsa （连续三次回车，即在A上生成公钥和私钥，不设置密码） 确保B机器上有/root/.ssh 目录，没有的时候mkdir /root/.ssh 新建，而且.ssh目录的权限是700 把A上的/root/.ssh/id_rsa.pub文件发送到B的/root/.ssh目录下： scp id_rsa.pub root@192.168.91.51:/root/.ssh/id_rsa.pub 在B（服务）上执行命令： touch /root/.ssh/authorized_keys（如果已经存在这个文件就不用新建了） chmod 600 /root/.ssh/authorized_keys（必须将/root/.ssh/authorized_keys的权限改为600，该文件用于保存ssh客户端生成的公钥，可以修改服务器的ssh服务端配置文件/etc/ssh/sshd_config来指定其他文件名） cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys（蒋id_rsa.pub的内容追加到authorized_keys中，注意不要覆盖authorized_keys的原先内容（如果之前有的话）） 此时再在A上登录B: ssh root@192.168.91.51 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"dubbo/使用dubbo+zookeeper.html":{"url":"dubbo/使用dubbo+zookeeper.html","title":"入门使用","keywords":"","body":"环境准备安装docker卸载旧版本使用apt安装验证是否安装成功配置Docker镜像加速器验证是否成功使用docker拉取zookeeper镜像并运行案例父项目依赖服务提供者依赖配置启动类接口(负载均衡策略)服务消费者依赖配置调用接口(服务提供者)dubbo的管理页面注意事项环境准备 ​ linux ubuntu 18.04 ​ docker 19.03.6 ​ jdk 1.8 ​ maven 3.6.0 ​ springboot 2.1.0 ​ zookeeper 3.4.13 ​ dubbo 2.6.4 安装docker 卸载旧版本 apt-get remove docker docker-engine docker.io containerd runc 使用apt安装 # 更新数据源 apt-get update # 安装所需依赖 apt-get -y install apt-transport-https ca-certificates curl software-properties-common # 安装 GPG 证书 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # 新增数据源 add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" # 更新并安装 Docker CE apt-get update && apt-get install -y docker-ce 验证是否安装成功 docker version # 输出如下 Client: Version: 18.09.6 API version: 1.39 Go version: go1.10.8 Git commit: 481bc77 Built: Sat May 4 02:35:57 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.6 API version: 1.39 (minimum version 1.12) Go version: go1.10.8 Git commit: 481bc77 Built: Sat May 4 01:59:36 2019 OS/Arch: linux/amd64 Experimental: false 配置Docker镜像加速器 阿里云搜索容器镜像服务,在最低下有镜像加速器,复制加速器地址 通过修改 daemon 配置文件 /etc/docker/daemon.json 来使用加速器 tee /etc/docker/daemon.json 验证是否成功 docker info # 输出如下 Containers: 38 Running: 18 Paused: 0 Stopped: 20 Images: 10 Server Version: 18.09.6 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84 runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30 init version: fec3683 Security Options: apparmor seccomp Profile: default Kernel Version: 4.15.0-51-generic Operating System: Ubuntu 18.04.2 LTS OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 1.924GiB Name: kubernetes-master ID: PJ4H:7AF2:P5UT:6FMR:W4DI:SSWR:IQQR:J6QO:ARES:BOAC:ZVMO:SV2Y Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 ## 这里是你配置的镜像加速器 Registry Mirrors: https://xxxxxxxx.mirror.aliyuncs.com/ Live Restore Enabled: false Product License: Community Engine WARNING: No swap limit support 使用docker拉取zookeeper镜像并运行 #拉取zk镜像 docker pull zookeeper:3.5 #创建容器 docker create --name zk -p 2181:2181 zookeeper:3.5 #启动容器 docker start zk 案例 父项目依赖 org.springframework.boot spring-boot-starter-parent 2.1.0.RELEASE org.springframework.boot spring-boot-starter-test test com.alibaba.boot dubbo-spring-boot-starter 0.2.0 com.alibaba dubbo 2.6.4 org.apache.zookeeper zookeeper 3.4.13 com.github.sgroschupf zkclient 0.1 org.springframework.boot spring-boot-maven-plugin 服务提供者 依赖 org.springframework.boot spring-boot-starter --> org.apache.zookeeper--> zookeeper--> 3.4.13--> --> --> com.github.sgroschupf--> zkclient--> 0.1--> --> 配置 application.properties # Spring boot application spring.application.name = czxy-dubbo-service server.port = 9090 # Service version dubbo.service.version = 1.0.0 # 服务的扫描包 dubbo.scan.basePackages = xyz.taoqz.service # 应用名称 可以在dubbo ops 中按照此名称搜索 dubbo.application.name = dubbo-provider-demo # 协议以及端口 如果修改负载均衡策略 可以在此修改端口号测试 dubbo.protocol.name = dubbo # dubbo.protocol.port = 20881 dubbo.protocol.port = 20881 # zk注册中心 ip:port dubbo.registry.address = zookeeper://xxxxxx:2181 dubbo.registry.client = zkclient 启动类 主要演示非web的项目的启动类,也可使用web项目的方式 import org.springframework.boot.WebApplicationType; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.builder.SpringApplicationBuilder; @SpringBootApplication public class DubboProvider { public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } web方式 需将默认的依赖改为 org.springframework.boot spring-boot-starter-web @SpringBootApplication public class DubboProvider { public static void main(String[] args) { SpringApplication.run(DubboProvider.class,args); } } 接口(负载均衡策略) 接口正常写 需要注意的是实现类 import com.alibaba.dubbo.config.annotation.Service; // 声明这是一个dubbo服务 // 这里的version对应的是配置文件中的配置 @Service(version = \"${dubbo.service.version}\") public class UserServiceImpl implements UserService { @Override public String query() { // return \"service1\" return \"service2\"; } } 服务消费者 依赖 org.springframework.boot spring-boot-starter-web xyz.taoqz.dubbo czxy-dubbo-service 1.0-SNAPSHOT junit junit org.springframework.boot spring-boot-test 配置 application.properties # Spring boot application spring.application.name = czxy-dubbo-consumer server.port = 9091 # 应用名称 在dubbo ops中的服务提供者点击detail后consumer处 dubbo.application.name = dubbo-consumer-demo # zk注册中心 注册中心地址ip+port dubbo.registry.address = zookeeper://xxx:2181 dubbo.registry.client = zkclient 调用接口(服务提供者) import com.alibaba.dubbo.config.annotation.Reference; @RequestMapping(\"/user\") @RestController public class ConsumerController { // loadbalance为负载均衡策略,此处是轮询,默认时随机 @Reference(version = \"1.0.0\",loadbalance = \"roundrobin\") private UserService userService; @GetMapping public String fun(){ return userService.query(); } } dubbo的管理页面 github地址:https://github.com/apache/dubbo-admin 在服务器中下载并解压 #下载 git clone https://github.com/apache/dubbo-admin.git 配置及生产设置 # 依次执行即可 mvn clean package mvn --projects dubbo-admin-server spring-boot:run http://服务器ip:8080 启动后会自动监测本地的2181端口(zookeeper注册中心),如果不在本地或者端口号不同可以按照上图第二点进行修改 账号密码都为root 注意事项 1.实体类需要实现序列化接口Serializable Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"android/root.html":{"url":"android/root.html","title":"刷机","keywords":"","body":"查看当前连接设备 adb devices 重启设备到bootloader adb reboot bootloader 查看fastboot设备 fastboot devices 解锁 fastboot oem unlock 上锁 fastboot oem lock 刷入twrp 查看设备对应的twrp https://twrp.me/Devices/ fastboot flash recovery twrp.img 使手机进入到twrp界面 fastboot boot twrp.img 线刷 首先需要进入twrp,选择高级重启中的adb asideload 电脑adb命令行输入 adb sideload 刷机包 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"android/android发送请求.html":{"url":"android/android发送请求.html","title":"android发送请求","keywords":"","body":"GET public static String get(final String url) { final StringBuilder sb = new StringBuilder(); FutureTask task = new FutureTask(new Callable() { @Override public String call() throws Exception { BufferedReader br = null; InputStreamReader isr = null; URLConnection conn; try { URL geturl = new URL(url); conn = geturl.openConnection();//创建连接 conn.connect();//get连接 isr = new InputStreamReader(conn.getInputStream());//输入流 br = new BufferedReader(isr); String line = null; while ((line = br.readLine()) != null) { sb.append(line);//获取输入流数据 } System.out.println(sb.toString()); } catch (Exception e) { e.printStackTrace(); } finally {//执行流的关闭 if (br != null) { try { if (br != null) { br.close(); } if (isr != null) { isr.close(); } } catch (IOException e) { e.printStackTrace(); } }} return sb.toString(); } }); new Thread(task).start(); String s = null; try { s = task.get();//异步获取返回值 } catch (Exception e) { e.printStackTrace(); } return s; } POST public static String post(final String url, final Map map) { final StringBuilder sb = new StringBuilder(); FutureTask task = new FutureTask(new Callable() { @Override public String call() throws Exception { DataOutputStream out = null; BufferedReader br = null; URLConnection conn; URL posturl = new URL(url); try { conn = posturl.openConnection();//创建连接 conn.setDoInput(true);//post请求必须设置 conn.setDoOutput(true);//post请求必须设置 out = new DataOutputStream(conn .getOutputStream());//输出流 StringBuilder request = new StringBuilder(); for (String key : map.keySet()) { request.append(key + \"=\" + URLEncoder.encode(map.get(key), \"UTF-8\") + \"&\"); }//连接请求参数 out.writeBytes(request.toString());//输出流写入请求参数 out.flush(); out.close(); br = new BufferedReader(new InputStreamReader(conn.getInputStream()));//获取输入流 String line; while ((line = br.readLine()) != null) { sb.append(line); } System.out.println(sb.toString()); } catch (Exception e) { e.printStackTrace(); } finally {//执行流的关闭 if (br != null) { br.close(); } if (out != null) { out.close(); } } return sb.toString(); } }); String s = null; new Thread(task).start(); try { s = task.get();//异步获取返回值 } catch (Exception e) { e.printStackTrace(); } return s; } 发送请求 // String url = \"http://192.168.0.199:8080/android\"; // String s = get(url); // System.out.println(s); HashMap stringStringHashMap = new HashMap<>(); stringStringHashMap.put(\"hid\",\"CC:1B:E0:B0:23:DE\"); stringStringHashMap.put(\"bid\",\"CC:1B:E0:B0:23:DE\"); // stringStringHashMap.put(\"question\",\"%e8%ae%b2%e4%b8%aa%e6%95%85%e4%ba%8b\"); stringStringHashMap.put(\"question\",message); stringStringHashMap.put(\"token\",\"a39813cf8a550312f24196e7d99c2bc8\"); String post = post(\"http://nim.prec-robot.com/api/speechrcognition/speechrcognition\", stringStringHashMap); System.out.println(post); Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"android/改造为web服务器.html":{"url":"android/改造为web服务器.html","title":"改造为web服务器","keywords":"","body":"教程环境配置BusyBox FreeLinux DeployTurboxshellngroknginx参考链接教程 Android改造为服务器,完成内网穿透 环境 ​ 手机: OnePlus3T Android 9.0 需ROOT ​ 所需软件: BusyBox Free、Linux Deploy、Turbo客户端(可管理服务器上的文件) ​ 内网穿透: https://www.ngrok.cc/ ​ ssh连接工具: xshell或final shell 配置 BusyBox Free 点击install安装,安装完成后看到sucessful字样即可 Linux Deploy 其中的架构根据自己手机cpu架构而定 Ubuntu源使用清华的:http://mirror.tuna.tsinghua.edu.cn/ubuntu-ports/ 本地化选择zh_CN.UTF-8 启用SSH 配置好点击右上角按钮选择安装,看到 左下角便是启用按钮,最上方的ip是路由器分配给Android设备的IP,为了避免每次变化可以在路由器设置中进行绑定(IP与MAC绑定) Turbo 左下角选择创建一个新用户,选择SFTP,输入IP和账号密码即可,可以修改linux中的html查看效果,可配合MT管理器使用 xshell 名称随便输入即可,主机直接写在Linux Deploy中显示的IP即可,用户名和密码同样写在Linux Deploy中配置的即可 ngrok 注册登录完成后,开通隧道,ngrok提供了一种免费的内网穿透方案(购买使用体验更加) 可以使用官方提供的使用文档:http://www.ngrok.cc/_book/ 下载对应的客户端:https://www.ngrok.cc/download.html 手机端的话选择Linux ARM版本即可 下载完成后将解压后的文件上传至服务器中即可 根据官方文档的命令启动即可 # 直接启动 ./sunny clientid yourClientID # 后台启动 setsid ./sunny clientid yourClientID & nginx 配置nginx.conf user www-data; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 768; # multi_accept on; } http { server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 } } ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## # 需要将以下两行注释,conf.d文件夹下的*.conf文件默认会被该主配置文件加载 # 在centos7中使用正常,在此处使用时出现修改文件,访问时一直是欢迎页面 #include /etc/nginx/conf.d/*.conf; #include /etc/nginx/sites-enabled/*; } #mail { # # See sample authentication script at: # # http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript # # # auth_http localhost/auth.php; # # pop3_capabilities \"TOP\" \"USER\"; # # imap_capabilities \"IMAP4rev1\" \"UIDPLUS\"; # # server { # listen localhost:110; # protocol pop3; # proxy on; # } # # server { # listen localhost:143; # protocol imap; # proxy on; # } #} nginx相关命令 # 查看nginx状态(启动端口) ps -ef|grep nginx # 停掉指定的端口 kill -TERM 2132 # 检查配置文件是否正确 nginx -t # 启动 nginx -c /etc/nginx/nginx.conf # 重启 nginx -s reload # 停止 nginx -s stop 参考链接 https://blog.csdn.net/qq_20084101/article/details/80913951 https://cloud.tencent.com/developer/article/1159800 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"jrebel/远程调试及热部署.html":{"url":"jrebel/远程调试及热部署.html","title":"Jrebel","keywords":"","body":"远程调试及热部署目的远程调试本地热部署远程热部署方法链路查看自行搭建jrebel激活服务器远程调试及热部署 目的 ​ 远程调试：有时候本地调试满足不了需要，比如该服务引用了其他的服务，拿公有云说想要调试scan-service，因为他用到了ssk的服务，而我们本地起不了ssk或者说起来也没啥用，因此可以使用远程调试的方式帮助我们查找问题。 ​ 热部署：热部署是为了避免我们修改代码每次都需要重启服务带来的耗时，spring提供了自带的热部署方式，但是它的机制还是监听到有修改会重启服务，为了避免这个问题可以使用IDEA的插件 JRebel。 ​ 远程热部署：比如我们在测试环境通过调试找出了问题，但是每次去以传jar包的方式频繁的部署服务也很耗时，也可以使用JRebel进行远程代码的热部署。 ​ 本服务链路调用查看：有时候一个请求可能会涉及很多操作，我们可以通过JrebelX 查看这个请求调用的所有链路（本方法，不包含服务之间调用的链路）。 ​ 貌似这些都依赖于java的agent机制，感兴趣的可以研究研究。 注意：开发环境玩玩得了。 远程调试 现在我们把包传到服务器上看一看效果 启动命令 nohup java -jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:10023 ./learn-debug-1.0.0.jar & 本地热部署 使用JRebel插件 IDEA里设置更新按钮的操作：更新classes文件和resources 刷新前 修改代码不重启服务，点刷新后 远程热部署 插件下载地址：https://www.jrebel.com/products/jrebel/download/prev-releases 生成UUID : https://www.uuidgenerator.net/ bash jrebel/bin/activate.sh http://激活地址/9d2e2710-a2de-47b9-80c6-1c8a9b4148eb email地址 java -jar jrebel/jrebel.jar -set-remote-password 12345321 nohup java -jar -agentpath:/home/app/jrebel-stabel/jrebel/lib/libjrebel64.so -Drebel.remoting_plugin=true ./learn-debug-1.0.0.jar & 方法链路查看 可以查看到这个方法具体的调用过程，以及数据库查询、es查询等等 自行搭建jrebel激活服务器 github: https://github.com/Byron4j/JrebelLicenseServerforJava 克隆下来打个包 根据readme.md 打个docker 镜像 Dockerfile FROM java:8-jre-alpine ENV PORT 8086 ADD target/JrebelBrainsLicenseServerforJava-1.0-SNAPSHOT-jar-with-dependencies.jar /JrebelBrains.jar CMD java -jar /JrebelBrains.jar -p $PORT docker run -d --name jrebel-ls --restart always -e PORT=8086 -p 8086:8086 jrebel-ls 这就成功了！ Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"elasticstack/elasticsearch.html":{"url":"elasticstack/elasticsearch.html","title":"ElasticSearch","keywords":"","body":"ElasticSearch简介下载基本概念基本操作插入数据更新数据删除数据搜索数据DSL搜索聚合批量操作分页映射结构化查询分词ElasticSearch 简介 如果你没有听说过Elastic Stack，那你一定听说过ELK，实际上ELK是三款软件的简称，分别是Elasticsearch、 Logstash、Kibana组成，在发展的过程中，又有新成员Beats的加入，所以就形成了Elastic Stack。所以说，ELK是 旧的称呼，Elastic Stack是新的名字。 下载 该案例使用的windows版本,直接使用bin目录下的 .bat文件启动,默认端口9200 最新版: https://www.elastic.co/cn/downloads/elasticsearch 历史版: https://www.elastic.co/cn/downloads/past-releases IK分词器: https://github.com/medcl/elasticsearch-analysis-ik ElasticSearch Head: 界面化工具,可以在谷歌浏览器的插件商店搜索安装 安装IK分词器: ​ 在elasticsearch的bin目录下运行(可能需要使用5.5-6.3版本) elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip 基本概念 索引 索引（index）是Elasticsearch对逻辑数据的逻辑存储，所以它可以分为更小的部分。 可以把索引看成关系型数据库的表，索引的结构是为快速有效的全文索引准备的，特别是它不存储原始值。Elasticsearch可以把索引存放在一台机器或者分散在多台服务器上，每个索引有一或多个分片（shard），每个分片可以有多个副本（replica）。 文档 存储在Elasticsearch中的主要实体叫文档（document）。用关系型数据库来类比的话，一个文档相当于数据库表中的一行记录。Elasticsearch和MongoDB中的文档类似，都可以有不同的结构，但Elasticsearch的文档中，相同字段必须有相 同类型。文档由多个字段组成，每个字段可能多次出现在一个文档里，这样的字段叫多值字段（multivalued）。每个字段的类型，可以是文本、数值、日期等。字段类型也可以是复杂类型，一个字段包含其他子文档或者数组。 映射 所有文档写进索引之前都会先进行分析，如何将输入的文本分割为词条、哪些词条又会被过滤，这种行为叫做映射（mapping）。一般由用户自己定义规则。 文档类型 在Elasticsearch中，一个索引对象可以存储很多不同用途的对象。例如，一个博客应用程序可以保存文章和评论。每个文档可以有不同的结构。不同的文档类型不能为相同的属性设置不同的类型。例如，在同一索引中的所有文档类型中，一个叫title的字段必须具有相同的类型。 基本操作 ElasticSearch中提供了丰富的RestfulAPI,可以进行基本的CRUD,创建删除索引等。 插入数据 POST /索引/类型/id {\"id\":\"1001\",\"name\": \"张三\",\"age\": 18,\"sex\": \"男\"} # 返回的结果 { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2772IXIBDTn7N9ThVkY8\", # 如果不写id,会默认添加一个id 32位长度的字符串 \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1 } 更新数据 在ElasticSearch中不支持使用PUT方式进行更新,但可以使用覆盖的方式进行更新 PUT|POST es/user/2772IXIBDTn7N9ThVkY8 {\"id\":\"1001\",\"name\":\"李四\",\"age\":18,\"sex\":\"女\"} { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2772IXIBDTn7N9ThVkY8\", \"_version\": 2, # 更新后其版本发生了变化 \"result\": \"updated\", # 更新 \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1 } 或者使用POST方式加在请求路径上添加 _update的方式 POST es/user/2772IXIBDTn7N9ThVkY8/_update { \"doc\":{ \"age\":100 } } 删除数据 DELETE es/user/2772IXIBDTn7N9ThVkY8 { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2772IXIBDTn7N9ThVkY8\", \"_version\": 5, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 4, \"_primary_term\": 1 注意: 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 搜索数据 # 根据指定id进行搜索 GET es/user/1001 # 查询所有数据 es/user/_search # 包含搜索条件 es/user/_search?q=age:24 # 显示指定字段 es/user/1001?_source=name,sex # 只显示原数据 es/user/1001/_source # 只显示原数据的情况下指定子段 es/user/1001/_source?_source=name,age DSL搜索 可以构建更加丰富灵活的查询语言,以JSON请求体出现 POST es/user/_search # 请求体 {\"query\": {\"match\": {\"age\": 18}}} # 查询年龄大于18的数据 {\"query\": {\"bool\": {\"filter\": {\"range\": {\"age\": {\"gt\":18}}}}}} # 查询名字为李四或者王五的数据 并且name字段高亮显示 {\"query\": {\"match\": {\"name\": \"张三李四\"}},\"highlight\": {\"fields\": {\"name\": {}}}} # 结果 { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 2, \"max_score\": 1.3862944, \"hits\": [ { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"1001\", \"_score\": 1.3862944, \"_source\": { \"name\": \"张三\", \"age\": 24, \"sex\": \"女\" }, \"highlight\": { \"name\": [ \"张三\" ] } } , { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"1002\", \"_score\": 1.3862944, \"_source\": { \"name\": \"李四\", \"age\": 18, \"sex\": \"女\" }, \"highlight\": { \"name\": [ \"李四\" ] } } ] } } 聚合 POST es/user/_search # 对sex字段进行分组操作时,会出现以下错误 {\"aggs\": {\"all_interests\": {\"terms\": {\"field\": \"sex\"}}}} { \"error\": { \"root_cause\": [ { \"type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default. Set fielddata=true on [sex] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\" } ], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"es\", \"node\": \"7uI7DaOfRIKx0FHb8o6G2g\", \"reason\": { \"type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default. Set fielddata=true on [sex] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\" } } ], \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default. Set fielddata=true on [sex] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default. Set fielddata=true on [sex] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\" } } }, \"status\": 400 } # 可改为 字段.keyword方式 { \"size\": 0, \"aggs\": { \"group_count\": { \"terms\": {\"field\": \"sex.keyword\"} } } } { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 4, \"max_score\": 0, \"hits\": [ ] }, \"aggregations\": { \"group_count\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"女\", \"doc_count\": 2 } , { \"key\": \"男\", \"doc_count\": 2 } ] } } } 批量操作 POST es/user/_mget { \"ids\" : [ \"1001\", \"1003\" ] } 如果有一条数据不存在,不影响整体查询,不存在的数据有如下结构 { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"1006\", \"found\": false } 批量插入 POST /_bulk 需要在最后一行加一个换行 {\"create\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2001}} {\"id\":2001,\"name\":\"name1\",\"age\": 20,\"sex\": \"男\"} {\"create\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2002}} {\"id\":2002,\"name\":\"name2\",\"age\": 20,\"sex\": \"男\"} {\"create\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2003}} {\"id\":2003,\"name\":\"name3\",\"age\": 20,\"sex\": \"男\"} { \"took\": 252, \"errors\": false, \"items\": [ { \"create\": { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2001\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 2, \"status\": 201 } }, { \"create\": { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2002\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 2, \"status\": 201 } }, { \"create\": { \"_index\": \"es\", \"_type\": \"user\", \"_id\": \"2003\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 2, \"status\": 201 } } ] } 批量删除 {\"delete\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2001}} {\"delete\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2002}} {\"delete\":{\"_index\":\"es\",\"_type\":\"user\",\"_id\":2003}} 分页 和SQL中的limit类似,其接受from和size参数 size: 结果数，默认10 from: 跳过开始的结果数，默认0 from参数的计算 (页数-1)*size GET /_search?size=5 GET /_search?size=5&from=5 GET /_search?size=5&from=10 映射 ​ 前面我们创建的索引以及插入数据，都是由Elasticsearch进行自动判断类型，有些时候我们是需要进行明确字段类型的，否则，自动判断的类型和实际需求是不相符的。 创建明确类型的索引 PUT dell { \"settings\": { \"index\": { \"number_of_shards\": \"2\", \"number_of_replicas\": \"0\" } }, \"mappings\": { \"person\": { \"properties\": { \"name\": { \"type\": \"text\" }, \"age\": { \"type\": \"integer\" }, \"mail\": { \"type\": \"keyword\" }, \"hobby\": { \"type\": \"text\" } } } } } 查看映射 GET dell/_mapping 插入数据 POST /_bulk {\"index\":{\"_index\":\"dell\",\"_type\":\"person\"}} {\"name\":\"张三\",\"age\": 20,\"mail\": \"111@qq.com\",\"hobby\":\"羽毛球、乒乓球、足球\"} {\"index\":{\"_index\":\"dell\",\"_type\":\"person\"}} {\"name\":\"李四\",\"age\": 21,\"mail\": \"222@qq.com\",\"hobby\":\"羽毛球、乒乓球、足球、篮球\"} {\"index\":{\"_index\":\"dell\",\"_type\":\"person\"}} {\"name\":\"王五\",\"age\": 22,\"mail\": \"333@qq.com\",\"hobby\":\"羽毛球、篮球、游泳、听音乐\"} {\"index\":{\"_index\":\"dell\",\"_type\":\"person\"}} {\"name\":\"赵六\",\"age\": 23,\"mail\": \"444@qq.com\",\"hobby\":\"跑步、游泳\"} {\"index\":{\"_index\":\"dell\",\"_type\":\"person\"}} {\"name\":\"孙七\",\"age\": 24,\"mail\": \"555@qq.com\",\"hobby\":\"听音乐、看电影\"} # 查询 POST /dell/person/_search 请求体:查询hobby中包含音乐的 { \"query\" : { \"match\" : { \"hobby\" : \"音乐\" } } } 结构化查询 term 主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed 的字符串(未经分析的文本 { \"term\": { \"age\": 26 }} { \"term\": { \"date\": \"2014-09-01\" }} { \"term\": { \"public\": true }} { \"term\": { \"tag\": \"full_text\" }} POST /dell/person/_search { \"query\" : { \"term\" : { \"age\" : 20 } } } terms查询 terms 跟 term 有点类似，但 terms 允许指定多个匹配条件。 如果某个字段指定了多个值，那么文档需要一起去做匹配： { \"query\" : { \"terms\" : { \"age\" : [20,21] } } } range 过滤允许我们按照指定范围查找一批数据： gt 大于 gte 大于等于 lt 小于 lte 小于等于 { \"query\": { \"range\": { \"age\": { \"gte\": 20, \"lte\": 22 } } } } exists 查询可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的IS_NULL 条件 { \"query\": { \"exists\": { \"field\": \"age\" } } } match 查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析match 一下查询字符：如果用match 下指定了一个确切值，在遇到数字，日期，布尔值或者not_analyzed 的字符串时，它将为你搜索你 给定的值： bool 查询可以用来合并多个条件查询结果的布尔逻辑，它包含一下操作符： must :: 多个查询条件的完全匹配,相当于 and 。 must_not :: 多个查询条件的相反匹配，相当于 not 。 should :: 至少有一个查询条件匹配, 相当于 or 。 分词 POST /_analyze { \"analyzer\":\"standard\", \"text\":\"hello world\" } POST /dell/_analyze { \"analyzer\": \"standard\", \"field\": \"hobby\", \"text\": \"听音乐\" } 使用IK分词器 \"analyzer\": \"ik_max_word\" Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"elasticstack/ElasticSearch主要搜索关键字.html":{"url":"elasticstack/ElasticSearch主要搜索关键字.html","title":"ElasticSearch","keywords":"","body":"ElasticSearch主要搜索关键字 前提条件 首先存入一条数据 i like eating and kuing 默认分词器应该将内容分为 “i” “like” “eating” “and” “kuing” 注意如果使用supplierName.keyword做查询（数据源采用es库中不分词数据） SearchSourceBuilder对应着查询json结构的最外层（from，size，query，filter，sort，aggs） QueryBuilder 对应着各种过滤条件：精确搜索，布尔，范围 1 QueryBuilders.matchQuery(“supplierName”,param) match查询，会将搜索词分词，再与目标查询字段进行匹配，若分词中的任意一个词与目标字段匹配上，则可查询到。 输入条件 param 结果 i 可查出 i li 可查出 i like 可查出 i like eat 可查出 and 可查出 kuing 可查出 ku 查不出 li 查不出 eat 查不出 i like eating and kuing 可查出 分词后精确查询，分词之间or关系,有一个分词匹配即匹配 如果使用 \"match\": {\"message.keyword\": \"xxx\"}，即不分词只有 i like eating and kuing可以查出 2 QueryBuilders.matchPhraseQuery(“supplierName”,param) 默认使用 match_phrase 时会精确匹配查询的短语，需要全部单词和顺序要完全一样，标点符号除外。 param 结果 i 可查出 i li 查不出 i like 可查出 i like eat 查不出 and 可查出 kuing 可查出 ku 查不出 li 查不出 eat 查不出 i like eating and kuing 可查出 有序连贯分词模糊查询（任意分词任意数量按照原有顺序连续排列组合可以查出，其他不可查出） 如果使用 \"match_phrase\": {\"message.keyword\": \"xxx\"}，即不分词只有 i like eating and kuing可以查出 3 QueryBuilders.matchPhrasePrefixQuery(“supplierName”,param) match_phrase_prefix 和 match_phrase 用法是一样的，区别就在于它允许对最后一个词条前缀匹配。 与match_phrase唯一区别 param = “i like eating and kui” 查出 最后一个词条之前的词匹配规则与match_phrase相同++最后一个此条为前缀进行模糊匹配 match_phrase_prefix不能使用supplierName.keyword模式 4 QueryBuilders.termQuery(“supplierName”,param) term query,输入的查询内容是什么，就会按照什么去查询，并不会解析查询内容，对它分词。 param 结果 i 可查出 i li 查不出 i like 查不出 i like eat 查不出 and 可查出 kuing 可查出 ku 查不出 li 查不出 eat 查不出 i like eating and kuing 查不出 查询条件不分词精确匹配分词数据，命中一个分词即匹配 param = “i like eating and kuing” 查不出 如果使用 \"term\": {\"message.keyword\": \"xxx\"}，即不分词只有 i like eating and kuing可以查出 5 QueryBuilders.wildcardQuery(“supplierName”,\"*\"+param+\"*\") 条件wildcard不分词查询，加*（相当于sql中的%）表示模糊查询，加keyword表示查不分词数据 {\"wildcard\":{\"message.keyword\":\"atin\"} 等同sql于like查询 不能使用supplierName.keyword模式 总结 fieldName.keyword决定是否采用es分词数据源，不带keyword即查询text格式数据（分词），带即查询keyword格式数据（不分词） 见https://blog.csdn.net/tyw15/article/details/111935033 match，match_phrase，match_phrase_prefix，term，wildcard决定查询条件分词形式以及分词后的查询条件关联关系 1、2、4使用.keyword查询相当于不分词精确匹配，5是模糊查询，其他都是变种分词组合模糊查询 BoolQueryBuilder queryBuilder= QueryBuilders.boolQuery(); * must: AND * mustNot: NOT * should:: OR queryBuilder.must(QueryBuilders.termQuery(\"user\", \"kimchy\")) ​ .mustNot(QueryBuilders.termQuery(\"message\", \"nihao\")) ​ .should(QueryBuilders.termQuery(\"gender\", \"male\")); Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"elasticstack/elasticsearch_api.html":{"url":"elasticstack/elasticsearch_api.html","title":"Api","keywords":"","body":"Elasticsearch apireindex远程集群reindex本集群reindex查看进度取消reindex基本操作创建索引-并设置mappings、settings和aliases查询插入删除添加mapping向索引增加mapping查看mapping查看索引的设置根据ID查询根据ID增量更新添加别名havingElasticsearch api reindex 远程集群reindex 远程集群需在 elasticsearch.yml 中配置 reindex.remote.whitelist: \"192.168.10.19:9200\" POST 192.168.10.63:9200/_reindex { \"source\": { \"remote\": { \"host\": \"http://192.168.10.63:9200\" }, \"index\": \"zc_machine\", \"size\": 10000 }, \"dest\": { \"index\": \"tao_dest_src\" }, \"size\": 100 } 本集群reindex POST 192.168.10.65:9200/_reindex { \"source\": { \"index\": \"zc_machine\", \"size\": 10000 }, \"dest\": { \"index\": \"tao_dest_src\" } } 查看进度 GET 192.168.10.65:9200/_tasks?detailed=true&actions=*reindex 取消reindex POST 192.168.10.63:9200/_tasks/FNotLQhjTP-0RvQNCV7jWQ:20558642/_cancel 基本操作 创建索引-并设置mappings、settings和aliases PUT 192.168.10.65:9200/tao_dest_src { \"aliases\":{ }, \"mappings\":{ \"properties\":{ \"appPerson\":{ \"type\":\"text\", \"fields\":{ \"keyword\":{ \"type\":\"keyword\", \"ignore_above\":256 } } } } }, \"settings\":{ \"index\":{ \"refresh_interval\":\"1s\", \"number_of_shards\":\"1\", \"translog\":{ \"sync_interval\":\"60s\", \"durability\":\"async\" }, \"max_result_window\":\"50000\", \"number_of_replicas\":\"1\" } } } 查询 POST http://192.168.123.252:9200/zc_network_connection/_search { \"query\":{ \"match\":{ \"_id\":\"0032043cccc8473da03656f482d4bc9e\" } }, \"from\": 0, \"size\": 10 } 插入 POST http://192.168.123.159:9200/zc_user/_doc { \"userExpireTime\":\"never\", \"syncTime\":1649185386309, \"userStatus\":1, \"softwareVersion\":\"win_3.1.23.17\", \"machineIp\":\"192.168.123.14\" } 删除 POST http://192.168.10.63:9200/tao_dest_event/_delete_by_query { \"query\": { \"match_all\": { } } } 添加mapping PUT http://192.168.10.201:9200/zc_process/_mapping { \"properties\":{ \"installationTime\":{ \"type\":\"long\", \"index\":false } } } 向索引增加mapping PUT http://192.168.123.159:9200/zc_web/_doc/_mapping?include_type_name=true { \"properties\":{ \"machinePrimaryIps\":{ \"type\": \"nested\", \"properties\":{ \"ip\":{ \"type\":\"text\", \"fields\":{ \"keyword\":{ \"type\":\"keyword\", \"ignore_above\":256 } } }, \"ipLong\":{ \"type\":\"long\" }, \"source\":{ \"type\":\"long\" }, \"type\":{ \"type\":\"long\" }, \"version\":{ \"type\":\"long\" } } } } } 查看mapping GET http://192.168.123.252:9200/zc_process/_mapping 查看索引的设置 GET http://192.168.123.159:9200/zc_machine/_settings 根据ID查询 GET http://192.168.123.159:9200/zc_database/_doc/o8EznIABWwFXVZOQ4Wuu 根据ID增量更新 POST 192.168.10.65:9200/tao_dest_src/_doc/6f98e312e4d3392e873324ff1aa36d63/_update?refresh=true { \"doc\":{ \"onlineStatus\": 1 } } 添加别名 POST 192.168.10.63:9200/_aliases { \"actions\":[ { \"add\":{ \"index\":\"tao_dest_event\", \"alias\":\"zzz\" } } ] } having POST _sql?format=json { \"query\" : \"select machineIp,count(1) c from zc_machine group by machineIp having c >= 5 \" } { \"columns\" : [ { \"name\" : \"machineIp\", \"type\" : \"text\" }, { \"name\" : \"c\", \"type\" : \"long\" } ], \"rows\" : [ [ \"192.168.10.56\", 5 ] ], \"cursor\" : \"49itAwFaAWMBCnpjX21hY2hpbmWUAgEBCWNvbXBvc2l0ZQdncm91cGJ5AAEPYnVja2V0X3NlbGVjdG9yC2hhdmluZy4yNDM0AQZfY291bnT/AQJhMAZfY291bnQAAQhwYWlubGVzc1ZJbnRlcm5hbFNxbFNjcmlwdFV0aWxzLm51bGxTYWZlRmlsdGVyKEludGVybmFsU3FsU2NyaXB0VXRpbHMuZ3RlKHBhcmFtcy5hMCxwYXJhbXMudjApKQoACgECdjABAAAABQH/AQAEMjQ4MwERbWFjaGluZUlwLmtleXdvcmQAAAEAAOgHAQoBBDI0ODMADTE5Mi4xNjguMTAuNzMAAgEAAAAAAQD/////DwAAAAAAAAAAAAAAAAFaAwACAgAAAAAAAP////8PAgFrBDI0ODMAAAFrBDI0ODMBAAEDAA==\" } POST _sql/translate { \"query\" : \"select machineIp,count(1) c from zc_machine group by machineIp having c >= 5 \" } SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); TermsAggregationBuilder field = AggregationBuilders.terms(\"machineUuidTerms\").field(\"machineIp.keyword\"); Script script = new Script(\"params.count >= 3\"); Map bucketsPathsMap = new HashMap<>(); bucketsPathsMap.put(\"count\", \"_count\"); BucketSelectorPipelineAggregationBuilder having = PipelineAggregatorBuilders.bucketSelector(\"having\", bucketsPathsMap, script); field.subAggregation(having); searchSourceBuilder.aggregation(field); BoolQueryBuilder boolQueryBuilders = QueryBuilders.boolQuery(); searchSourceBuilder.query(boolQueryBuilders); Search search = new Search.Builder(searchSourceBuilder.toString()).addIndex(\"zc_machine\").build(); SearchResult searchResult = null; searchResult = jestClient.execute(search); System.out.println(); { \"query\":{ \"bool\":{ \"adjust_pure_negative\":true, \"boost\":1 } }, \"aggregations\":{ \"machineUuidTerms\":{ \"terms\":{ \"field\":\"machineIp.keyword\", \"size\":10, \"min_doc_count\":1, \"shard_min_doc_count\":0, \"show_term_doc_count_error\":false, \"order\":[ { \"_count\":\"desc\" }, { \"_key\":\"asc\" } ] }, \"aggregations\":{ \"having\":{ \"bucket_selector\":{ \"buckets_path\":{ \"count\":\"_count\" }, \"script\":{ \"source\":\"params.count >= 3\", \"lang\":\"painless\" }, \"gap_policy\":\"skip\" } } } } } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"elasticstack/ElasticSearch_Nested类型对文档的影响.html":{"url":"elasticstack/ElasticSearch_Nested类型对文档的影响.html","title":"Nested类型对文档的影响","keywords":"","body":"ElasticSearch Nested类型对文档的影响对聚合操作的影响不使用nested使用nested对查询的影响对文档本身的影响为什么 nested类型的文档变成了7个？总结ElasticSearch Nested类型对文档的影响 对聚合操作的影响 存在的问题：如果索引中存在对象数组的属性，在聚合时会出现数据不准确的错误，跟他的存储结构也有关系 不使用nested mapping { \"user\" : { \"mappings\" : { \"properties\" : { \"hobbies\" : { \"properties\" : { \"id\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"tag\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } 插入数据进行测试 POST user/_doc/11111 { \"name\":\"Smith\", \"hobbies\": [ { \"id\": \"1\", \"tag\": \"足球\" }, { \"id\": \"2\", \"tag\": \"篮球\" } ] } POST user/_doc/22222 { \"name\":\"Andi\", \"hobbies\": [ { \"id\": \"1\", \"tag\": \"足球\" }, { \"id\": \"3\", \"tag\": \"游泳\" } ] } POST user/_doc/33333 { \"name\":\"Jane\", \"hobbies\": [] } POST user/_search { \"query\": { \"match_all\": { } } } 聚合 这段聚合的意思是根据 hobbies.id + hobbies.tag 两个字段加起来进行分组算出count POST user/_search { \"query\":{ \"match_all\":{ } }, \"aggs\":{ \"hobbies_agg\":{ \"terms\":{ \"field\":\"hobbies.id.keyword\", \"size\":10 }, \"aggs\":{ \"tag_aggs\":{ \"terms\":{ \"field\":\"hobbies.tag.keyword\", \"size\":10 } } } } } } 聚合结果 可以看到 key为 “1” 的 对应的buckets 出现了篮球以及游泳，我们的原意是只想查询id为1的tag出现了几次，id为1的tag为足球，出现了两次，所以正确数据应该是2 { \"aggregations\" : { \"hobbies_agg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"1\", \"doc_count\" : 2, \"tag_aggs\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"足球\", \"doc_count\" : 2 }, { \"key\" : \"游泳\", \"doc_count\" : 1 }, { \"key\" : \"篮球\", \"doc_count\" : 1 } ] } }, { \"key\" : \"2\", \"doc_count\" : 1, \"tag_aggs\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"篮球\", \"doc_count\" : 1 }, { \"key\" : \"足球\", \"doc_count\" : 1 } ] } }, { \"key\" : \"3\", \"doc_count\" : 1, \"tag_aggs\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"游泳\", \"doc_count\" : 1 }, { \"key\" : \"足球\", \"doc_count\" : 1 } ] } } ] } } } 为什么会这样？ 我们先不看es的原理可以根据索引中的数据进行分析推测一下，为什么会这样 id为1,tag为足球的数据存在两条数据中，分别是id为 11111 和 id为 22222 使用nested mapping { \"user_nested\" : { \"mappings\" : { \"properties\" : { \"hobbies\" : { \"type\" : \"nested\", \"properties\" : { \"id\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"tag\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } 插入数据进行测试 POST user_nested/_doc/123456 { \"name\":\"Smith\", \"hobbies\": [ { \"id\": \"1\", \"tag\": \"足球\" }, { \"id\": \"2\", \"tag\": \"篮球\" } ] } POST user_nested/_doc/789123 { \"name\":\"Andi\", \"hobbies\": [ { \"id\": \"1\", \"tag\": \"足球\" }, { \"id\": \"3\", \"tag\": \"游泳\" } ] } POST user_nested/_doc/456789 { \"name\":\"Jane\", \"hobbies\": [] } POST user_nested/_search { \"query\": { \"match_all\": { } } } 聚合 POST user_nested/_search { \"query\":{ \"match_all\":{ } }, \"aggs\": { \"nested_agg\": { \"nested\": { \"path\": \"hobbies\" }, \"aggs\": { \"id_agg\": { \"terms\": { \"field\": \"hobbies.id.keyword\", \"size\": 10 }, \"aggs\": { \"tag_agg\": { \"terms\": { \"field\": \"hobbies.tag.keyword\", \"size\": 10 } } } } } } } } 聚合结果 可以看到这次的结果和我们预期的结果是一致的 { \"aggregations\" : { \"nested_agg\" : { \"doc_count\" : 4, \"id_agg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"1\", \"doc_count\" : 2, \"tag_agg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"足球\", \"doc_count\" : 2 } ] } }, { \"key\" : \"2\", \"doc_count\" : 1, \"tag_agg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"篮球\", \"doc_count\" : 1 } ] } }, { \"key\" : \"3\", \"doc_count\" : 1, \"tag_agg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"游泳\", \"doc_count\" : 1 } ] } } ] } } } } 对查询的影响 这是全部数据 我们使用一个混乱的查询逻辑 POST user/_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"hobbies.id\": { \"value\": \"1\" } } }, { \"term\": { \"hobbies.tag.keyword\": { \"value\": \"游泳\" } } } ] } } } ​ 也可以查询出结果，这样就不对了（我们的原意是想查hobbies数组中 id为1并且tag是游泳的，很显然现在的索引里不存在这样的数据，但还是查出来了，因为他同一个数组里，有一条数据id是1的，有另一条数据tag是游泳，他把这种情况也当作正当数据返回了，这显然不符合我们的需求），如果使用nested类型就不会出现这种情况，不会查出数据 对文档本身的影响 查看文档个数 GET /_cat/count/user?v 结果：3 GET /_cat/count/user_nested?v 结果：3 GET _cat/indices/user?v 结果：3 GET _cat/indices/user_nested?v 结果：7 为什么 nested类型的文档变成了7个？ ​ 另外嵌套文档：nested类型的属性其实在lucene内部是独立的文档，只不过在ES这一层隐藏了这些细节。 ​ 比如一个文档中存在一个nested属性的数组，该数组中又有三条数据，这三条数据其实会被另外当成一个文档存储，再加上文档父文档本身就是4个。 总结 ​ 索引中存在对象数组的属性，并且需要聚合或者查询时，要考虑数据正确性的问题时，需使用nested类型，但这种类型非常消耗性能，也需谨慎使用，尽量在定义结构时更加的合理化。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"elasticstack/ES_工作中遇到的问题.html":{"url":"elasticstack/ES_工作中遇到的问题.html","title":"ES_工作中遇到的问题.md","keywords":"","body":"工作中遇到的问题1、数据添加到索引后延迟显示问题2、时间聚合，类型问题3、nested字段 + 普通类型字段 聚合4、es having操作5、查询数组为空的数据6、查询数组中同时包含两个及以上的值工作中遇到的问题 1、数据添加到索引后延迟显示问题 当数据添加到索引后并不能马上被查询到，等到索引刷新后才会被查询到。 refresh_interval 配置的刷新间隔。如果我们有需求必须要进行实时更新可以添加参数 refresh(true) UpdateByQuery updateByQuery = new UpdateByQuery.Builder(source).addIndex(getIndexName(userUuid)).addType(\"doc\").refresh(true).build(); 2、时间聚合，类型问题 有一些查询会使用到比如 24小时内或者一周或者一个月的 数据，在使用对应的API时需要注意 DateHistogramAggregationBuilder dateHistogramAggregationBuilder = AggregationBuilders.dateHistogram(\"dayAgg\") .field(\"standardTimestamp\") .fixedInterval(DateHistogramInterval.DAY) .order(BucketOrder.key(false)) .minDocCount(0L) .timeZone(zoneId) .format(DateUtil.YYYY_MM_DD_HH_MM_SS) .extendedBounds(new ExtendedBounds(now.toInstant(ZoneOffset.ofHours(8)).toEpochMilli(), plus.toInstant(ZoneOffset.ofHours(8)).toEpochMilli())); 该示例使用的字段是 standardTimestamp 一开始使用的是long类型，得出的结果会从前一天的8点开始，总之数据是错误的，需要将该字段改为 date 类型 又因为索引建立后不允许修改mapping,所以在入库时还需要注意这个问题 3、nested字段 + 普通类型字段 聚合 // 使用nested字段 + 普通类型字段聚合 // 如果还是直接去追加 subAggregation 那么会自动把这个字段也当作是第一个的nested path（userDatas）下的字段，但machineUuid并不是，所以需要先从nested字段中跳出来 // 使用 AggregationBuilders.reverseNested API 可以做到 NestedAggregationBuilder nested = AggregationBuilders.nested(\"userDatas\", \"userDatas\"); TermsAggregationBuilder machineTags = AggregationBuilders.terms(\"machineTags\").field(\"userDatas.machineTags.keyword\").size(Integer.MAX_VALUE); TermsAggregationBuilder userDataUuid = AggregationBuilders.terms(\"userUuid\").field(\"userDatas.userUuid.keyword\").size(Integer.MAX_VALUE); userDataUuid.subAggregation(machineTags); nested.subAggregation(userDataUuid); CardinalityAggregationBuilder machineCount = AggregationBuilders.cardinality(\"machineCount\").field(\"machineUuid.keyword\"); ReverseNestedAggregationBuilder revers = AggregationBuilders.reverseNested(\"revers\").subAggregation(machineCount); machineTags.subAggregation(revers); searchSourceBuilder.aggregation(nested); 4、es having操作 // 这里的count对应的 bucketsPathsMap 中的key count Script script = new Script(\"params.count == 0\"); Map bucketsPathsMap = new HashMap<>(); // 这里的malicious对应的 AggregationBuilders.cardinality(\"malicious\") 里的名称 bucketsPathsMap.put(\"count\", \"malicious.value\"); BucketSelectorPipelineAggregationBuilder having = PipelineAggregatorBuilders.bucketSelector(\"having\", bucketsPathsMap, script); CardinalityAggregationBuilder malicious = AggregationBuilders.cardinality(\"malicious\").field(\"maliciousList.keyword\"); TermsAggregationBuilder machineTerm = AggregationBuilders.terms(\"machineTerm\").field(\"outreachMachine.keyword\").size(Integer.MAX_VALUE); machineTerm.subAggregation(malicious); machineTerm.subAggregation(having); searchSourceBuilder.query(boolQueryBuilder); searchSourceBuilder.aggregation(machineTerm); 5、查询数组为空的数据 exists query Returns documents that contain an indexed value for a field. An indexed value may not exist for a document’s field due to a variety of reasons: The field in the source JSON is null or [] The field has “index” : false set in the mapping The length of the field value exceeded an ignore_above setting in the mapping The field value was malformed and ignore_malformed was defined in the mapping GET zc_machine/_search { \"query\": { \"bool\": { \"must\": [ { \"exists\": { \"field\": \"orgDatas\" } } ] } }, \"aggs\": { \"ms\": { \"terms\": { \"field\": \"machineUuid.keyword\", \"size\": 1000 } } } } 6、查询数组中同时包含两个及以上的值 PUT tao_ce { \"mappings\" : { \"properties\" : { } } } POST _bulk { \"index\" : { \"_index\" : \"tao_ce\", \"_id\" : \"1\" } } { \"name\": \"hh\", \"hobbis\":[\"游泳\", \"篮球\",\"足球\"]} { \"index\" : { \"_index\" : \"tao_ce\", \"_id\" : \"2\" } } { \"name\": \"xx\", \"hobbis\":[\"乒乓球\", \"篮球\",\"足球\"]} { \"index\" : { \"_index\" : \"tao_ce\", \"_id\" : \"3\" } } { \"name\":\"zz\", \"hobbis\":[\"乒乓球\", \"篮球\",\"网球\"]} GET tao_ce/_search { \"query\": { \"match_all\": { } } } ### 可以，使用 must-term array GET tao_ce/_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"hobbis.keyword\": { \"value\": \"游泳\" } } }, { \"term\": { \"hobbis.keyword\": { \"value\": \"足球\" } } } ] } } } ### 不可以，文档中包含查询条件数组其一即返回 GET tao_ce/_search { \"query\": { \"bool\": { \"must\": [ { \"terms\": { \"hobbis.keyword\": [ \"游泳\", \"足球\" ] } } ] } } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:45 "},"salesforce/apex.html":{"url":"salesforce/apex.html","title":"Apex","keywords":"","body":"Apex特点:数据类型基本数据类型sObject集合类型定义常量类型转换创建类命名规则类的继承，与接口实现注意事项单元测试@isTest@TestSetUp@isTest(SeeAllData=true)断言数据库操作1.DML语句2.SOQL3.SOSL4.DataBase类注意事项Trigger触发器阻止保存或者删除异常处理发送网络请求设置远程站点GETPOST创建Rest服务数据批处理处理流程及注意事项代码实现SelectOptionString类formatescapeSingleQuotes数据库操作使用技巧IN操作附件Apex Salesforce为开发者提供了Apex语言,语法类似java 特点: ​ 面向对象,完全在云端处理,包括保存编译执行,强类型,大小写不敏感 ​ 强类型语言:指是一种强制类型定义的语言,即一旦定义某一个变量,如果不进行强制类型转换,他就是永远是该数据类型,所对应的弱类型语言则可以根据环境变化自动进行转换,不需要进过强制类型转换来实现。 数据类型 基本数据类型 // 声明为一个测试类 @isTest public class BaseDataType { // 必须为静态的,可以使用@isTest注解和testMethod来进行替换,表述这是一个测试方法 static testMethod void test(){ // 整型 Integer num = 100; System.debug(num); // 双精度类型 Double a = 12.10; System.debug(a); // 字符串类型,只能使用单引号来引用,不可以使用双引号 // String name = \"taoqz\"; String name = 'taoqz'; System.debug(name); // 布尔值 Boolean flag; // 默认为null,也可以为true或false System.debug(flag); System.debug(1111111); // 小数类型 Decimal dec = 1.78; System.debug(dec); // 这是Salesforce特有的一种类型，系统中的对象都是继承自sObject类型， // sObject sobj = new Account(name='精琢科技'); // System.debug(sobj.name); // Date(日期)和DateTime(日期时间)都需要使用其静态方法newInstance创建 // 并且这些变量之间无法直接进行运算,需要使用其静态方法 // 1999-10-09 00:00:00 Date d = Date.newInstance(1999,10,09); System.debug(d); Datetime dt = Datetime.newInstance(2020,5,1); // 2020-04-30 16:00:00 System.debug(dt); // salesfoce中ID类型是一个特有的基本类型,有18个大小写敏感的字符组成 // 也有包含15个字符的,这时是区分大小写敏感的 } } sObject 所有的对象都是sObject类型，所以当创建任何一个对象时，可以声明为sObject类型。 @IsTest static void sObjectTest(){ SObject obj = new Account(Name='obj'); // {Name=obj} System.debug(obj); // sObject类型可以转换为某一对象类型，反之则不行。 // 另外，新建sObject类型的实例只能通过函数newSObject()，而不能通过new关键字。 // 这里还是需要声明创建的sObject类型，比如这里的“Account” sObject sObj = Schema.getGlobalDescribe().get('Account').newSObject(); System.debug(sObj); // sObject obj = new sObject(); // 错误的用法 } 集合类型 List List类型时最常用的一种集合类型,和数组等价,可以相互转换 如果需要判断集合中是否存在值,需要使用 集合.size() == 0进行判断,默认是()代表长度为空 @IsTest static void listTest(){ // List类型,不能使用list作为变量名,因为apex是大小写不敏感的 List list1 = new List(); // () System.debug(list1); // 在创建集合时就添加数据 List list2 = new List{'hello','world','apex'}; // (hello, world, apex) System.debug(list2); List list3 = new String[10]; list3[1] = '测试数据'; // (null, 测试数据, null, null, null, null, null, null, null, null) System.debug(list3); String[] list4 = new String[10]; // (null, null, null, null, null, null, null, null, null, null) System.debug(list4); // apex ,角标也是从0开始的 System.debug(list2.get(2)); } Set @IsTest static void setTest(){ Set strings = new Set(); strings.add('北京'); strings.add('上海'); // 这条记录不会被添加到set集合,set集合的特性 strings.add('上海'); System.debug(strings); for (String str : strings) { System.debug(str); } // false 可以使用contains方法检查元素是否存在于set集合中 System.debug(strings.contains('广州')); } Map @IsTest static void mapTest(){ Map mapsStrings = new Map(); mapsStrings.put('company','jingzhuo'); // {company=jingzhuo} System.debug(mapsStrings); Map strings = new Map{'a' => 'b', 'c' => 'd'.toUpperCase()}; // {a=b, c=D} System.debug(strings); // 迭代获取数据 for (String str : strings.keySet()) { System.debug('key:'+str+' value:'+strings.get(str)); } } 定义常量 // 再类中定义常量 public static final String COMPANY_NAME = '北京**有限公司'; @IsTest static void finalTest(){ System.debug(COMPANY_NAME); } 类型转换 @IsTest static void formatTest(){ // 由低到高的进行自动赋值 Integer num = 100; Long l = num; Double d = l; Decimal dec = d; // 会报语法错误 // Integer i = dec; // System.debug(i); // 将Integer类型转换为String类型 String str = num.format()+100; // 100100 System.debug(str); // 将String类型转换为Integer类型 num = Integer.valueOf(str); System.debug(num); } 创建类 每一个自定义的类在创建对象时都会默认加后缀__c,并且每个自定义字段使用时也需要添加后缀 (两条下划线c),不添加在对数据进行操作时会识别不到 public class Student { public String name; String studentNumber; public String showName() { return name; } public void setName(String sName){ name = sName; } public String getNumber() { return studentNumber; } private void setNumber(String num) { studentNumber = num; } } @IsTest static void studentTest(){ Student student = new Student(); student.name = '123'; // student.setName('zzzz'); // 属性默认是private // student.studentNumber = '123456'; System.debug(student); System.debug(11111111); } 命名规则 我们建议遵循Java命名标准，即类以大写字母开头，方法以小写动词开头，变量名应该有意义。 在同一类中定义具有相同名称的类和接口是不合法的。内部类与其外部类同名也是不合法的。但是，方法和变量在类中具有自己的名称空间，因此这三种类型的名称不会相互冲突。特别是，在一个类中的变量，方法和类具有相同的名称是合法的。 类的继承，与接口实现 父类需要 virtual 关键字， 子类需要 override 关键字来重写方法 具体看代码： 父类 Marker： public virtual class Marker { //要被子类继承的方法，用 virtual 虚拟关键字 public virtual void write(){ System.debug('父类的方法'); } } 父接口 Myinterface： //需要 virtual 关键字，给子类实现 public virtual interface Myinterface { void method(); //方法不需要 virtual关键字 } 实现类 子类 YellowMarker ： //子类要实现接口，需要 virtual 关键字 ， 如果但继承 Marker则不需要 public virtual class YellowMarker extends Marker implements Myinterface{ //继承重写 父类的 write方法 用override关键字 public override void write(){ System.debug('子类的方法'); } //重写接口中的 method方法 , 用virtual关键字 public virtual void method(){ System.debug('重写的接口中反方'); } } 注意事项 自定义对象时,系统会默认为所有自定义对象添加如下几个字段 ​ Name : 名称 ​ LastModifiedById: 上次修改人 ​ CreatedById : 创建人 ​ OwnerId : 所有人(所属) 默认情况下方法和类是final,使用virtual关键字表示该类允许扩展和覆盖(在方法中添加override关键字用于重写) 可以使用super关键字调用父类的构造函数 this代表当前实例,可以调用变量,方法以及构造方法this() Transient关键字的作用是,被其修饰的变量将不作为Visualforce页面的视图状态的一部分,也就是每次都会进行更新(被static修饰的变量也有此效果) 单元测试 单元测试中所有的数据都是独立于实际的数据库的,在单元测试结束后,所有往数据库添加,修改的数据都会被清理掉。 @isTest @TestSetUp /** * Created by T on 2020/4/28. */ @IsTest public with sharing class ExampleTestSetUp { /** * 在运行标记有@IsTest注解上的方法之前会先执行该方法,相当于java中的before * 测试设置方法仅在测试类的默认数据隔离模式下受支持 * 每个测试类别只能有一个测试设置方法 */ @TestSetup static void setUp(){ List accounts = new List{ new Account(name='谷歌',phone='123'), new Account(name='亚马逊',phone='456'), new Account(name='微软',phone='188'), new Account(name='salesforce',phone='789') }; insert accounts; System.debug('我执行了!!'); } @IsTest static void test(){ // 可以查询到,setUp方法中初始的数据,但是不会查询到实际数据库中的数据 List accounts = [SELECT Id,Name,Phone FROM Account]; for (Account account : accounts) { System.debug(account); } System.debug(accounts.size()); } } @isTest(SeeAllData=true) @isTest // 该注解会使测试类可以访问到数据库中真实数据 // 添加在类上则该类中所有的方法都有此特性 // 如果在类上添加了该注解的同时在方法上也添加该注解但其值为false,该false是无效的 // 也可以单独将该注解添加到方法上 //@isTest(SeeAllData=true) public with sharing class ApexTest { @isTest(SeeAllData=true) public static void test(){ List obj = Database.query('select Id,name from account'); for(Account ac : obj){ System.debug(ac); } System.debug('---------------------------'); } @isTest(SeeAllData=true) public static void test2(){ Account acc = new Account(Name = 'IDEA'); // 测试打开权限后,是否可以在数据库中添加 insert acc; List obj = Database.query('select Id,name from account'); // 迭代获取时可以拿到上述添加的数据 // 但在真实数据库中并未添加该数据 for(Account ac : obj){ System.debug(ac); } } } 断言 需要注意的是多个断言只会执行一次,也就是说只要有一个断言成功执行,那么下面的语句则不会执行 @IsTest static void assertTest(){ // 前面的参数为条件表达式,后面则是表达式不成立时打印的数据 // System.AssertException: Assertion Failed: 1大于2 System.assert(1 > 2,'1大于2'); // 条件成立则不会打印 System.assert(1 数据库操作 Salesforce为用户和开发者提供了四种基本的数据库操作方式 // 初始化一些数据 @TestSetup static void setUp(){ List accounts = new List{ new Account(name='谷歌',phone='123'), new Account(name='亚马逊',phone='456'), new Account(name='微软',phone='188'), new Account(name='salesforce',phone='789') }; // 批量插入 insert accounts; System.debug('我执行了!!'); } 1.DML语句 主要用于插入,更新,删除数据 @IsTest static void test(){ List accounts = [SELECT Id,Name,Phone FROM Account]; for (Account account : accounts) { System.debug(account); } // 查询指定数据 Account ac = [SELECT Id,Name,Phone FROM Account WHERE Name = '微软']; ac.Name = 'MicroSoftware'; // 进行更新 update ac; // 再次查询数据 已被更新 System.debug([SELECT Id,Name,Phone FROM Account]); // 删除多条数据 try { delete accounts; } catch (DmlException e) { // 如果出现错误进行处理 } System.debug([SELECT Id,Name,Phone FROM Account]); System.debug(22222); } update和delete 注意事项,如果在单独使用时需要指定id,否则会出现以下错误 Line: 5, Column: 1 System.DmlException: Update failed. First exception on row 0; first error: MISSING_ARGUMENT, Id not specified in an update call: [] upsert 特点: 如果密钥不匹配，则会创建一个新的对象记录。 如果密钥匹配一次，则现有对象记录将更新。 如果键多次匹配，则会生成错误，并且对象记录不会插入或更新。 默认使用ID进行比较,由于在添加对象时会自动分配一个id,所以我们也可以自定义字段进行比较 由于默认使用id,所以在使用DML的upsert的操作时需要指定id 原本的数据 Emp__c emp = new Emp__C(Name='lisi',age__c=20,sex__c='男'); upsert emp; 执行后的数据,这是默认使用id比较,由于自动分配,所以表中数据没有与之相同的id,直接进行了添加 由于upsert的特性,不能同时处理多条匹配的数据 // 修改年龄和性别 Emp__c emp = new Emp__C(Name='lisi',age__c=180,sex__c='男'); // 删除指定ID的对象 Database.delete('a002x0000039SMtAAM'); // 根据指定的字段进行匹配,如果匹配成功修改否则添加 upsert emp Emp__c.Fields.Name; 修改后的数据 undelete 数据删除后在数据库中会有一个保留期也就是不会立即删除,相当于电脑中回收站的操作,可以使用该语句对数据进行恢复操作 // 需要在查询时添加 All rows,代表查询所有行,包括已被标记为删除的行 List emps = [select Id,Name from emp__C where name = 'lisi' all rows]; // 没有添加all rows不会有效果 //List emps = [select Id,Name from emp__c where name = 'lisi']; System.debug(emps); undelete emps; 注意事项 最好不要直接使用DML语句操作对象 2.SOQL 主要用于查询数据 SOQL 基本语法和SQL类似,select后面声明的字段需要用逗号隔开,并且不能使用 * 来选取所有字段,必须声明每个要查询的字段 对于不存在于 SELECT 语句的字段，系统不会去查询其值，所以后面的语句无法使用这些字段的值 如果需要在SOQL中引用对象的属性使用 :对象名.属性名 List accList = [SELECT Id, Name FROM Account]; // 使用 Phone 字段会出错 // accList[0].Phone = '12345678'; 子查询 只支持查询ID Contact con = [SELECT id,Name FROM Contact where name = 'Green Avi']; System.debug(con); Account accs = [SELECT Id,Name FROM Account WHERE Name = :con.Name]; System.debug(accs); String name = 'Green Avi'; // System.QueryException: semi join sub selects can only query id fields, cannot use: 'Name' // Account acc = Database.query('SELECT Id,Name FROM Account WHERE name in (SELECT name FROM Contact where name = \\'Green Avi\\')'); System.debug('acc'); System.debug(acc); map映射 // 会将查询结果直接映射到map中 // Populate map from SOQL query Map m = new Map([SELECT Id, Name FROM Account LIMIT 10]); // After populating the map, iterate through the map entries for (ID idKey : m.keyset()) { Account a = m.get(idKey); System.debug(a); } 案例 根据时间删除 List accs = [SELECT id,name,CreatedDate FROM Account]; for (Account account : accs) { if (account.CreatedDate.date() == Date.today()) { System.debug(account.Name+' '+account.CreatedDate.date()); delete account; } } 3.SOSL 可以返回同时查询多个对象中含有相同字符串的对象 基本语法 FIND 要查询的字符串 IN 要查找的字段 RETURNING 查询结果包含的对象和字段名 规则: 要查询的字符串是无关大小写的 要查找的字段是可选参数，默认是所有字段，即 “ALL FIELDS”，也可以在此声明只在某几个字段中查询，可以使用的仅限于：“NAME FIELDS”、“EMAIL FIELDS”、“PHONE FIELDS”、“SIDEBAR FIELDS” 查询结果包含的对象和字段名可以包含一个或多个想要查询的对象，并声明哪些字段保存在查询结果中 List> searchList = [FIND 'sale' IN ALL FIELDS RETURNING Account (Id, Name),Contact]; System.debug(searchList); 4.DataBase类 可以在数据操作时,拿到操作的状态,其有两个参数,第二个删除表示在进行数据更新操作时其中的数据不符合条件或者出错是否继续执行 其中包含了一组静态函数，它们的作用和 DML 语句类似, 与 DML 语句不同的是，每个函数都有一个可选布尔型参数，可以决定当操作的一组数据中部分数据出现错误时，是否将没有出错的数据继续执行相应的命令。 Database.insert(accList, false); SObject s = Database.query('SELECT Id FROM Account LIMIT 1'); 如果插入的一组数据 accList 中有不符合条件的数据，那么系统会跳过这些数据，将剩下的数据成功插入数据库，并且不会报错。 该布尔型参数默认为“真”（true）。当有部分数据出错时，所有数据都不会被执行相应的操作，并报错。 事务 // 设置保存点 Savepoint sq = Database.setSavepoint(); // 根据保存点进行回滚 Database.rollback(sq); System.debug('first print start'); for (Account account : [SELECT id,Name FROM Account]) { System.debug(account); } System.debug('first print end'); Savepoint sq = Database.setSavepoint(); Account acc = new Account(Name='zs!!!!!!!!!!'); insert acc; System.debug('point print start'); for (Account account : [SELECT id,Name FROM Account]) { System.debug(account); } System.debug('point print end'); Database.rollback(sq); System.debug('done print start'); for (Account account : [SELECT id,Name FROM Account]) { System.debug(account); } System.debug('done print end'); 注意事项 使用SOQL进行变量引用模糊查询时不需要对%的引号进行转义,如果使用DataBase类的query方法在传入字符串时需要对其中的引号进行转义。 String nameCondition = '%' + String.escapeSingleQuotes(query.replaceAll('%', '\\\\%')) + '%'; List accs; // 查询符合条件的特约经销商 // 直接使用SOQL的方式进行模糊查询时不需要对引号进行转义 // 如果使用DataBase类的query方法则传入的字符串中如果有引号需要对其进行转义 // Date.Today() 可以拿到当前日期 accs = [SELECT id,Parentid FROM Account WHERE Parent.Name like :nameCondition AND Contract_Decide_Start_Date__c = :Date.Today()]; Trigger触发器 http://www.ponybai.com/2017/12/07/salesforce%E8%A7%A6%E5%8F%91%E5%99%A8/ 触发器是一种特殊的Apex类,主要作用是在一条记录被插入,修改删除之前或者之后自动执行的一系列操作,每一个Trigger类必须对应一种对象 创建Trigger类 定义Trigger类的名称和对应的对象 IDEA中创建Trigger类 在trigger包中直接创建即可 定义Trigger类中的内容 trigger AccountTriggers on Account (before insert) { // Trigger.New可以拿到添加的数据,代表即将被插入和更新的数据 // 如果有多条数据,可以通过迭代获取,或者使用角标访问 比如[0] System.debug('添加了一条新数据'+Trigger.New); } 在匿名的Debug窗口执行 Account ac = new Account(Name='new account'); insert ac; 打印的日志 14:58:48:022 USER_DEBUG [2]|DEBUG|添加了一条新数据(Account:{Id=null, IsDeleted=false, MasterRecordId=null, Name=new account, Type=null, ParentId=null, BillingStreet=null, BillingCity=null, BillingState=null, BillingPostalCode=null, BillingCountry=null, BillingLatitude=null, BillingLongitude=null, BillingGeocodeAccuracy=null, ShippingStreet=null, ShippingCity=null, ShippingState=null, ShippingPostalCode=null, ShippingCountry=null, ShippingLatitude=null, ShippingLongitude=null, ShippingGeocodeAccuracy=null, Phone=null, Fax=null, AccountNumb Trigger 的触发事件分为以下几种： before insert：插入数据之前 before update：更新数据之前 before delete：删除数据之前 after insert：插入数据之后 after update：更新数据之后 after delete：删除数据之后 after undelete：恢复数据之后 Trigger.New和Trigger.Old是两个预定义的变量,New即将被插入和更新的数据,Old代表更新和删除之前的数据,如果有多条数据可以使用迭代集合的方式获得,也可以通过[0]角标的方式获取 new:返回sObject的记录的最新的数据的列表; newMap:返回一个ID映射到最新的数据列表的Map集合; old:返回sObject的记录修改以前的数据的列表; oldMap:返回一个ID映射到修改以前的数据列表的Map集合; 除了这两个可以拿到运行时的数据之外,还预定义了许多返回值为布尔类型用来判断数据的状态 if (Trigger.isInsert) { if (Trigger.isBefore) { // Process before insert } else if (Trigger.isAfter) { // Process after insert } } else if (Trigger.isDelete) { // Process after delete } 最常用的有： isInsert：是否是 insert 操作 isUpdate：是否是 update 操作 isDelete：是否是 delete 操作 isBefore：是否是操作之前 isAfter：是否是操作之后 阻止保存或者删除 案例:在添加之前使用触发器判断客户名称 客户数据 List accs = new List{ new Account(Name='张三'), new Account(Name='李四'), new Account(Name='王五') }; insert accs; trigger AccountTriggers on Account (before insert) { for (Account acc : Trigger.new) { // 判断新添加的客户名称是否为王五,如果是王五则给出提示信息,会显示在操作的页面的上 // 需要注意的是,该操作会导致整个事务进行回滚 if (acc.Name == '王五') { acc.addError('不能插入名称为王五的客户'); }else { System.debug('添加成功:'+acc.Name); } // System.debug('添加成功:'+acc.Name); } } 异常处理 Apex中和Java中处理异常的方式很像,同样使用try,catch,finally,throw等关键字 try{ Account ac = Null; insert ac; }catch(NullPointerException e){ System.debug('空指针异常!'); System.debug('异常出现在第'+e.getLineNumber()+'行'); }finally{ System.debug('我无论如何都会执行!!!'); } 系统定义的异常类型有： DmlException：关于数据库操作的异常 ListException：关于列表操作的异常 NullPointerException：关于空指针的异常 QueryException：关于查询语句的异常 SObjectException：关于SObject对象的异常 他们共有的方法: getCause()：给出异常原因 getLineNumber()：给出发生异常的行数 getMessage()：给出异常的详细信息 getStackTraceString()：给出异常发生的栈信息 getTypeName()：给出异常的类型，比如 DmlException、NullPointerException 等 发送网络请求 方法文档:https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_classes_restful_http_httprequest.htm 设置远程站点 使用Apex发送网络请求需要在salesforce中设置远程站点 进入设置 --> 搜索 远程站点 --> 新建 在Developer console中打开open Execute Anonymous Window,测试以下代码 GET 接口数据 Http http = new Http(); HttpRequest request = new HttpRequest(); // 设置网络服务接口的地址,需要在salesforce中设置远程站点 request.setEndpoint('https://www.somekey.cn/tiangou/random.php'); // 设置REST方法 request.setMethod('GET'); // 发送HTTP请求 HttpResponse response = http.send(request); // 检查HTTP通信结果状态代码 if (response.getStatusCode() == 200) { // 将拿到的数据(JSON)进行转换为键值对 Map results = (Map) JSON.deserializeUntyped(response.getBody()); // 根据不同的JSON的数据结构获取,这里是为了获取data中的数据 Map data = (Map) results.get('data'); System.debug(results.get('data')); System.debug(data.get('content')); } POST 数据转JSON:https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_class_System_Json.htm#apex_class_System_Json Http http = new Http(); HttpRequest request = new HttpRequest(); // 设置网络服务接口的地址 request.setEndpoint('http://47.102.119.200:8080/user-service/user/login'); // 设置REST方法 request.setMethod('POST'); // 设置请求的Header，类型为JSON request.setHeader('Content-Type', 'application/json;charset=UTF-8'); // 将一个JSON对象传入请求的Body，设置编程语言的名字 request.setBody('{\"username\":\"admin\",\"password\":\"123\"}'); // 发送HTTP请求 HttpResponse response = http.send(request); // 检查HTTP通信结果状态代码 if (response.getStatusCode() == 200) { // 在控制台输出通信结果 System.debug(response.getBody()); } 创建Rest服务 将Apex作为Rest接口提供访问 文档:https://trailhead.salesforce.com/en/content/learn/modules/apex_integration_services/apex_integration_webservices 通过Java访问:https://cloud.tencent.com/developer/article/1014133 // Apex的基本端点是 https://yourInstance.salesforce.com/services/apexrest/URL // URL映射区分大小写,可以包括通配符 @RestResource(urlMapping='/account') global with sharing class MyRestResource { @HttpGet global static List getRecord() { // 添加你的代码 List accs = [select ID,Name from account]; return accs; } } 导航至https://workbench.developerforce.com/login.php进行访问 数据批处理 参考:https://www.cnblogs.com/zero-zyq/p/5287343.html 处理流程及注意事项 代码实现 global with sharing class GoodsBatch implements Database.Batchable,Database.Stateful{ Integer queryCount = 0; String myEmailAddress = 'xxx@gmail.com'; global Database.QueryLocator start(database.BatchableContext bc ) { String query = 'select goodsDescribe__c,Id from GOODS__c'; return Database.getQueryLocator(query); } global void execute (Database.BatchableContext bc, List goodsList) { System.debug(goodsList.size()); for(GOODS__c goods : goodsList) { String price = goods.goodsDescribe__c; // 修改数据 goods.goodsDescribe__c = 'new describe'; queryCount += 1; } System.debug('----------------------------------------'); upsert goodsList; } global void finish(Database.BatchableContext bc) { /*--------execute finish----------*/ /*注意：如果不实现Database.Stateful接口，则queryCount为0 因为在execute执行完成便会回滚到初始状态*/ System.debug('query count:' + queryCount); //send email 可以完成发送邮件 Messaging.SingleEmailMessage email = new Messaging.SingleEmailMessage(); email.setToAddresses(new String[]{myEmailAddress});//set mail getter email.setSubject('show count'); //set subject email.setHtmlBody('query Count' + queryCount); Messaging.sendEmail(new Messaging.SingleEmailMessage[] { email }); } } 执行 // 创建对象 GoodsBatch gb = new GoodsBatch(); // 执行 Database.executeBatch(gb); SelectOption SelectOption option = new SelectOption(value, label, isDisabled); 在自定义控制器或控制器扩展中,可以通过该方式实例化SelectOption 其中，value是如果用户选择了该选项，则返回给控制器的String ，label是作为选项选择显示给用户的String，isDisabled是一个布尔值，如果为true，则指定用户不能选择该选项，但仍可以查看。 SelectOption option = new SelectOption(value, label); 其中，value是如果用户选择了选项，则返回给控制器的字符串，而label是作为选项选择显示给用户的字符串。由于未指定isDisabled的值，因此用户可以查看和选择该选项。 String类 format 该方法可以将集合或者数组中的字符串替换至指定字符中 String str = 'select {0},{1}'; List strings = new List(); strings.add('tao'); strings.add('qz'); strings.add('!!!'); // 同样可以用 // String[] strings = new String[]{'jingzhuo','keji'}; System.debug(String.format(str,strings)); escapeSingleQuotes 可以保留 \\ 符号不将其作为转义符,貌似还有放置sql注入功能 static void escapeSingleQuotesTest(){ String str = 'L\\'Oreal'; // accts = Database.query('Select Id FROM Account WHERE Name like'+wQuotes); // List accts = Database.query('Select Id FROM Account WHERE Name like \\'%'+strEsc+'%\\''); // Select Id FROM Account WHERE Name like '%L\\'Oreal%' // Select Id FROM Account WHERE Name like '%L\\'Oreal%' //debug of SOQL statement reads: Select Id FROM Account WHERE Name like '%L\\'Oreal%' } 数据库操作使用技巧 查询时如果不指定id,会默认查出id IN操作 String[] str = new String[]{'0012x000004FK5oAAG'}; // 可以直接在字符串方式中直接引用可以 迭代 的变量 List accounts = Database.query('select id,name from account where id in :str'); 附件 Salesforce存储的附件的对象为Attachment 几个重要的子段 ParentId : 该附件属于哪条数据 Name : 文件的名称,同时需要有文件的后缀 Body : 则是真正存储附件的位置,显示的为Base64类型,但在apex类中操作时,可以赋值给Blob类型 Attachment att = [select id,name,body from attachment where name = 'quartz.sql']; Blob myBlob = att.body; 获取指定的附件 Attachment att = [select id,name,body from attachment where name = 'quartz.sql']; Blob myBlob = att.body; // 将Blob转为Base64字符串 String blobToBase64String = EncodingUtil.base64Encode(myBlob); // 将Blob转换为字段串 Blob的toString()方法,只支持UTF-8的文本 否则会出现:BLOB is not a valid UTF-8 string String originString = att.body.toString(); System.debug('blobToBase64String++'+blobToBase64String); // 直接打印的格式为 Blob[该附件的字节长度] System.debug('att+++'+att.body); System.debug('originString++'+originString); 插入指定的附件 // 将指定的Base64字符串转为Blob类型 // name需要指定文件的名称以及后缀,后缀决定了你是否能正确打开该文件 // ParentId指定该附件属于哪条数据 Blob myBlob = EncodingUtil.base64Decode(Base64字符串); Attachment att = new Attachment( Name = 'soj.xlsx', ParentId = '01t10000000wAFRAA2', Body = myBlob ); // 添加附件 insert att; 操作Base64和Blob的文档: Blob : https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_methods_system_blob.htm?search_text=blob Base64 : https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_classes_restful_encodingUtil.htm?search_text=blob#apex_classes_restful_encodingutil Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"salesforce/visualforce.html":{"url":"salesforce/visualforce.html","title":"Visualforce","keywords":"","body":"Visualfore基本使用绑定列表变量控制器扩展自定义控制器创建自定义对象静态资源Visualfore ​ Visualforce是一个和Apex语言相匹配的开发框架,开发者可以使用Visualforce和Apex建立网络应用程序,其基本功能类似于前端框架,可以构建页面,与Apex中的进行数据交流,并将结果显示给用户,相当于MVC中的V 基本使用 This is title. This is content of p tag. {!$User.FirstName} + {!$User.LastName} {! 1+1} {! YEAR(TODAY())} {! IF(1 > 2,'1大于2','1小与2')} This is inside page block section. 绑定列表变量 控制器扩展 page 111111111 {!ExampleCustomMessage} 扩展类 /** * Created by T on 2020/4/28. */ public class ExampleControllerExtension { private final Account acc; // 扩展类的构造函数必须使用ApexPages.StandardController类型的参数 public ExampleControllerExtension(ApexPages.StandardController stdController) { // 使用标准控制器变量的getRecord()方法来得到相应SObject对象的值 this.acc = (Account)stdController.getRecord(); } // 方法需要加get前缀,但是在变量表达式调用时不需要加get前缀 public String getExampleCustomMessage() { return 'Hello world!!!'; } } 自定义控制器 Salesforce有默认的“get函数”机制，可以从控制器类中自动得到页面中使用的变量的值 当页面中使用变量“abc”，那么“abc”的值会自动由控制器中的“getAbc()”函数来得到。 此机制适用于单独对象变量和列表对象变量。 自定义控制器 public class CustomAccountListController { public List getMyAccounts() { List results = Database.query( 'SELECT Id, Name, Phone, Fax ' + 'FROM Account ' + 'LIMIT 10' ); return results; } } 页面 创建自定义对象 进入设置界面，搜索“对象”，选择“创建”菜单下的“对象”链接，即可进入“自定义对象”界面。 在“自定义对象”界面中，点击“新建自定义对象”，进入“新建自定义对象”界面。 静态资源 在设置中搜索静态资源,选择新建,设置一个唯一的名称,在页面使用的是该名称,上传文件后保存 页面中使用静态资源 使用 $Resource. 调用js文件,在script标签中调用具体的方法 myFuntion(); Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"salesforce/遇到的问题.html":{"url":"salesforce/遇到的问题.html","title":"遇到的问题","keywords":"","body":"可以将该sql查出来的id映射到Map中 Map maps = new Map((List) Database.query('select id,name from book__c')); for (Id id : maps.keySet()) { System.debug(id); } apex的日志表名称 : ApexLog 请不要在for循环中使用DML语句 对集合中的数据操作时,如果在使用soql查询赋值时没有添加对应的字段,那么集合中数据对应的字段也不会有 使用sql查询赋值对象或集合时,如果对象中某一字段的值为null输出时将不会打印在控制台 // 测试Batch System.Test.startTest(); Database.executeBatch(new AddAutoJudge(ids)); System.Test.stopTest(); // 测试类最好自己新建数据进行测试 // 修改后通过sql重新查然后使用System类提供的方法进行验证 System.assertEquals(); // assertXXX等方法 用到的关于Date的操作 // Date日期类用到的相关操作 // 获取当前月份的第一个工作日 public Date getFirstWorkDayOfMonth() { // 获取当前月份的第一天 Date firstDayOfMonth = Date.today().toStartOfMonth(); // 将月份推到下一个月 Date nextMonth = Date.newInstance(firstDayOfMonth.year(), firstDayOfMonth.month() + 1, 1); // 获取当前月份最后一天 System.debug('lastDay:' + nextMonth.addDays(-1)); Date lastDayOfMonth = nextMonth.addDays(-1); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"salesforce/在IDEA中配置salesforce环境.html":{"url":"salesforce/在IDEA中配置salesforce环境.html","title":"在IDEA中配置salesforce环境","keywords":"","body":"IDEA实在是太强大了,简直什么都能写 注册好salesforce环境后,记得账号密码 打开IDEA,双击shift所有plugins,找到illuminated Cloud插件,点击安装后重启 在IDEA窗口中找到build --> illuminated Cloud --> Configure Application对项目进行配置 输入对应的账号密码 Security Token的获取,在salesfoce中找到 我的设置 --> 个人 --> 重新设置我的安全标记找到后点击重新设置安全标记按钮,会将安全验证码发送到注册时使用的邮箱,填入到一下配置项中即可 配置好后,选择创建一个新的项目,选择对应的选项Connection,就是我们刚才添加的配置,项目创建好后会自动加载salesforce云端中的数据(Apex类等等) 在IDEA的设置中查询以及自定义快捷键 其中该插件针对Apex有许多预定义的快捷键,如果需要自定义快捷键,点击右边的+号选择Live Template 参考链接:https://zhuanlan.zhihu.com/p/53627319 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"salesforce/VSCode配置salesforce环境.html":{"url":"salesforce/VSCode配置salesforce环境.html","title":"VSCode配置salesforce环境","keywords":"","body":"VSCode搭建Salesforce环境工具准备配置VSCode搭建Salesforce环境 工具准备 使用VS Code配置Salesforce环境之前,需要先安装Salesforce CLI Salesforce CLI (选择对应系统安装即可) : https://developer.salesforce.com/tools/sfdxcli VS Code (选择对应系统安装,Windows选择System Installer避免出现小问题) : https://code.visualstudio.com/download 可能还需要安装node,在前端篇可以找到地址和历史版本地址 配置 安装好Salesforce CLI之后,验证安装 # 验证 sfdx --version # 结果 sfdx-cli/7.59.0-05e06e37e0 win32-x64 node-v10.15.3 打开VS Code 安装插件(直接搜索安装即可):Salesforce Extension Pack 按下 shift+ctrl+p 搜索 Create Project with Manifest 选择 Standard 创建项目即可 搜索 Authorize an Org 选择 正式OR测试环境 选择输入Org Alias,登录成功后代表连接成功 连接成功后,会在右侧多出两个图标 选择云形状的图标,找到对应的数据文件(Apex,Page等)点开后下载到本地即可 同样可以通过搜索,完成soql查询等操作 上传合并文件操作,在需要进行操作的文件处右击即可 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/css/css.html":{"url":"worknote/css/css.html","title":"css","keywords":"","body":"块级元素水平垂直居中 .screen-center { width: 500px; height: 500px; background: white; position: absolute; left: 0; top: 0; bottom: 150px; right: 0; margin: auto; } 选项按照内容长度显示 .taoqz { height: auto; display: table; background: #FFF; padding: 8px 15px 8px 10px; border-radius: 3px; border: 1px solid #ebeef5; } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/nacos/nacos配置自动刷新.html":{"url":"worknote/nacos/nacos配置自动刷新.html","title":"nacos","keywords":"","body":"Nacos配置自动更新环境总结示例代码Nacos配置自动更新 环境 JDK11、SpringBoot 2.3.4.RELEASE、Nacos1.4.2 总结 首先在bootstrap.yml中的nacos配置里需要设置对应的 配置文件 自动更新参数 refresh: true 在使用 @Value(“${tao.name}”) 引用配置的类上添加 @RefreshScope 注解，与上面 refresh: true 缺一不可 使用 static 修饰的变量拿不到引用的值 使用接口和实现类，在实现类中也可以达到自动刷新的效果 示例代码 controller package com.fri.commonSrv.controller; import com.fri.commonSrv.constant.Constant; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Value; import org.springframework.cloud.context.config.annotation.RefreshScope; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.bind.annotation.RequestMapping; @RefreshScope @RestController @RequestMapping(Constant.CONTEXT_PATH + \"/versionInfoController\") public class HelloController { @Value(\"${tao.name}\") private String name; @Value(\"${tao.name}\") private static String stringname; @Autowired private Xservice xService; @GetMapping(\"/get\") public String test() { return name; } @GetMapping(\"/getStatic\") public String stringname() { return stringname; } @GetMapping(\"/xService\") public String xService() { return xService.name(); } } service @Service @RefreshScope public class XServiceImpl implements Xservice{ @Value(\"${tao.name}\") private String name; @Override public String name() { return name; } } bootstrap.yml spring: application: cloud: nacos: config: #默认为public命名空间（进行环境隔离，指定不同环境） namespace: @nacos.namespace@ #服务器地址 server-addr: @nacos.url@ #配置文件后缀 file-extension: yml shared-configs[0]: #公用配置文件 data-id: mysql-common.yml refresh: true #是否支持自动刷新，默认false不支持 shared-configs[1]: data-id: consul-common.yml refresh: true profiles: active: @profiles.active@ nacos consul-common.yml spring: cloud: consul: host: 127.0.0.1 port: 8500 tao: name: gggggggggggg Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/docker/docker.html":{"url":"worknote/docker/docker.html","title":"docker","keywords":"","body":"查看容器的日志 docker logs 容器Id 查看容器详情信息 docker inspect 容器Id Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/elementui/elementui.html":{"url":"worknote/elementui/elementui.html","title":"elementui","keywords":"","body":"keydown事件 下载文件 vue+elementui+springboot 多文件上传 选取文件 支持图片、音频、视频、文档，且视频最大100M，文档最大15M handleRemove(file, fileList) { this.fileList = fileList }, handleChange(file, fileList) { this.fileList = fileList }, fileList: [], addResource() { let formData = new FormData() for (let i = 0; i { this.$message({ message: res.data.message, type: 'warning' }); console.log(res.data.message) this.findResourceByRefCatalogId() }) this.fileList = [] // upload 是文件上传组件的ref this.$refs.upload.clearFiles() }, @PostMapping(\"/upload\") public Result uploadResource(@RequestParam MultipartFile[] files, ResourceManage resourceManage) { if (ArrayUtil.isEmpty(files)) { return Result.fail(\"未上传文件\"); } StringBuilder result = new StringBuilder(); for (MultipartFile file : files) { result.append(resourceManageService.uploadResource(file, resourceManage.getRefCatalogId())); } if (StrUtil.isNotBlank(result.toString())) { return Result.ok(result.toString()); } return Result.ok(\"上传成功\"); } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/elk/elk.html":{"url":"worknote/elk/elk.html","title":"elk","keywords":"","body":"elk使用的时5.6.4版本 遇到的问题 1.启动elasticsearch,不以root身份运行 ./elasticsearch 开放端口9200后不可访问? 修改配置文件中的network.host = 0.0.0.0 尽量让elk组件的版本一致,elasticsearch需要java依赖 logstash 报错 Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c5330000, 986513408, 0) failed; error='Cannot allocate memory' (errno=12) 修改配置文件 conf --> jvm.options -Xms1g -Xmx1g # 该为以下 -Xms512m -Xmx512m elasticsearch不能使用root用户登录,其他组件如果遇到应该使用一下方法都可以 logstash 配置文件 input { file { path => \"/var/log/nginx/access.log\" codec => \"json\" } } filter { mutate { split => [ \"upstreamtime\", \",\" ] } mutate { convert => [ \"upstreamtime\", \"float\" ] } } output { stdout { codec => rubydebug } elasticsearch { hosts => [\"39.107.142.3:9200\"] index => \"logstash-%{type}-%{+YYYY.MM.dd}\" document_type => \"%{type}\" flush_size => 20000 idle_flush_time => 10 sniffing => true template_overwrite => true } } nginx log_format配置 log_format json '{\"@timestamp\":\"$time_iso8601\",' '\"host\":\"$server_addr\",' '\"clientip\":\"$remote_addr\",' '\"size\":$body_bytes_sent,' '\"responsetime\":$request_time,' '\"upstreamtime\":\"$upstream_response_time\",' '\"upstreamhost\":\"$upstream_addr\",' '\"http_host\":\"$host\",' '\"url\":\"$uri\",' '\"xff\":\"$http_x_forwarded_for\",' '\"referer\":\"$http_referer\",' '\"agent\":\"$http_user_agent\",' '\"status\":\"$status\"}'; kibana中的索引时根据logstash配置文件中的output下elasticsearch的index的名字决定的 博客 https://developer.ibm.com/zh/technologies/analytics/articles/os-cn-elk/ 搭建ELK 为了兼容性使用的版本全部是5.6.4,注意elasticsearch还需要jdk,至少需要1.8 下载地址 Elasticsearch: https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.4.tar.gz Kibana: https://artifacts.elastic.co/downloads/kibana/kibana-5.6.4-linux-x86_64.tar.gz Logstash: https://artifacts.elastic.co/downloads/logstash/logstash-5.6.4.tar.gz 默认端口 ElasticSearch: 9200 Kibana: 5601 Logstash: 5043 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/gitlab/gitlab.html":{"url":"worknote/gitlab/gitlab.html","title":"gitlab","keywords":"","body":"docker run --detach --hostname taoqz.xyz --publish 8443:443 --publish 8086:80 --publish 8222:22 --name gitlab --restart always --volume /usr/local/gitlab/config:/etc/gitlab --volume /usr/local/gitlab/logs:/var/log/gitlab --volume /usr/local/gitlab/data:/var/opt/gitlab --privileged=true gitlab/gitlab-ce:latest Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/jpa/jpa.html":{"url":"worknote/jpa/jpa.html","title":"jpa","keywords":"","body":"jpa 在in的时候分页 @Override public Page findPageByIdInAndSubjectCode(List taskIdList, Integer subjectCode, int pageNum, int pageSize) { return studyTaskDao.findAll((Specification) (root, query, criteriaBuilder) -> { List predicateList = new ArrayList<>(); Expression exp = root.get(\"id\"); predicateList.add(exp.in(taskIdList)); predicateList.add(criteriaBuilder.equal(root.get(\"taskType\"), StudyTaskEnum.CLASS_CLEAN.getCode())); predicateList.add(criteriaBuilder.equal(root.get(\"subjectCode\"), String.valueOf(subjectCode))); return criteriaBuilder.and(predicateList.toArray(new Predicate[0])); }, PageRequest.of(pageNum - 1, pageSize, Sort.by(\"createTime\").descending())); } 待测试Specification in的时候传入的集合不能为空,需要判断一下 jpa默认映射的时候是按照以下方式映射 java: userId mysql: user_id 但是如果是这样 java: userId mysql: userId 以上会报错,user_id列找不到 解决方案,修改配置,改变映射方案 spring: jpa: database: mysql show-sql: false open-in-view: false hibernate: naming: physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl 问题 rg.hibernate.LazyInitializationException: failed to lazily initialize a collection of role: xyz.taoqz.entity.Teacher.students, could not initialize proxy - no Session 解决方案 在配置文件中添加配置 spring.jpa.properties.hibernate.enable_lazy_load_no_trans=true 问题 org.hibernate.MappingException: Could not determine type for: java.util.List, at table: teacher, for columns: [org.hibernate.mapping.Column(students)] 解决方案 改变bean类映射，要不都在getter方法上写映射关系，要不就在变量上写映射关系 JPA多对多关系 @JoinTable name:关联关系表，joinColumns name:当前类映射的表Id在关联表中的字段名 ，inverseJoinColumns name: 对应的该属性在关联表的Id @ManyToMany @JoinTable(name = \"teacher_student\", joinColumns = {@JoinColumn(name = \"teacherId\")}, inverseJoinColumns = {@JoinColumn(name = \"studentId\")}) public List getStudents() { return students; } JPA使用动态查询 JPA想使用动态查询必须在DAO接口继承多加一个JpaSpecificationExecutor public interface TeacherDao extends JpaRepository , JpaSpecificationExecutor { private Specification createSpecification(Teacher teacher,List ids) { return (Specification) (root, criteriaQuery, criteriaBuilder) -> { List predicateList = new ArrayList<>(); predicateList.add(root.get(\"id\").in(ids)); if (null != teacher){ if (StrUtil.isNotBlank(teacher.getName())) { predicateList.add(criteriaBuilder.like(root.get(\"name\").as(String.class), teacher.getName())); } } return criteriaBuilder.and(predicateList.toArray(new Predicate[0])); }; } jpa in操作 private Specification createSpecification(NotifyEntity notifyEntity) { return (Specification) (root, criteriaQuery, criteriaBuilder) -> { List predicateList = new ArrayList<>(); if (notifyEntity.getTeacherId() != null) { predicateList.add(criteriaBuilder.equal(root.get(\"teacherId\"), notifyEntity.getTeacherId())); } if (StrUtil.isNotEmpty(notifyEntity.getTitle())) { predicateList.add(criteriaBuilder.equal(root.get(\"title\").as(String.class), notifyEntity.getTitle())); } if (StrUtil.isNotEmpty(notifyEntity.getContent())) { predicateList.add(criteriaBuilder.equal(root.get(\"account\").as(String.class), notifyEntity.getContent())); } if (notifyEntity.getBeginTime() != null && notifyEntity.getEndTime() != null) { predicateList.add(criteriaBuilder.between(root.get(\"publishTime\").as(Timestamp.class), notifyEntity.getBeginTime(), notifyEntity.getEndTime())); } if (CollUtil.isNotEmpty(notifyEntity.getTeacherIdList())) { CriteriaBuilder.In in = criteriaBuilder.in(root.get(\"teacherId\")); for (Integer teacherId : notifyEntity.getTeacherIdList()) { in.value(teacherId); } predicateList.add(in); } predicateList.add(criteriaBuilder.equal(root.get(\"flag\"), 0)); return criteriaBuilder.and(predicateList.toArray(new Predicate[0])); }; } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/centos7安装mysql5.7.html":{"url":"worknote/mysql/centos7安装mysql5.7.html","title":"centos7安装mysql5.7","keywords":"","body":"Centos7安装mysql5.71.添加mysql yum仓库2.选择发行版3.安装mysql4.启动mysql5.创建临时密码并登录6.修改密码7.开放端口8.root账户远程连接9.博客来源Centos7安装mysql5.7 1.添加mysql yum仓库 #下载rpm包 wget https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm #安装rpm包获得yum仓库 rpm -Uvh mysql80-community-release-el7-3.noarch.rpm 2.选择发行版 #修改/etc/yum.repos.d/mysql-community.repo配置文件，启用你想安装的mysql版本，这里我们选择mysql5.7 vi /etc/yum.repos.d/mysql-community.repo # 验证是否mysql5.7已启用 yum repolist enabled | grep mysql 3.安装mysql #安装mysql yum install mysql-community-server 4.启动mysql #启动mysql systemctl start mysqld.service #检查状态 systemctl status mysqld.service 5.创建临时密码并登录 #找到临时密码 grep 'temporary password' /var/log/mysqld.log #输入以下命令，用上面的密码登陆mysql mysql -uroot -p 6.修改密码 注意，这里的密码至少包含一位数字，一位大写字母，一位特殊符号，总长度大于8个字符 --修改密码 ALTER USER 'root'@'localhost' IDENTIFIED BY '你的新密码'; 7.开放端口 如果时阿里云在安全组开放端口 #打开3306端口，重启防火墙，或者直接关闭防火墙，二选一即可 firewall-cmd --zone=public --add-port=3306/tcp --permanent systemctl restart firewalld #关闭防火墙，或者选择打开3306接口访问，二选一即可 systemctl stop firewalld 8.root账户远程连接 ---选择数据库 use mysql; --允许任何主机连接 update user set host='%' where user='root'; --刷新权限 flush privileges; 9.博客来源 https://www.jianshu.com/p/b64e6e3bfa24 systemctl start firewalld # 开启防火墙 systemctl stop firewalld # 关闭防火墙 systemctl status firewalld # 查看防火墙开启状态，显示running则是正在运行 firewall-cmd --reload # 重启防火墙，永久打开端口需要reload一下 # 添加开启端口，--permanent表示永久打开，不加是临时打开重启之后失效 firewall-cmd --permanent --zone=public --add-port=8888/tcp # 查看防火墙，添加的端口也可以看到 firewall-cmd --list-all Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/centos卸载mysql.html":{"url":"worknote/mysql/centos卸载mysql.html","title":"centos卸载mysql","keywords":"","body":"centos完全卸载mysql1.查看mysql安装了哪些东西2.卸载3.再次查看是否卸载完全4.查找mysql相关目录5.删除相关目录6.删除mysql的配置文件7.删除mysql日志文件8.博客来源centos完全卸载mysql 1.查看mysql安装了哪些东西 rpm -qa |grep -i mysql 2.卸载 yum remove mysql57..... 3.再次查看是否卸载完全 rpm -qa |grep -i mysql 4.查找mysql相关目录 find / -name mysql 5.删除相关目录 rm -rf 6.删除mysql的配置文件 有可能 my.cnf 文件还有一个后缀,可以cat一下查看是否时mysql的配置文件然后删除 rm -rf /etc/my.cnf 7.删除mysql日志文件 rm -rf /var/log/mysqld.log 8.博客来源 https://www.jianshu.com/p/ef58fb333cd6 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql 主从复制.html":{"url":"worknote/mysql/mysql 主从复制.html","title":"mysql 主从复制","keywords":"","body":"mysql 主从复制 查看从服务器状态 show slave status \\G 查看主服务器状态 show master status; master配置 #Server ID，一般设置成IP地址的最后一位 server_id=3 ##开启log bin，名字最好有意义用来区分 log-bin=dev-bin ##需要进行复制的数据库，可以指定数据库,这里我注释掉不用 ##binlog-do-db=DB_master ##不需要备份的数据库，可以设置多个数据库，一般不会同步mysql这个库 binlog-ignore-db=mysql binlog-ignore-db=information_schema binlog-ignore-db=performance_schema ##为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1m # #二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 # # 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 # # 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 skip-name-resolve salve配置 server_id=22 #binlog-ignore-db=mydql ##binlog-ignore-db=information_schema ##binlog-ignore-db=performance_schema ##log-bin=dev-slave-bin binlog_cache_size=1M binlog_format=mixed expire_logs_days=7 slave_skip_errors=1062 relay_log=dev-relay-bin ##log_slave_updates=1 read_only=1 # 重启mysql systemctl restart mysqld # 从服务器启动slave start slave; # 从服务器启动slave stop slave; 主服务器为从服务器添加用户权限 grant replication slave, replication client on *.* to '用户名'@'从服务器IP' identified by '密码'; 从服务器 change master to master_host='主服务器IP',master_user='上面的用户名',master_password='上面的密码',master_port=3306,master_log_file='dev-bin.000002',master_log_pos=1261,master_connect_retry=30; # 对应下图的FILE的值 master_log_file # 对应下图Position的值 master_log_pos show master status; 先删除从库的数据,后删除主库的数据会导致问题如下(主库的更新操作会复制到从库里) Could not execute Delete_rows event on table mytest.tb_test; Can't find record in 'tb_test', Error_code: 1032; 重新启动配置一下 stop slave ;set global sql_slave_skip_counter=1;start slave; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql时区.html":{"url":"worknote/mysql/mysql时区.html","title":"mysql时区","keywords":"","body":"mysql 时区 mysql字段类型为datatime时,java使用时间戳Timestamp(使用IDEA自带的pojo生成工具生成时的结果),时间戳也就是UTC,从1970年01月01日 0:00:00)开始计算秒数,使用java中的new Timestamp(System.currentTimeMillis())为属性赋值插入到数据库后会少8个小时,因为我们所在的时区对UTC来说是多8个小时的,所以存的时候会少8小时 解决办法: springboot 2.1.6.RELEASE 修改mysql的连接配置 # serverTimezone=GMT%2b8 : mysql默认时区是UTC,修改serverTimezone意为告诉mysql服务器我使用的是哪个时区,这样存时间就会按照本地时区存储 # 只配置了这个代表着从java存储到mysql数据库时可以存储对当前系统时区来说的正常的时间,但是获取时还是会少8小时,并且还需格式化显示 ,在javabean的属性上添加注解 @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\", timezone=\"GMT+8\"),pattern:格式化 timezone:时区,在将数据库中的数据映射到javabean上时按照指定的时区赋值 url: jdbc:mysql://127.0.0.1:3306/ols_gaosan?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2b8&allowPublicKeyRetrieval=true java获取当前系统时区 ZoneId aDefault = TimeZone.getDefault().toZoneId(); System.out.println(aDefault); // Asia/Shanghai(也可将上面连接的时区配置改为这个) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql主从复制_test.html":{"url":"worknote/mysql/mysql主从复制_test.html","title":"mysql主从复制_test","keywords":"","body":"#Server ID，一般设置成IP地址的最后一位 server_id=52 #开启log bin，名字最好有意义用来区分 log-bin=dev-bin #需要进行复制的数据库，可以指定数据库,这里我注释掉不用 #binlog-do-db=DB_master #不需要备份的数据库，可以设置多个数据库，一般不会同步mysql这个库 binlog-ignore-db=mysql binlog-ignore-db=information_schema binlog-ignore-db=performance_schema #为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1m #二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 # 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 # 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 server_id=9 #binlog-ignore-db=mydql #binlog-ignore-db=information_schema #binlog-ignore-db=performance_schema #log-bin=dev-slave-bin binlog_cache_size=1M binlog_format=mixed expire_logs_days=7 slave_skip_errors=1062 relay_log=dev-relay-bin #log_slave_updates=1 read_only=1 master-host=47.105.182.19 master-user=root master-password=Tao.120908!!! master-port=3306 change master to master_host='39.107.142.3',master_user='47.105.182.19',master_password='Mysql.123456',master_port=3306,master_log_file='dev-bin.000002',master_log_pos=613,master_connect_retry=30; mysql-community-common-5.7.32-1.el7.x86_64 mysql80-community-release-el7-3.noarch mysql-community-libs-5.7.32-1.el7.x86_64 mysql-community-server-5.7.32-1.el7.x86_64 mysql-community-client-5.7.32-1.el7.x86_64 mysql-community-libs-compat-5.7.32-1.el7.x86_64 /usr/share/mysql /etc/selinux/targeted/tmp/modules/100/mysql /etc/selinux/targeted/active/modules/100/mysql /var/lib/mysql /var/lib/mysql/mysql grant replication slave, replication client on *.* to 'repl'@'47.105.182.19' identified by 'slavePassword!123'; change master to master_host='39.107.142.3',master_user='repl',master_password='slavePassword!123',master_port=3306,master_log_file='dev-bin.000003',master_log_pos=1231,master_connect_retry=30; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql日期函数.html":{"url":"worknote/mysql/mysql日期函数.html","title":"mysql日期函数","keywords":"","body":"mysql日期函数 12.5. 日期和时间函数 本章论述了一些可用于操作时间值的函数。关于每个时间和日期类型具有的值域及指定值的有效格式，请参见11.3节，“日期和时间类型”。 下面的例子使用了时间函数。以下询问选择了最近的 30天内所有带有date_col 值的记录： mysql> SELECT *something* FROM *tbl_name* -> WHERE DATE_SUB(CURDATE(),INTERVAL 30 DAY) 注意，这个询问也能选择将来的日期记录。 以下是其他日期函数使用说明 l ADDDATE(date,INTERVAL expr type) ADDDATE(expr,days) 当被第二个参数的INTERVAL格式激活后， ADDDATE()就是DATE_ADD()的同义词。相关函数SUBDATE() 则是DATE_SUB()的同义词。对于INTERVAL参数上的信息 ，请参见关于DATE_ADD()的论述。 mysql> SELECT DATE_ADD('1998-01-02', INTERVAL 31 DAY); ​ -> '1998-02-02' mysql> SELECT ADDDATE('1998-01-02', INTERVAL 31 DAY); ​ -> '1998-02-02' 若 days 参数只是整数值，则 MySQL 5.1将其作为天数值添加至 expr。 mysql> SELECT ADDDATE('1998-01-02', 31); ​ -> '1998-02-02' l CURDATE() 将当前日期按照'YYYY-MM-DD' 或YYYYMMDD 格式的值返回，具体格式根据函数用在字符串或是数字语境中而定。 mysql> SELECT CURDATE(); ​ -> '1997-12-15' mysql> SELECT CURDATE() + 0; ​ -> 19971215 l CURTIME() 将当前时间以'HH:MM:SS'或 HHMMSS 的格式返回， 具体格式根据函数用在字符串或是数字语境中而定。 mysql> SELECT CURTIME(); ​ -> '23:50:26' l DATE(expr) 提取日期或时间日期表达式expr中的日期部分。 mysql> SELECT DATE('2003-12-31 01:02:03'); ​ -> '2003-12-31' l DATEDIFF(expr,expr2) DATEDIFF() 返回起始时间 expr和结束时间expr2之间的天数。Expr和expr2 为日期或 date-and-time 表达式。计算中只用到这些值的日期部分。 mysql> SELECT DATEDIFF('1997-12-31 23:59:59','1997-12-30'); ​ -> 1 mysql> SELECT DATEDIFF('1997-11-30 23:59:59','1997-12-31'); ​ -> -31 l DATE_ADD(date,INTERVAL expr type) DATE_SUB(date,INTERVAL expr type) 这些函数执行日期运算。 date 是一个 DATETIME 或DATE值，用来指定起始时间。 expr 是一个表达式，用来指定从起始日期添加或减去的时间间隔值。 Expr是一个字符串;对于负值的时间间隔，它可以以一个 ‘-’开头。 type 为关键词，它指示了表达式被解释的方式。 关键词INTERVA及 type 分类符均不区分大小写。 以下表显示了type 和expr 参数的关系： type 值 预期的 expr 格式 MICROSECOND MICROSECONDS SECOND SECONDS MINUTE MINUTES HOUR HOURS DAY DAYS WEEK WEEKS MONTH MONTHS QUARTER QUARTERS YEAR YEARS SECOND_MICROSECOND 'SECONDS.MICROSECONDS' MINUTE_MICROSECOND 'MINUTES.MICROSECONDS' MINUTE_SECOND 'MINUTES:SECONDS' HOUR_MICROSECOND 'HOURS.MICROSECONDS' HOUR_SECOND 'HOURS:MINUTES:SECONDS' HOUR_MINUTE 'HOURS:MINUTES' DAY_MICROSECOND 'DAYS.MICROSECONDS' DAY_SECOND 'DAYS HOURS:MINUTES:SECONDS' DAY_MINUTE 'DAYS HOURS:MINUTES' DAY_HOUR 'DAYS HOURS' YEAR_MONTH 'YEARS-MONTHS' MySQL 允许任何expr 格式中的标点分隔符。表中所显示的是建议的 分隔符。若 date 参数是一个 DATE 值，而你的计算只会包括 YEAR、MONTH和DAY部分(即, 没有时间部分), 其结果是一个DATE 值。否则，结果将是一个 DATETIME值。 若位于另一端的表达式是一个日期或日期时间值 ， 则INTERVAL expr type只允许在 + 操作符的两端。对于 –操作符， INTERVAL expr type 只允许在其右端，原因是从一个时间间隔中提取一个日期或日期时间值是毫无意义的。 (见下面的例子）。 mysql> SELECT DATE_ADD('1997-12-31 23:59:59', -> INTERVAL 1 SECOND); ​ -> '1998-01-01 00:00:00' mysql> SELECT DATE_ADD('1997-12-31 23:59:59', -> INTERVAL 1 DAY); ​ -> '1998-01-01 23:59:59' mysql> SELECT DATE_ADD('1997-12-31 23:59:59', -> INTERVAL '1:1' MINUTE_SECOND); ​ -> '1998-01-01 00:01:00' mysql> SELECT DATE_SUB('1998-01-01 00:00:00', -> INTERVAL '1 1:1:1' DAY_SECOND); ​ -> '1997-12-30 22:58:59' mysql> SELECT DATE_ADD('1998-01-01 00:00:00', -> INTERVAL '-1 10' DAY_HOUR); ​ -> '1997-12-30 14:00:00' mysql> SELECT DATE_SUB('1998-01-02', INTERVAL 31 DAY); ​ -> '1997-12-02' mysql> SELECT DATE_ADD('1992-12-31 23:59:59.000002', -> INTERVAL '1.999999' SECOND_MICROSECOND); ​ -> '1993-01-01 00:00:01.000001' 若你指定了一个过于短的时间间隔值 (不包括type 关键词所预期的所有时间间隔部分), MySQL 假定你已经省去了时间间隔值的最左部分。 例如，你指定了一种类型的DAY_SECOND, expr 的值预期应当具有天、 小时、分钟和秒部分。若你指定了一个类似 '1:10'的值, MySQL 假定天和小时部分不存在，那么这个值代表分和秒。换言之, '1:10' DAY_SECOND 被解释为相当于 '1:10' MINUTE_SECOND。这相当于 MySQL将TIME 值解释为所耗费的时间而不是日时的解释方式。 假如你对一个日期值添加或减去一些含有时间部分的内容，则结果自动转化为一个日期时间值： mysql> SELECT DATE_ADD('1999-01-01', INTERVAL 1 DAY); ​ -> '1999-01-02' mysql> SELECT DATE_ADD('1999-01-01', INTERVAL 1 HOUR); ​ -> '1999-01-01 01:00:00' 假如你使用了格式严重错误的日期,则结果为 NULL。假如你添加了 MONTH、YEAR_MONTH或YEAR ，而结果日期中有一天的日期大于添加的月份的日期最大限度，则这个日期自动被调整为添加月份的最大日期： mysql> SELECT DATE_ADD('1998-01-30', INTERVAL 1 MONTH); ​ -> '1998-02-28' l DATE_FORMAT(date,format) 根据format 字符串安排date 值的格式。 以下说明符可用在 format 字符串中： 说明符 说明 %a 工作日的缩写名称 (Sun..Sat) %b 月份的缩写名称 (Jan..Dec) %c 月份，数字形式(0..12) %D 带有英语后缀的该月日期 (0th, 1st, 2nd, 3rd, ...) %d 该月日期, 数字形式 (00..31) %e 该月日期, 数字形式(0..31) %f 微秒 (000000..999999) %H 小时(00..23) %h 小时(01..12) %I 小时 (01..12) %i 分钟,数字形式 (00..59) %j 一年中的天数 (001..366) %k 小时 (0..23) %l 小时 (1..12) %M 月份名称 (January..December) %m 月份, 数字形式 (00..12) %p 上午（AM）或下午（ PM） %r 时间 , 12小时制 (小时hh:分钟mm:秒数ss 后加 AM或PM) %S 秒 (00..59) %s 秒 (00..59) %T 时间 , 24小时制 (小时hh:分钟mm:秒数ss) %U 周 (00..53), 其中周日为每周的第一天 %u 周 (00..53), 其中周一为每周的第一天 %V 周 (01..53), 其中周日为每周的第一天 ; 和 %X同时使用 %v 周 (01..53), 其中周一为每周的第一天 ; 和 %x同时使用 %W 工作日名称 (周日..周六) %w 一周中的每日 (0=周日..6=周六) %X 该周的年份，其中周日为每周的第一天, 数字形式,4位数;和%V同时使用 %x 该周的年份，其中周一为每周的第一天, 数字形式,4位数;和%v同时使用 年份, 数字形式,4位数 %y 年份, 数字形式 (2位数) %% ‘%’文字字符 所有其它字符都被复制到结果中，无%Y需作出解释。 注意， ‘%’字符要求在格式指定符之前。 月份和日期说明符的范围从零开始，原因是 MySQL允许存储诸如 '2004-00-00'的不完全日期. mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00', '%W %M %Y'); -> 'Saturday October 1997' mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00', '%H:%i:%s'); -> '22:23:00' mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00', '%D %y %a %d %m %b %j'); -> '4th 97 Sat 04 10 Oct 277' mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00', '%H %k %I %r %T %S %w'); -> '22 22 10 10:23:00 PM 22:23:00 00 6' mysql> SELECT DATE_FORMAT('1999-01-01', '%X %V'); -> '1998 52' l DAY(date) DAY() 和DAYOFMONTH()的意义相同。 l DAYNAME(date) 返回date 对应的工作日名称。 mysql> SELECT DAYNAME('1998-02-05'); ​ -> '周四' l DAYOFMONTH(date) 返回date 对应的该月日期，范围是从 1到31。 mysql> SELECT DAYOFMONTH('1998-02-03'); ​ -> 3 l DAYOFWEEK(date) 返回date (1 = 周日, 2 = 周一, ..., 7 = 周六)对应的工作日索引。这些索引值符合 ODBC标准。 mysql> SELECT DAYOFWEEK('1998-02-03'); ​ -> 3 l DAYOFYEAR(date) 返回date 对应的一年中的天数，范围是从 1到366。 mysql> SELECT DAYOFYEAR('1998-02-03'); ​ -> 34 l FROM_DAYS(N) 给定一个天数 N, 返回一个DATE值。 mysql> SELECT FROM_DAYS(729669); ​ -> '1997-10-07' 使用 FROM_DAYS()处理古老日期时，务必谨慎。他不用于处理阳历出现前的日期(1582)。请参见12.6节，“MySQL使用什么日历？”。 l HOUR(time) 返回time 对应的小时数。对于日时值的返回值范围是从 0 到 23 。 mysql> SELECT HOUR('10:05:03'); ​ -> 10 然而, TIME 值的范围实际上非常大, 所以HOUR可以返回大于23的值。 mysql> SELECT HOUR('272:59:59'); ​ -> 272 l LAST_DAY(date) 获取一个日期或日期时间值，返回该月最后一天对应的值。若参数无效，则返回NULL。 mysql> SELECT LAST_DAY('2003-02-05'); ​ -> '2003-02-28' mysql> SELECT LAST_DAY('2004-02-05'); ​ -> '2004-02-29' mysql> SELECT LAST_DAY('2004-01-01 01:01:01'); ​ -> '2004-01-31' mysql> SELECT LAST_DAY('2003-03-32'); ​ -> NULL l MINUTE(time) 返回 time 对应的分钟数,范围是从 0 到 59。 mysql> SELECT MINUTE('98-02-03 10:05:03'); ​ -> 5 l MONTH(date) 返回date 对应的月份，范围时从 1 到 12。 mysql> SELECT MONTH('1998-02-03'); ​ -> 2 l NOW() 返回当前日期和时间值，其格式为 'YYYY-MM-DD HH:MM:SS' 或YYYYMMDDHHMMSS ， 具体格式取决于该函数是否用在字符串中或数字语境中。 mysql> SELECT NOW(); ​ -> '1997-12-15 23:50:26' mysql> SELECT NOW() + 0; ​ -> 19971215235026 在一个存储程序或触发器内, NOW() 返回一个常数时间，该常数指示了该程序或触发语句开始执行的时间。这同SYSDATE()的运行有所不同。 l WEEK(date[,mode]) 该函数返回date 对应的星期数。WEEK() 的双参数形式允许你指定该星期是否起始于周日或周一， 以及返回值的范围是否为从0 到53 或从1 到53。若 mode参数被省略，则使用default_week_format系统自变量的值。请参见5.3.3节，“服务器系统变量”。 以下表说明了mode 参数的工作过程：d 第一天 Mode 工作日 范围 Week 1 为第一周 ... 0 周日 0-53 本年度中有一个周日 1 周一 0-53 本年度中有3天以上 2 周日 1-53 本年度中有一个周日 3 周一 1-53 本年度中有3天以上 4 周日 0-53 本年度中有3天以上 5 周一 0-53 本年度中有一个周一 6 周日 1-53 本年度中有3天以上 7 周一 1-53 本年度中有一个周一 mysql> SELECT WEEK('1998-02-20'); ​ -> 7 mysql> SELECT WEEK('1998-02-20',0); ​ -> 7 mysql> SELECT WEEK('1998-02-20',1); ​ -> 8 mysql> SELECT WEEK('1998-12-31',1); ​ -> 53 注意，假如有一个日期位于前一年的最后一周， 若你不使用2、3、6或7作为mode 参数选择，则MySQL返回 0： mysql> SELECT YEAR('2000-01-01'), WEEK('2000-01-01',0); ​ -> 2000, 0 有人或许会提出意见，认为 MySQL 对于WEEK() 函数应该返回 52 ，原因是给定的日期实际上发生在1999年的第52周。我们决定返回0作为代替的原因是我们希望该函数能返回“给定年份的星期数”。这使得WEEK() 函数在同其它从日期中抽取日期部分的函数结合时的使用更加可靠。 假如你更希望所计算的关于年份的结果包括给定日期所在周的第一天，则应使用 0、2、5或 7 作为mode参数选择。 mysql> SELECT WEEK('2000-01-01',2); ​ -> 52 作为选择，可使用 YEARWEEK()函数: mysql> SELECT YEARWEEK('2000-01-01'); ​ -> 199952 mysql> SELECT MID(YEARWEEK('2000-01-01'),5,2); ​ -> '52' l YEAR(date) 返回date 对应的年份,范围是从1000到9999。 mysql> SELECT YEAR('98-02-03'); ​ -> 1998 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql_bin-log.html":{"url":"worknote/mysql/mysql_bin-log.html","title":"mysql_bin-log","keywords":"","body":"mysql开启bin-log,在my.ini中配置 # Binary Logging. log-bin=D:\\MySQL\\mybinlog\\mysql_bin server-id=120908 binlog-format=Row mysqlbinlog --start-position=391772 --stop-position=392035 D:\\MySQL\\mybinlog\\mysql_bin.000001 > d:\\test.sql source d:\\test.sql mysqlbinlog --start-datetime=\"2021-05-11 12:28:00\" --stop-datetime=\"2021-05-11 12:28:10\" D:\\MySQL\\mybinlog\\mysql_bin.000001 > d:\\test2.sql Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql_null.html":{"url":"worknote/mysql/mysql_null.html","title":"mysql_null","keywords":"","body":"mysql中sum函数没统计到任何记录时,会返回null而不是0,可以使用ifnull()函数将null转换为0 mysql中count字段不统计null值所在的列,count(*)才是统计所有记录数量的正确方式 mysql中 = null并不是判断条件而是赋值,对null进行判断只能使用is null 或者is not null Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/mysql/mysql8.html":{"url":"worknote/mysql/mysql8.html","title":"mysql8","keywords":"","body":"MySQL8修改密码MySQL8 修改密码 use mysql; ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'zzz'; Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/rabbitmq/docker安装rabbitmq.html":{"url":"worknote/rabbitmq/docker安装rabbitmq.html","title":"rabbitmq","keywords":"","body":"docker安装rabbitmq 拉取镜像 docker pull rabbitmq 创建容器并运行 15672: web管理页面 5672: 数据通信端口 rabbitmq: 镜像名称 docker run -itd --name myrabbitmq -p 15672:15672 -p 5672:5672 rabbitmq 启动管理页面 myrabbitmq: 容器名称 docker exec -it myrabbitmq rabbitmq-plugins enable rabbitmq_management 默认账号密码 username: guest password: guest 清空rabbitmq中的所有队列和交换机 # 进入容器 docker exec -it 容器ID /bin/bash # 进入容器 /sbin 目录 # 关闭应用 rabbitmqctl stop_app # 清除 rabbitmqctl reset # 重启 rabbitmqctl start_app rabbitmq使用过程中遇到的问题 1.再使用docker安装rabbitmq时,还需要启动其内置的管理程序 2.再结合springboot使用传递对象时,对象的全路径名称要一致,否则序列化失败,也可以转为json字符串传输 3.注册TopicExchange // 报错 @Bean public TopicExchange exchange() { return new TopicExchange(\"exchange\"); } // 正常 @Bean public TopicExchange topicExchange() { return new TopicExchange(\"exchange\"); } 交换机 Direct : 默认,进行路由键的全值匹配 Topic: 和Direct类似,在Diret的基础上添加模糊匹配机制,名称使用.分割,*代表一个指定的字符,#代表零个或多个字符 Fanout: 广播模式,无视路由键和路由模式,会将消息发送给绑定至该交换机的所有队列,如果配置了routing_key会被忽略 http://www.ityouknow.com/springboot/2016/11/30/spring-boot-rabbitMQ.html Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/redis/redis.html":{"url":"worknote/redis/redis.html","title":"redis","keywords":"","body":"redis下载安装配置环境变量生成redis服务化管理脚本查看redis服务状态配置远程访问redis下载安装 https://redis.io/download 安装 # 下载安装包 wget https://download.redis.io/releases/redis-6.0.9.tar.gz # 安装环境 yum install gcc cmake -y # 安装scl源 yum install centos-release-scl scl-utils-build # 解压 tar zxvf redis-6.0.9.tar.gz # 解压后进入目录编译安装 make && make install # 验证 yum install tcl -y make test 配置环境变量 # REDIS export REDIS_HOME=/usr/local/redis export PATH=$PATH:$REDIS_HOME/bin 或者用src目录 # 重新加载配置文件 source /etc/profile 生成redis服务化管理脚本 # 进入utils目录,生成redis服务化管理脚本 ./install_server.sh 运行上面的语句后会出现警告提示语,注释掉./install_server.sh文件中的如下行即可 再使用./install_server.sh命令,选择默认的端口和配置 查看redis服务状态 # 查看端口号6379对应redis实例的运行状态 service redis_6379 status # 停止端口号6379对应redis实例服务 service redis_6379 stop # 启动端口号6379对应redis实例服务 service redis_6379 start 配置远程访问 修改启动时使用的配置文件中如下项 bind 127.0.0.1　　改为　　#bind 127.0.0.1 daemonize no　　　　改为　　 daemonize yes protected-mode yes 　　改为　　protected-mode no 博客 https://blog.csdn.net/goodsirlee/article/details/106078654 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/websocket/springboot+vue+websocket.html":{"url":"worknote/websocket/springboot+vue+websocket.html","title":"websocket","keywords":"","body":"springboot+vue+websocket 环境 java: 1.8 springboot: 2.3.4.RELEASE 所需依赖 org.springframework.boot spring-boot-starter-websocket org.projectlombok lombok WebSocketServer 其中会维护一个键值对,键可以通过session中的属性设置,value存session,利用该session进行推送消息 package xyz.taoqz.component; import lombok.extern.slf4j.Slf4j; import org.springframework.stereotype.Component; import javax.annotation.PostConstruct; import javax.websocket.*; import javax.websocket.server.ServerEndpoint; import java.io.IOException; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.CopyOnWriteArraySet; import java.util.concurrent.atomic.AtomicInteger; /** * @author taoqz */ @ServerEndpoint(value = \"/websocket\") @Component @Slf4j public class WebSocketServer { @PostConstruct public void init() { } private static final AtomicInteger ONLINE_COUNT = new AtomicInteger(0); /** * concurrent包的线程安全Set，用来存放每个客户端对应的Session对象。 */ private static final CopyOnWriteArraySet SESSION_SET = new CopyOnWriteArraySet<>(); private static final ConcurrentHashMap SESSION_MAP = new ConcurrentHashMap<>(); /** * 连接建立成功调用的方法 */ @OnOpen public void onOpen(Session session) { String host = session.getRequestURI().getHost(); // SESSION_MAP.put(\"192.168.1.142\", session); SESSION_MAP.put(host, session); SESSION_SET.add(session); // 在线数加1 int cnt = ONLINE_COUNT.incrementAndGet(); log.info(\"有连接加入，当前连接数为：{}\", cnt); sendMessage(session, \"连接成功\"); } /** * 连接关闭调用的方法 */ @OnClose public void onClose(Session session) { SESSION_SET.remove(session); SESSION_MAP.remove(session.getRequestURI().getHost()); int cnt = ONLINE_COUNT.decrementAndGet(); log.info(\"有连接关闭，当前连接数为：{}\", cnt); } /** * 收到客户端消息后调用的方法 * * @param message 客户端发送过来的消息 */ @OnMessage public void onMessage(String message, Session session) { log.info(\"来自客户端的消息：{}\", message); sendMessage(session, \"收到消息，消息内容：\" + message); } /** * 出现错误 * * @param session session 对象 * @param error 错误信息 */ @OnError public void onError(Session session, Throwable error) { log.error(\"发生错误：{}，Session ID： {}\", error.getMessage(), session.getId()); error.printStackTrace(); } /** * 发送消息，实践表明，每次浏览器刷新，session会发生变化。 * * @param session session 对象 * @param message 消息 */ public void sendMessage(Session session, String message) { try { session.getBasicRemote().sendText(message); } catch (IOException e) { log.error(\"发送消息出错：{}\", e.getMessage()); e.printStackTrace(); } } public void sendMessageByHost(String host, String message) { try { Session session = SESSION_MAP.get(host.replace(\"http://\", \"\")); if (session.isOpen()) { session.getBasicRemote().sendText(message); } else { SESSION_MAP.remove(host); log.error(\"该session已关闭！\"); } } catch (Exception e) { log.error(\"发送消息出错：{}\", e.getMessage()); e.printStackTrace(); } } /** * 群发消息 * * @param message 消息 */ public void broadCastInfo(String message) { for (Session session : SESSION_SET) { if (session.isOpen()) { sendMessage(session, message); } } } /** * 指定Session发送消息 * * @param sessionId sessionId * @param message message */ public void sendMessage(String sessionId, String message) throws IOException { Session session = null; for (Session s : SESSION_SET) { if (s.getId().equals(sessionId)) { session = s; break; } } if (session != null) { sendMessage(session, message); } else { log.warn(\"没有找到你指定ID的会话：{}\", sessionId); } } } WebSocketConfig package xyz.taoqz.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.socket.server.standard.ServerEndpointExporter; /** * @author waver */ @Configuration public class WebSocketConfig { /** * 给spring容器注入这个ServerEndpointExporter对象 * 相当于xml： * * * * * 检测所有带有@serverEndpoint注解的bean并注册他们。 * * @return ServerEndpointExporter */ @Bean public ServerEndpointExporter serverEndpointExporter() { return new ServerEndpointExporter(); } } vue所需内容 下载依赖 npm install vue-native-websocket 导包 import VueNativeSock from 'vue-native-websocket' // 这里的地址,写后端的全路径 Vue.use(VueNativeSock, 'ws://localhost:8989/resourcenter/websocket', { connectManually: true }) 使用 webSocketMessage(host) { // 这里的host需要和后端中session的key设置为相同的属性 let wsServer = host.replace('http://', 'ws://') console.log(wsServer + this.websocketURL) // 开启连接 this.$connect(wsServer + this.websocketURL, {format: 'json'}) // 接收消息 this.$options.sockets.onmessage = function (msg) { if (msg.data === '连接成功') { console.log(msg.data) } else { this.$message.success(msg.data) } } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/tess4j_ocr/Tess4j使用记录.html":{"url":"worknote/tess4j_ocr/Tess4j使用记录.html","title":"tess4j_ocr","keywords":"","body":"Tess4j使用记录 springboot 依赖 net.sourceforge.tess4j tess4j 4.5.1 com.google.cloud libraries-bom 16.1.0 pom import java代码 tessdata: 将tessdata文件夹放到resources目录下 public class Tess4jUtil { public static String graphicRecognition(File imageFile, String language) throws TesseractException { // JNA Interface Mapping ITesseract instance = new Tesseract(); // JNA Direct Mapping // ITesseract instance = new Tesseract1(); //In case you don't have your own tessdata, let it also be extracted for you //这样就能使用classpath目录下的训练库了 File tessDataFolder = LoadLibs.extractTessResources(\"tessdata\"); //Set the tessdata path instance.setDatapath(tessDataFolder.getAbsolutePath()); instance.setTessVariable(\"user_defined_dpi\", \"300\"); //英文库识别数字比较准确 instance.setLanguage(language); String result = instance.doOCR(imageFile); return result; } } 训练字库 tesseract zwp.test.exp0.tif zwp.test.exp0 batch.nochop makebox echo test 0 0 0 0 0 >font_properties tesseract zwp.test.exp0.tif zwp.test.exp0 nobatch box.train unicharset_extractor zwp.test.exp0.box shapeclustering -F font_properties -U unicharset -O zwp.unicharset zwp.test.exp0.tr mftraining -F font_properties -U unicharset -O zz.unicharset zwp.test.exp0.tr cntraining zwp.test.exp0.tr combine_tessdata zwp. 参考链接 安装与使用: https://sourceforge.net/projects/tesseract-ocr-alt/files/ https://digi.bib.uni-mannheim.de/tesseract/ https://www.cnblogs.com/pejsidney/p/9487881.html https://tesseract-ocr.github.io/tessdoc/Data-Files 文字训练: https://www.cnblogs.com/xpwi/p/9604567.html https://sourceforge.net/projects/vietocr/files/jTessBoxEditor/ 训练字库 https://www.jianshu.com/p/c8ba23ec672a Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/swagger/swagger.html":{"url":"worknote/swagger/swagger.html","title":"swagger","keywords":"","body":"swagger2 版本 : 2.9.2 记录一次错误 java.lang.NumberFormatException: For input string: \"\" 当写dataType时,需要填写example参数 @ApiImplicitParams({ // @ApiImplicitParam(value = \"10\", name = \"名称\", example = \"1000\",required = false, paramType = \"query\", dataType = \"int\", dataTypeClass = Integer.class) @ApiImplicitParam(name = \"名称\",required = false, paramType = \"query\", dataType = \"int\") }) Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/stream.html":{"url":"worknote/other/stream.html","title":"stream","keywords":"","body":"map和flatMap的区别 List a = new ArrayList<>(); a.add(1); a.add(2); List b = new ArrayList<>(); b.add(3); b.add(4); // 分隔字符串时会比较常用,因为split方法会把字符串分割成多个数组,利用Stream.of()方法可以获得各数组对应的流,flatMap将这多个流合并成一个流 List figures = Stream.of(a, b).flatMap(u -> u.stream()).collect(Collectors.toList()); System.out.println(figures); // [1, 2, 3, 4] // 返回的是流对象 List> collect1 = Stream.of(a, b).map(u -> u.stream()).collect(Collectors.toList()); for (Stream integerStream : collect1) { integerStream.forEach(System.out::println); } Stream.of(a, b).map(u -> u.stream()).forEach(ele -> ele.forEach(System.out::println)); flatMap可以将多个流合并为一个流,而map不可以合并,拿到的每一个元素还是一个流 groupingBy/partitioningBy分组 groupingBy 在collect中的操作Collectors.groupingBy可以按照指定属性分组,返回一个map集合,key的类型是分组属性的类型,value的类型是分组属性的类型的集合 List persons = new ArrayList(); Person person1 = new Person(1, \"name\" + 1); Person person2 = new Person(2, \"name\" + 2); Person person3 = new Person(2, \"name\" + 2); persons.add(person1); persons.add(person2); persons.add(person3); Map> collect = persons.stream().collect(Collectors.groupingBy(Person::getId)); for (Integer integer : collect.keySet()) { System.out.println(integer+\" \"+collect.get(integer)); } partitioningBy 按照条件进行分组,返回一个Map>,符合Collectors.partitioningBy(条件)的元素则为true,否则为false List persons = new ArrayList(); for (int i = 1; i > collect = persons.stream().collect(Collectors.partitioningBy(ele -> ele.getId() > 3)); System.out.println(collect.get(true)); // [Person(id=4, name=name4), Person(id=5, name=name5)] System.out.println(collect.get(false));// [Person(id=1, name=name1), Person(id=2, name=name2), Person(id=3, name=name3)] System.out.println(collect.size()); 根据对象的属性去重 List cloudResources = new ArrayList<>(); CloudResource cloudResource1 = new CloudResource(); cloudResource1.setMd5(\"1\"); cloudResource1.setRefFilePath(\"1\"); CloudResource cloudResource2 = new CloudResource(); cloudResource2.setMd5(\"2\"); cloudResource2.setRefFilePath(\"2\"); System.out.println(cloudResource1); System.out.println(cloudResource2); cloudResources.add(cloudResource1); cloudResources.add(cloudResource2); // cloudResources = cloudResources.stream().filter(ele -> fileUploadUtilByMinio.statObject(ele.getRefFilePath())).collect(Collectors.collectingAndThen(Collectors.toCollection(() -> cloudResources = cloudResources.stream().filter(ele -> Integer.parseInt(ele.getMd5()) >= 1).collect(Collectors.collectingAndThen(Collectors.toCollection(() -> // new TreeSet<>(Comparator.comparing(CloudResource::getMd5))), ArrayList::new)); new TreeSet<>(Comparator.comparing(ele -> ele.getMd5()+ele.getRefFilePath()) )), ArrayList::new)); cloudResources.forEach(ele -> System.out.println(ele.getRefFilePath()+\"==\"+ele.getMd5())); 将一对象集合根据属性转变为一个map集合,如果作为key得对象得属性有重复得可以选择使用第一个还是第二个 ArrayList usersA = new ArrayList<>(); ArrayList usersB = new ArrayList<>(); Collections.addAll(usersA, new User(\"张三\",10.0), new User(\"李四\",20.0), new User(\"王五\",30.0) ); Collections.addAll(usersB, new User(\"李四\",40.0), new User(\"赵六\",50.0), new User(\"张三\",60.0) ); ArrayList result = new ArrayList<>(); result.addAll(usersA); result.addAll(usersB); System.out.println(result); Map collect = result.stream().collect(Collectors.toMap(User::getName, User::getSalary, (e1, e2) -> e2)); for (String s : collect.keySet()) { System.out.println(s+\" \"+collect.get(s)); } // 效果 [User(name=张三, salary=10.0), User(name=李四, salary=20.0), User(name=王五, salary=30.0), User(name=李四, salary=40.0), User(name=赵六, salary=50.0), User(name=张三, salary=60.0)] 李四 40.0 张三 60.0 王五 30.0 赵六 50.0 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/windows操作端口.html":{"url":"worknote/other/windows操作端口.html","title":"windows操作端口","keywords":"","body":"windows下查看端口 查看所有运行的端口 netstat -ano 查看指定端口的占用情况 netstat -aon|findstr \"8081\" 结束指定PID的进程 taskkill /T /F /PID 9088 server { listen 8080; server_name localhost; ssl_session_timeout 5m; #charset koi8-r; location /resourcenter { proxy_pass http://localhost:8989/resourcenter; proxy_set_header Host $host:80; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 1000m; client_body_buffer_size 10m; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 64k; proxy_intercept_errors on ; } location /resourceupload { proxy_pass http://127.0.0.1:9000; proxy_set_header Host $host:80; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 1000m; client_body_buffer_size 10m; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 64k; proxy_intercept_errors on ; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/读取文件时自动添加BOM头的问题.html":{"url":"worknote/other/读取文件时自动添加BOM头的问题.html","title":"读取文件时自动添加BOM头的问题","keywords":"","body":"Java读取Unicode文件时碰到的BOM首字符问题,及处理方法 https://blog.csdn.net/ClementAD/article/details/47168573 在windows下用文本编辑器创建的文本文件,如果用UTF-8等 Unicode格式保存,会在文件头,第一个字符加入一个BOM标识 在Java读取文件时,不会被去掉,String.trim()也无法删除 Java在读取Unicode文件时会统一把BOM变成\\uFEFF,可以手动解决 number = number.trim().replaceAll(\"\\uFEFF\",\"\"); 什么是BOM？ BOM = Byte Order Mark BOM是Unicode规范中推荐的标记字节顺序的方法。比如说对于UTF-16，如果接收者收到的BOM是FEFF，表明这个字节流是Big-Endian的；如果收到FFFE，就表明这个字节流是Little-Endian的。 UTF-8不需要BOM来表明字节顺序，但可以用BOM来表明“我是UTF-8编码”。BOM的UTF-8编码是EF BB BF（用UltraEdit打开文本、切换到16进制可以看到）。所以如果接收者收到以EF BB BF开头的字节流，就知道这是UTF-8编码了。 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/java_pdf转图片+打压缩包.html":{"url":"worknote/other/java_pdf转图片+打压缩包.html","title":"java_pdf转图片+打压缩包","keywords":"","body":"依赖 commons-logging commons-logging 1.2 org.apache.pdfbox fontbox 2.0.1 org.apache.pdfbox pdfbox 2.0.1 cn.hutool hutool-all 5.3.3 package com.tifenpai.plum.common.util; import cn.hutool.core.io.FileUtil; import org.apache.pdfbox.pdmodel.PDDocument; import org.apache.pdfbox.rendering.PDFRenderer; import org.junit.Test; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.web.multipart.MultipartFile; import javax.imageio.ImageIO; import java.awt.image.BufferedImage; import java.io.*; import java.util.zip.ZipEntry; import java.util.zip.ZipOutputStream; /** * @author taoqz * Created by taoqz on 2020/11/3. */ public class PdfCovertImageUtil { private static final Logger log = LoggerFactory.getLogger(PdfCovertImageUtil.class); @Test public void demo() { String fileName = \"人教版九年级全一册初中英语电子课本（2013年最新版）.pdf\"; fileName = fileName.substring(0, fileName.lastIndexOf(\".\")); System.out.println(fileName); } public static File covert(String fileTempPath, MultipartFile file) throws IOException { long currentTimeMillis = System.currentTimeMillis(); File parentDir = new File(fileTempPath, String.valueOf(currentTimeMillis)); boolean mkdir = parentDir.mkdir(); // 将pdf文件下载到指定的文件夹下,进行pdf转图片之后删除源文件 InputStream inputStream = file.getInputStream(); String fileName = file.getOriginalFilename(); String prefix = fileName.substring(0, fileName.lastIndexOf(\".\")); String suffix = fileName.substring(fileName.lastIndexOf(\".\"), fileName.length()); File pdfFilePath = new File(fileTempPath, prefix + \"_\" + currentTimeMillis + \"_\" + suffix); BufferedInputStream bis = new BufferedInputStream(inputStream); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(pdfFilePath)); int len; byte[] bytes = new byte[1024]; while ((len = bis.read(bytes)) != -1) { bos.write(bytes); bos.flush(); } bos.close(); bis.close(); // 加载PDF文件 // PdfDocument doc = new PdfDocument(); // doc.loadFromFile(pdfFilePath.getAbsolutePath()); // 保存PDF的每一页到图片 BufferedImage image; PDDocument doc = PDDocument.load(pdfFilePath); PDFRenderer renderer = new PDFRenderer(doc); System.out.println(doc.getPages().getCount()); int count = doc.getPages().getCount(); int zeroNum = 3; if (count >= 1000) { zeroNum = String.valueOf(doc.getPages().getCount()).length(); } for (int i = 0; i Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/element-ui解决notify速度太快会重叠的问题.html":{"url":"worknote/other/element-ui解决notify速度太快会重叠的问题.html","title":"element-ui解决notify速度太快会重叠的问题","keywords":"","body":"element-ui解决notify速度太快会重叠的问题 data(){ return{ notifyPromise: Promise.resolve(), } }, methods:{ zz(){ this.notifyPromise = this.notifyPromise.then(this.$nextTick).then(() => { this.$notify.error({ title: '题目：' + this.tableSelectionQuestionList[i].questionId + ' 添加试题篮失败!' , message: '', duration: 3000 }) }) } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/vue获取htmlcollection的length为0问题.html":{"url":"worknote/other/vue获取htmlcollection的length为0问题.html","title":"vue获取htmlcollection的length为0问题","keywords":"","body":"this.$nextTick(function () { let imgDom = document.getElementsByClassName('ql-image') console.log(imgDom) console.log(imgDom.length) for (let i = 0; i 参考博客 https://blog.csdn.net/weixin_41602509/article/details/86661758 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/解决a标签自动浏览问题.html":{"url":"worknote/other/解决a标签自动浏览问题.html","title":"解决a标签自动浏览问题","keywords":"","body":" 下载 修改 删除 downloadResource(row) { if (!this.serverUrl) { this.getMinioConfig() } var request = new XMLHttpRequest(); request.responseType = \"blob\"; request.open(\"GET\", this.serverUrl + row.filePath); request.onload = function () { var url = window.URL.createObjectURL(this.response); var a = document.createElement(\"a\"); document.body.appendChild(a); a.href = url; a.download = row.fileName a.click(); } request.send(); }, Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/解决element-ui在展示数据时会少显示一个空格的问题.html":{"url":"worknote/other/解决element-ui在展示数据时会少显示一个空格的问题.html","title":"解决element-ui在展示数据时会少显示一个空格的问题","keywords":"","body":"解决element-ui在展示数据时会少显示一个空格的问题 解决方案:将空格替换为 &nbsp; {{scope.row.chinese.replace(/ /g, '&nbsp;')}} Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"worknote/other/解决vue使用video播放视频无画面问题.html":{"url":"worknote/other/解决vue使用video播放视频无画面问题.html","title":"解决vue使用video播放视频无画面问题","keywords":"","body":"chrom只支持H264编码的MP4文件播放,在vue中使用H5中的video标签播放视频时无画面 在java中进行转换编码 maven导入jar包 ws.schild jave-core 2.4.5 ws.schild jave-native-win64 2.4.5 核心代码 public class Mpeg4ToH264Util { private static Logger log = LoggerFactory.getLogger(Mpeg4ToH264Util.class); private static final String SOURCE = \"sourcehomework\"; private static final String TARGET = \"targethomework\"; public static String covertToH264(MultipartFile file, String tempPath, FileBusinessModuleEnum fileTypeEnum, FileUploadUtilByMinio fileUploadUtilByMinio) { // 将文件下载到本地,从本地读取出来后进行转码再生成文件上传到minio String filePath; long currentTimeMillis = System.currentTimeMillis(); String suffix = Objects.requireNonNull(file.getOriginalFilename()).substring(file.getOriginalFilename().lastIndexOf(\".\")); File localFile = new File(tempPath, SOURCE + \"_\" + currentTimeMillis + suffix); BufferedInputStream bis = null; BufferedOutputStream bos = null; try { bis = new BufferedInputStream(file.getInputStream()); bos = new BufferedOutputStream(new FileOutputStream(localFile)); int len; byte[] bytes = new byte[1024]; while ((len = bis.read(bytes)) != -1) { bos.write(bytes); bos.flush(); } } catch (IOException e) { e.printStackTrace(); } finally { try { bis.close(); bos.close(); } catch (IOException e) { e.printStackTrace(); } } File target = new File(tempPath, TARGET + \"_\" + currentTimeMillis + suffix); log.info(\"转换前的路径:\" + localFile); log.info(\"转换后的路径:\" + target); AudioAttributes audio = new AudioAttributes(); //音频编码格式 audio.setCodec(\"libmp3lame\"); audio.setBitRate(800000); audio.setChannels(1); //audio.setSamplingRate(new Integer(22050)); VideoAttributes video = new VideoAttributes(); //视频编码格式 video.setCodec(\"libx264\"); video.setBitRate(3200000); //数字设置小了，视频会卡顿 video.setFrameRate(15); EncodingAttributes attrs = new EncodingAttributes(); attrs.setFormat(\"mp4\"); attrs.setAudioAttributes(audio); attrs.setVideoAttributes(video); Encoder encoder = new Encoder(); MultimediaObject multimediaObject = new MultimediaObject(localFile); try { log.info(\"Mpeg4转H264 --- 转换开始:\" + new Date()); encoder.encode(multimediaObject, target, attrs); log.info(\"Mpeg4转H264 --- 转换结束:\" + new Date()); } catch (Exception e) { log.info(localFile.getAbsolutePath() + \" 转换失败\"); e.printStackTrace(); } filePath = fileUploadUtilByMinio.uploadByFile(target, fileTypeEnum); localFile.delete(); target.delete(); return filePath; } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:47 "},"other/使用Google Cloud搭建ssr.html":{"url":"other/使用Google Cloud搭建ssr.html","title":"搭建SSR","keywords":"","body":"使用Google Cloud搭建ssr1.需要用到2.申请注册谷歌云3.创建实例4.创建防火墙规则5.开始搭建使用Google Cloud搭建ssr ​ 在youtube或者各种搜索引擎上已经有不少的视频和教程,写本文也是分享一下自己的经验:happy:。 ​ 谷歌云平台会赠送新用户300美金，免费使用一年，薅羊毛嘛:joy:。 1.需要用到 ​ 1.1.前置的科学上网工具 ​ 1.2.谷歌账号 ​ 1.3.支持外币支付的信用卡（ps.我申请的中国银行的万事达） ​ 这里放上我申请的信用卡链接:https://mp.weixin.qq.com/s/NXSM8C5mnq7I4NAlOOMlNw 2.申请注册谷歌云 ​ 2.1.地址:https://cloud.google.com/free/ ​ 2.2.其中的地区已经没有中国了选其他地区或国家,我选的香港,将自己的信用卡绑定后进入主页面 ​ 可以看到下图说明已经成功激活谷歌云了 3.创建实例 ​ 3.1.点击左上角菜单栏图标选中结算,去激活你的结算账号 ​ 3.2.选中Compute Engine ​ 3.3.创建实例详细信息 ​ 3.4.测试延迟 ​ 创建VM实例完成后,会有一个外部IP,复制该IP进行 ​ 测速:https://tools.ipip.net/traceroute.php ​ 延迟在50左右还行,100以下都能用,越小越好 4.创建防火墙规则 ​ 4.1选中菜单栏VPC网络点击防火墙规则 ​ 注意需要创建两个,下图中会有说明 5.开始搭建 ​ 5.1.在创建好的实例有一个SSH选择在浏览器窗口打开 ​ 依次输入以下命令 ​ 5.1.1. sudo -i 切换到root ​ 5.1.2. 安装SSR wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.sh chmod +x shadowsocksR.sh ./shadowsocksR.sh 2>&1 | tee shadowsocksR.log ​ 5.1.3. ./shadowsocksR.sh 运行 ​ 5.2.回车运行后会看到下图,有解释说明 到此一个通过Google Cloud搭建的SSR就可以使用了:beers: Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"other/kaptcha验证码.html":{"url":"other/kaptcha验证码.html","title":"Kaptcha验证码","keywords":"","body":"Kaptcha结合springboot使用依赖配置类controllerKaptcha Kaptcha是一个高度可配置的生成验证码的工具 结合springboot使用 依赖 com.google.code.kaptcha kaptcha 2.3 配置类 @Configuration public class KaptchaConfig { @Bean public DefaultKaptcha getDefaultKaptcha(){ com.google.code.kaptcha.impl.DefaultKaptcha defaultKaptcha = new com.google.code.kaptcha.impl.DefaultKaptcha(); Properties properties = new Properties(); // 图片边框 properties.put(\"kaptcha.border\", \"no\"); properties.put(\"kaptcha.textproducer.font.color\", \"black\"); // 图片高宽 properties.put(\"kaptcha.image.width\", \"150\"); properties.put(\"kaptcha.image.height\", \"40\"); // 字体大小 properties.put(\"kaptcha.textproducer.font.size\", \"30\"); // 存在session中的key properties.put(\"kaptcha.session.key\", \"verifyCode\"); // 验证码长度 properties.put(\"kaptcha.textproducer.char.length\", \"5\"); // 文字间隔 properties.put(\"kaptcha.textproducer.char.space\", \"5\"); // 图片样式 properties.put(\"kaptcha.obscurificator.impl\", \"com.google.code.kaptcha.impl.FishEyeGimpy\"); Config config = new Config(properties); defaultKaptcha.setConfig(config); return defaultKaptcha; } } controller @Controller public class KaptchaController { @Autowired private DefaultKaptcha captchaProducer; @GetMapping(\"/kaptcha\") public void defaultKaptcha(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception { byte[] captchaOutputStream = null; ByteArrayOutputStream imgOutputStream = new ByteArrayOutputStream(); try { // 生产验证码字符串并保存到session中 String verifyCode = captchaProducer.createText(); System.out.println(verifyCode); httpServletRequest.getSession().setAttribute(\"verifyCode\", verifyCode); BufferedImage challenge = captchaProducer.createImage(verifyCode); ImageIO.write(challenge, \"jpg\", imgOutputStream); } catch (IllegalArgumentException e) { httpServletResponse.sendError(HttpServletResponse.SC_NOT_FOUND); return; } captchaOutputStream = imgOutputStream.toByteArray(); httpServletResponse.setHeader(\"Cache-Control\", \"no-store\"); httpServletResponse.setHeader(\"Pragma\", \"no-cache\"); httpServletResponse.setDateHeader(\"Expires\", 0); httpServletResponse.setContentType(\"image/jpeg\"); ServletOutputStream responseOutputStream = httpServletResponse.getOutputStream(); responseOutputStream.write(captchaOutputStream); responseOutputStream.flush(); responseOutputStream.close(); } } Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"other/ffmpeg.html":{"url":"other/ffmpeg.html","title":"ffmpeg","keywords":"","body":"#x86下载二进制文件 wget https://www.moerats.com/usr/down/ffmpeg/ffmpeg-git-32bit-static.tar.xz #x86_64下载二进制文件 wget https://www.moerats.com/usr/down/ffmpeg/ffmpeg-git-64bit-static.tar.xz #解压文件 tar -xvf ffmpeg-git-64bit-static.tar.xz #tar xvf ffmpeg-git-*-static.tar.xz && rm -rf ffmpeg-git-*-static.tar.xz #将ffmpeg和ffprobe可执行文件移至/usr/bin方便系统直接调用 mv ffmpeg-git-*/ffmpeg ffmpeg-git-*/ffprobe /usr/bin/ #查看版本 ffmpeg ffprobe # 反转视频 ./ffmpeg -i /usr/local/ffmpeg/oceans.mp4 -vf reverse -af areverse -preset superfast /usr/local/ffmpeg/reverse.mp4 Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:55:46 "},"other/搭建基于公网的samba服务.html":{"url":"other/搭建基于公网的samba服务.html","title":"samba","keywords":"","body":"搭建基于公网的samba服务解决方案如下访问搭建配置文件创建用户遇到文件夹没有权限时搭建基于公网的samba服务 现在有一台某运营商的服务器，centos8，要在上面搭建一个samba文件共享服务 遇到的问题，本地的windows主机445端口被运营商禁用导致无法连接（同局域网下的samba服务连接时没有问题） 解决方案如下 再windows主机上做如下修改 使用管理员打开cmd，设置完成之后 需要重启 # 将本地445端口转发对应服务器上samba的端口 samba服务的默认端口为445 需要提前将其改为指定端口 如6727 netsh interface portproxy add v4tov4 listenaddress=127.0.0.1 listenport=445 connectaddress=远程服务器IP connectport=6727 # 查看当前windows主机所有连接 netsh interface portproxy show all # 删除配置 netsh interface portproxy delete v4tov4 listenaddress=127.0.0.1 listenport=445 访问 在windows资源管理器输入 windows默认时445现在已经将其转发到对应samba服务的主机上的指定端口 这样的弊端就是 其他的映射磁盘将不会再受用，根据情况进行设置 \\\\127.0.0.1\\shared 搭建 # 安装 install samba samba-client samba-common 配置文件 /etc/samba/smb.conf 示例 # See smb.conf.example for a more detailed config file or # read the smb.conf manpage. # Run 'testparm' to verify the config is correct after # you modified it. [global] workgroup = SAMBA security = user passdb backend = tdbsam printing = cups printcap name = cups load printers = yes cups options = raw # samba服务使用的端口 默认为445 #smb ports = 6727 [homes] comment = Home Directories valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes [printers] comment = All Printers path = /var/tmp printable = Yes create mask = 0600 browseable = No [print$] comment = Printer Drivers path = /var/lib/samba/drivers write list = @printadmin root force group = @printadmin create mask = 0664 directory mask = 0775 [shared] # 共享文件夹地址 path = /srv/samba/share writable = yes browsable = yes guest ok = yes # 无论什么用户登录，强制转换为root用户进行操作 #force user = root # 登录时需要验证的用户 valid users = taoqz create mask = 0755 directory mask = 0775 创建用户 输入命令后后让你输入密码和确认密码 sudo smbpasswd -a taoqz 遇到文件夹没有权限时 sudo chown -R taoqz:taoqz /srv/samba/share sudo chmod -R 0777 /srv/samba/share Copyright © TaoQZ 2019 all right reserved，powered by Gitbook作者联系方式：taoqingzhou@gmail.com 修订时间： 2024-10-03 11:40:18 "}}